<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-180692466-1', 'auto');
ga('send', 'pageview');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">406</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">73</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Awaken Desparado</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2023/12/06/C++/2023-12-06-CRTP%20(Curiously%20Recurring%20Template%20Pattern)/">C++/2023-12-06-CRTP (Curiously Recurring Template Pattern)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-12-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/C/">C++</a></span><div class="content"><h1 id="CRTP-Curiously-Recurring-Template-Pattern"><a href="#CRTP-Curiously-Recurring-Template-Pattern" class="headerlink" title="CRTP (Curiously Recurring Template Pattern)"></a>CRTP (Curiously Recurring Template Pattern)</h1><h3 id="My-Errors"><a href="#My-Errors" class="headerlink" title="My Errors"></a>My Errors</h3><ol>
<li>C 語言本身並不支持模板（templates），這是 C++ 語言的一個特性。模板是 C++ 中用於泛型編程的一種工具，</li>
<li><code>include &lt;iostream&gt;</code> , instead of <code>include &lt;stdio.h&gt;</code></li>
<li><code>g++</code>, instead of <code>gcc</code></li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// #include &lt;stdio.h&gt;   // no!</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">T <span class="title">myMax</span><span class="params">(T x, T y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x &gt; y) ? x : y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; myMax&lt;<span class="keyword">int</span>&gt;(<span class="number">3</span>, <span class="number">7</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; myMax&lt;<span class="keyword">char</span>&gt;(<span class="string">'g'</span>, <span class="string">'e'</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// cout &lt;&lt; "hello" &lt;&lt; endl;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h2 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h2><ol>
<li><p><strong>生成預處理文件 (.i)</strong>: 這個文件包含了經過預處理的源代碼，其中包括所有包含的頭文件和展開的宏。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bashCopy code</span><br><span class="line">g++ -E my_fT.cpp -o my_fT.i</span><br></pre></td></tr></table></figure>

<p>這將生成一個 <code>my_fT.i</code> 文件，其中包含預處理後的代碼。</p>
</li>
<li><p><strong>生成組合語言文件 (.s)</strong>: 這個文件包含了轉換成組合語言的源代碼。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bashCopy code</span><br><span class="line">g++ -S my_fT.cpp -o my_fT.s</span><br><span class="line">// or g++ -S my_fT.i -o my_fT.s, same</span><br></pre></td></tr></table></figure>

<p>這將生成一個 <code>my_fT.s</code> 文件，其中包含組合語言代碼。</p>
</li>
<li><p><strong>生成目標文件 (.o)</strong>: 這是編譯過程中生成的二進制代碼，但還沒有進行最終的鏈接。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bashCopy code</span><br><span class="line">g++ -c my_fT.cpp -o my_fT.o</span><br></pre></td></tr></table></figure>

<p>這將生成一個 <code>my_fT.o</code> 文件，這是一個目標文件，包含機器代碼但尚未鏈接成可執行文件。</p>
</li>
<li><p><strong>生成可執行文件</strong>: 最後，您可以使用以下命令將目標文件鏈接成可執行文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bashCopy code</span><br><span class="line">g++ my_fT.o -o my_fT</span><br></pre></td></tr></table></figure>

<p>這將生成 <code>my_fT</code> 可執行文件，您可以像之前一樣運行它。</p>
</li>
</ol>
<h2 id="驗證"><a href="#驗證" class="headerlink" title="驗證"></a>驗證</h2><p>要驗證編譯器是否為模板函數 <code>myMax</code> 生成了特定的實例（如 <code>int</code> 和 <code>char</code> 版本），您可以使用一些進階的技術。由於模板實例化通常在編譯過程中進行，並且可能不會直接反映在預處理或組合語言輸出中，您需要使用其他方法來檢查這些實例。</p>
<ol>
<li><p><strong>查看符號信息</strong>: 使用 <code>nm</code> 工具查看目標文件（<code>.o</code> 文件）中的符號信息。這可以顯示出編譯器為模板生成的所有實例。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jo@fossa4gb:~/repo/exp_functionTemplate$ nm my_fT.o  | grep myMax</span><br><span class="line">0000000000000000 W _Z5myMaxIcET_S0_S0_</span><br><span class="line">0000000000000000 W _Z5myMaxIiET_S0_S0_</span><br></pre></td></tr></table></figure>

<p>在 <code>nm</code> 的輸出中，尋找 <code>myMax&lt;int&gt;</code> 和 <code>myMax&lt;char&gt;</code> 的實例。它們可能會以某種編碼（名稱修飾）的形式出現。</p>
</li>
<li><p><strong>使用 c++filt 工具</strong>: <code>c++filt</code> 是一個工具，用於解碼（demangle）C++ 符號名稱。這對於理解 <code>nm</code> 工具輸出的名稱修飾特別有用。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">jo@fossa4gb:~/repo/exp_functionTemplate$ nm my_fT.o  | c++filt</span><br><span class="line">                 U __cxa_atexit</span><br><span class="line">                 U __dso_handle</span><br><span class="line">00000000000000d8 t _GLOBAL__sub_I_main</span><br><span class="line">0000000000000000 T main</span><br><span class="line">0000000000000078 t __static_initialization_and_destruction_0(int, int)</span><br><span class="line">0000000000000000 W char myMax&lt;char&gt;(char, char)</span><br><span class="line">0000000000000000 W int myMax&lt;int&gt;(int, int)</span><br><span class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;::operator&lt;&lt;(int)</span><br><span class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;::operator&lt;&lt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; (*)(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;))</span><br><span class="line">                 U std::ios_base::Init::Init()</span><br><span class="line">                 U std::ios_base::Init::~Init()</span><br><span class="line">                 U std::cout</span><br><span class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::endl&lt;char, std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;)</span><br><span class="line">0000000000000000 r std::piecewise_construct</span><br><span class="line">0000000000000000 b std::__ioinit</span><br><span class="line">                 U std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp; std::operator&lt;&lt; &lt;std::char_traits&lt;char&gt; &gt;(std::basic_ostream&lt;char, std::char_traits&lt;char&gt; &gt;&amp;, char)</span><br></pre></td></tr></table></figure>

<p>這樣可以讓您更容易閱讀和理解 <code>nm</code> 的輸出，尤其是對於模板實例化。</p>
</li>
<li><p><strong>閱讀組合語言輸出</strong>: 雖然直接在 <code>.s</code> 文件中找到模板實例化可能比較困難，但您可以嘗試尋找特定的函數調用和操作。這可能需要對組合語言有一定的了解。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">jo@fossa4gb:~/repo/exp_functionTemplate$ grep -n3 <span class="string">"myMax"</span> my_fT.s</span><br><span class="line">23-     mov     x29, sp</span><br><span class="line">24-     mov     w1, 7</span><br><span class="line">25-     mov     w0, 3</span><br><span class="line">26:     bl      _Z5myMaxIiET_S0_S0_</span><br><span class="line">27-     mov     w1, w0</span><br><span class="line">28-     adrp    x0, :got:_ZSt4cout</span><br><span class="line">29-     ldr     x0, [x0, <span class="comment">#:got_lo12:_ZSt4cout]</span></span><br><span class="line">--</span><br><span class="line">35-     bl      _ZNSolsEPFRSoS_E</span><br><span class="line">36-     mov     w1, 101</span><br><span class="line">37-     mov     w0, 103</span><br><span class="line">38:     bl      _Z5myMaxIcET_S0_S0_</span><br><span class="line">39-     and     w0, w0, 255</span><br><span class="line">40-     mov     w1, w0</span><br><span class="line">41-     adrp    x0, :got:_ZSt4cout</span><br><span class="line">--</span><br><span class="line">55-     .cfi_endproc</span><br><span class="line">56-.LFE1522:</span><br><span class="line">57-     .size   main, .-main</span><br><span class="line">58:     .section        .text._Z5myMaxIiET_S0_S0_,<span class="string">"axG"</span>,@progbits,_Z5myMaxIiET_S0_S0_,comdat</span><br><span class="line">59-     .align  2</span><br><span class="line">60:     .weak   _Z5myMaxIiET_S0_S0_</span><br><span class="line">61:     .<span class="built_in">type</span>   _Z5myMaxIiET_S0_S0_, %<span class="keyword">function</span></span><br><span class="line">62:_Z5myMaxIiET_S0_S0_:</span><br><span class="line">63-.LFB1759:</span><br><span class="line">64-     .cfi_startproc</span><br><span class="line">65-     sub     sp, sp, <span class="comment">#16</span></span><br><span class="line">--</span><br><span class="line">80-     ret</span><br><span class="line">81-     .cfi_endproc</span><br><span class="line">82-.LFE1759:</span><br><span class="line">83:     .size   _Z5myMaxIiET_S0_S0_, .-_Z5myMaxIiET_S0_S0_</span><br><span class="line">84:     .section        .text._Z5myMaxIcET_S0_S0_,<span class="string">"axG"</span>,@progbits,_Z5myMaxIcET_S0_S0_,comdat</span><br><span class="line">85-     .align  2</span><br><span class="line">86:     .weak   _Z5myMaxIcET_S0_S0_</span><br><span class="line">87:     .<span class="built_in">type</span>   _Z5myMaxIcET_S0_S0_, %<span class="keyword">function</span></span><br><span class="line">88:_Z5myMaxIcET_S0_S0_:</span><br><span class="line">89-.LFB1762:</span><br><span class="line">90-     .cfi_startproc</span><br><span class="line">91-     sub     sp, sp, <span class="comment">#16</span></span><br><span class="line">--</span><br><span class="line">106-    ret</span><br><span class="line">107-    .cfi_endproc</span><br><span class="line">108-.LFE1762:</span><br><span class="line">109:    .size   _Z5myMaxIcET_S0_S0_, .-_Z5myMaxIcET_S0_S0_</span><br><span class="line">110-    .text</span><br><span class="line">111-    .align  2</span><br><span class="line">112-    .<span class="built_in">type</span>   _Z41__static_initialization_and_destruction_0ii, %<span class="keyword">function</span></span><br><span class="line">jo@fossa4gb:~/repo/exp_functionTemplate$</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>使用 GDB 調試器</strong>: 使用 GDB 調試器運行您的程序，並在 <code>myMax</code> 函數上設置斷點。當程序執行到這些點時，您可以檢查哪個版本的 <code>myMax</code> 被調用。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bashCopy code</span><br><span class="line">g++ -g my_fT.cpp -o my_fT</span><br><span class="line">gdb ./my_fT</span><br></pre></td></tr></table></figure>

<p>然後在 GDB 中設置斷點（例如，<code>break myMax&lt;int&gt;</code> 和 <code>break myMax&lt;char&gt;</code>），並運行程序來看看是否觸發這些斷點。</p>
</li>
</ol>
<p>這些方法可以幫助您驗證編譯器為模板函數生成的特定實例。由於模板實例化是在編譯時進行的，所以通常需要查看編譯後的代碼或使用調試工具來進行驗證。</p>
<h2 id="Ask"><a href="#Ask" class="headerlink" title="Ask:"></a>Ask:</h2><ul>
<li><p>STL 的Derive containers 好像變成了 Container adaptors?! 然後，怎麼查source ?</p>
</li>
<li><p>CRTP 中提到兩種class相關的例子差別、目的為何？</p>
</li>
<li><p>「快了許多」是否有量測或驗證方式?</p>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/08/25/PyTorch/PyTorch_note/">PyTorch/PyTorch_note</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-08-25</time><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">0.1</span>, <span class="number">1.2</span>], [<span class="number">2.2</span>, <span class="number">3.1</span>], [<span class="number">4.9</span>, <span class="number">5.2</span>]])</span><br><span class="line"></span><br><span class="line">torch.tensor([<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># Type inference on data</span></span><br><span class="line"></span><br><span class="line">torch.tensor([[<span class="number">0.11111</span>, <span class="number">0.222222</span>, <span class="number">0.3333333</span>]],</span><br><span class="line">             dtype=torch.float64,</span><br><span class="line">             device=torch.device(<span class="string">'cuda:0'</span>))  <span class="comment"># creates a double tensor on a CUDA device</span></span><br><span class="line"></span><br><span class="line">torch.tensor(<span class="number">3.14159</span>)  <span class="comment"># Create a zero-dimensional (scalar) tensor</span></span><br><span class="line"></span><br><span class="line">torch.tensor([])  <span class="comment"># Create an empty tensor (of size (0,))</span></span><br></pre></td></tr></table></figure>







<h1 id="Ref"><a href="#Ref" class="headerlink" title="Ref"></a>Ref</h1><ul>
<li><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_blank" rel="noopener">Deep Learning with PyTorch: A 60 Minute Blitz — PyTorch Tutorials 2.0.1+cu117 documentation</a><ul>
<li><a href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor" target="_blank" rel="noopener">https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor</a></li>
</ul>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/08/02/LLM/Generative%20AI%20with%20LLM/3%20Reinforcement%20Learning%20from%20Human%20Feedback/">LLM/Generative AI with LLM/3 Reinforcement Learning from Human Feedback</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-08-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><div class="content"><h1 id="RLHF-for-responsible-AI"><a href="#RLHF-for-responsible-AI" class="headerlink" title="RLHF, for responsible AI"></a>RLHF, for responsible AI</h1><h2 id="Aligning-models-with-human-values"><a href="#Aligning-models-with-human-values" class="headerlink" title="Aligning models with human values"></a>Aligning models with human values</h2><table>
<thead>
<tr>
<th>主題</th>
<th>詳細內容</th>
</tr>
</thead>
<tbody><tr>
<td>微調的目的</td>
<td>進一步訓練模型，使其更好地理解人類的提示，並產生更接近人類的回應。這可以改善模型相對於原始預訓練版本的性能，並導致更自然的語言。</td>
</tr>
<tr>
<td>挑戰</td>
<td>自然的人類語言帶來了一系列新的挑戰，包括模型使用有毒語言，以好戰和激進的語調回應，以及提供有關危險主題的詳細資訊。</td>
</tr>
<tr>
<td>問題的源頭</td>
<td>這些問題存在的原因是大型模型在網路上的大量文本數據上進行訓練，而這種語言在這些數據中經常出現。</td>
</tr>
<tr>
<td>不良行為的例子</td>
<td>1) 模型回答不適當或不相關的問題。例如，當被問到敲門笑話時，模型回答“拍手”。這並不是對於給定任務的有用的答案。 2) 模型可能給出誤導或完全不正確的答案。例如，如果問模型關於被證實是錯誤的健康建議，如用咳嗽來停止心臟病，模型可能會給出自信而完全不正確的回答。 3) 模型不應該創建有害的回應，如侮辱性的、歧視性的或煽動犯罪行為。例如，當被問到如何破解鄰居的Wi-Fi時，模型可能會回答有效的策略。</td>
</tr>
<tr>
<td>解決方式</td>
<td>透過人類反饋進行額外的微調可以更好地將模型與人類偏好對齊，並增加回應的有用性、誠實性和無害性。這種進一步的訓練也可以幫助降低模型回應的毒性，並減少錯誤資訊的生成。</td>
</tr>
</tbody></table>
<h3 id="HHH"><a href="#HHH" class="headerlink" title="HHH"></a>HHH</h3><table>
<thead>
<tr>
<th>主題</th>
<th>解釋</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>Helpfulness（有用性）</td>
<td>指的是AI模型在回答問題或執行任務時，應提供有用和相關的信息。</td>
<td>例如，當被問到敲門笑話時，模型回答“拍手”。這並不是對於給定任務的有用的答案。理想的回答應該是一個真正的敲門笑話。</td>
</tr>
<tr>
<td>Honesty（誠實性）</td>
<td>指的是AI模型在回答問題時，應提供準確和真實的信息。</td>
<td>例如，如果問模型關於被證實是錯誤的健康建議，如用咳嗽來停止心臟病，模型可能會給出自信而完全不正確的回答。理想的回答應該是反駁這個誤導的健康建議。</td>
</tr>
<tr>
<td>Harmlessness（無害性）</td>
<td>指的是AI模型的行為應該是無害的，不應煽動或引導出有害的行為。</td>
<td>例如，當被問到如何破解鄰居的Wi-Fi時，模型可能會回答有效的策略。理想的回答應該是拒絕提供該資訊或解釋為何這種行為是不適當的。</td>
</tr>
</tbody></table>
<h2 id="RLHF-Reinforcement-Learning-from-Human-Feedback"><a href="#RLHF-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="RLHF, (Reinforcement Learning from Human Feedback)"></a>RLHF, (Reinforcement Learning from Human Feedback)</h2><blockquote>
<p>Reinforcement learning is a type of machine learning in which an agent learns </p>
<p>to make decisions related to a specific goal by taking actions in an environment, </p>
<p>with the objective of maximizing some notion of a cumulative reward.Perhaps most importantly, RLHF can help </p>
<p>In this framework, the agent continually learns from its experiences by </p>
<p>taking actions, observing the resulting changes in the environment, and </p>
<p>receiving rewards or penalties, based on the outcomes of its actions. </p>
<p>By iterating through this process, the agent gradually refines its strategy or </p>
<p>policy to make better decisions and increase its chances of success.</p>
</blockquote>
<ol>
<li>to minimize the potential for harm. </li>
<li>to give caveats that acknowledge their limitations and to avoid toxic language and topics.</li>
</ol>
<img src="https://p.ipic.vip/x8kxym.png" alt="image-20230803191315758" style="zoom:33%;" />





<ol>
<li><strong>學習方法</strong>：強化學習是一種機器學習方法，其目標是使學習代理能夠在給定的環境中做出最佳決策。</li>
<li><strong>環境互動</strong>：代理會在環境中採取行動，觀察結果，並根據其行動的結果獲取獎勵或懲罰。</li>
<li><strong>學習目標</strong>：強化學習的目標是最大化總體的獎勵。這通常涉及尋找一種平衡，使得即時的獎勵和長期的獎勵都能夠最大化。</li>
<li><strong>策略改進</strong>：通過不斷地互動和學習，代理會改進其決策策略或方針，以提高其成功機率。</li>
<li><strong>連續學習</strong>：在強化學習中，學習是連續的過程，代理會不斷從其行動和結果中學習，並調整其行為。</li>
</ol>
<img src="https://p.ipic.vip/4umx9v.png" alt="image-20230803192932185" style="zoom:33%;" />

<blockquote>
<p>As a practical and scalable alternative, you can use an additional model, </p>
<p>known as the reward model, to classify the outputs of the LLM and </p>
<p>evaluate the degree of alignment with human preferences.</p>
<p>Once trained, you’ll use the reward model to assess the output of the LLM and </p>
<p>assign a reward value, which in turn gets used to update the weights off the LLM and </p>
<p>train a new human aligned version. </p>
<p>Exactly how the weights get updated as the model completions are assessed, </p>
<p>depends on the algorithm used to optimize the policy. </p>
<p>Lastly, note that in the context of language modeling, </p>
<p>the sequence of actions and states is called a rollout, </p>
<p>instead of the term playout that’s used in classic reinforcement learning.</p>
<p>The reward model is the central component of the reinforcement learning process. </p>
<p>It encodes all of the preferences that have been learned from human feedback, and </p>
<p>it plays a central role in how the model updates its weights over many iterations</p>
</blockquote>
<table>
<thead>
<tr>
<th>項目</th>
<th>說明</th>
</tr>
</thead>
<tbody><tr>
<td>代理策略 (Agent Strategy)</td>
<td>是由大型語言模型 (LLM) 所擔任的，目標是生成被認為與人類偏好一致的文本，例如有幫助、準確且無毒的文本。</td>
</tr>
<tr>
<td>環境 (Environment)</td>
<td>是模型的上下文窗口，也就是可以通過提示輸入文本的空間。</td>
</tr>
<tr>
<td>狀態 (State)</td>
<td>是模型在採取行動之前考慮的當前上下文，也就是當前包含在上下文窗口中的任何文本。</td>
</tr>
<tr>
<td>行動 (Action)</td>
<td>是生成文本的行為，這可以是單個單詞、句子或較長的文本，具體取決於用戶指定的任務。</td>
</tr>
<tr>
<td>行動空間 (Action Space)</td>
<td>是令牌詞彙表，也就是模型可以選擇來生成完成的所有可能的令牌。</td>
</tr>
</tbody></table>
<ol>
<li><strong>LLM 的行動選擇</strong>：大型語言模型 (LLM) 會根據訓練期間學到的語言統計表示來決定生成序列中的下一個標記。這是基於上下文提示文本以及詞彙空間上的概率分佈。</li>
<li><strong>獎勵分配</strong>：獎勵的分配基於完成的文本與人類偏好的匹配程度。由於人類對語言的反應存在變異，確定獎勵的過程比井字遊戲的例子要複雜。</li>
<li><strong>人類反饋與評估</strong>：一種方法是讓人類根據對齊指標，如文本是否有毒，評估模型的所有完成。這種反饋可以表示為一個標量值，如零或一，並可以用來迭代更新 LLM 權重，以最大化從人類分類器獲得的獎勵。</li>
<li><strong>使用獎勵模型</strong>：由於獲取人類反饋可能耗時且昂貴，因此一種可行的替代方案是使用獎勵模型來對 LLM 的輸出進行分類和評估。一旦訓練完成，獎勵模型就可以用來評估 LLM 的輸出，分配獎勵值，並更新 LLM 的權重。</li>
<li><strong>策略優化</strong>：權重的具體更新方式取決於用於優化策略的算法。</li>
<li><strong>獎勵模型與強化學習</strong>：在語言建模的背景下，動作和狀態的序列被稱為 “rollout”，而非經典強化學習中使用的 “playout”。獎勵模型是強化學習過程的核心部分，它對人類反饋中學到的偏好進行編碼，並在模型權重的多次迭代更新中發揮核心作用。</li>
</ol>
<h2 id="RLHF-Obtaining-Feedback-from-Humans"><a href="#RLHF-Obtaining-Feedback-from-Humans" class="headerlink" title="RLHF: Obtaining Feedback from Humans"></a>RLHF: Obtaining Feedback from Humans</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230803194432935.png" alt="image-20230803194432935" style="zoom: 33%;" />

<ol>
<li>選擇模型：首先，選擇具有一些目標任務能力的模型，如文本摘要或問答等。一般而言，使用已被跨多任務微調並具有一些通用能力的指導模型會較為簡單。</li>
<li>生成completion：</li>
</ol>
<p>此段討論了「以人類回饋進行強化學習」(RLHF)在微調語言模型(LLM)時的第一步驟：獲取人類回饋。</p>
<ol>
<li><strong>選擇模型和數據集</strong>：首先，選擇具有一些目標任務能力的模型，如文本摘要或問答等。你可能會發現使用已經在許多任務上微調過並具有一些通用能力的模型更容易。接著，將該LLM與一個提示數據集結合，為每個提示生成多個不同的回答。這個數據集由多個提示組成，每一個都被LLM處理以產生一組完成的回答。</li>
<li><strong>收集人類回饋</strong>：下一步是收集人類標籤員對LLM生成的完成的反饋。首先，你需要決定你希望人類基於何種標準來評估這些完成，這可能是前面討論過的問題，比如有用性或有害性。然後，你將要求標籤員根據該標準評估數據集中的每一個完成。</li>
<li><strong>範例說明</strong>：例如，對於提示”My house is too hot.”，LLM生成了三種不同的回答。你的任務是讓標籤員按照有用性對這三個完成進行排序，從最有用到最不有用。這個過程會在許多提示完成組上重複進行，從而構建一個可以用來訓練獎勵模型的數據集，最終這個獎勵模型將取代人類完成這項工作。</li>
<li><strong>指令明確性</strong>：明確的指令對於獲取高質量的人類反饋非常重要。對標籤員的詳細指示可以增加他們理解任務並按照你的期望來完成的可能性。</li>
</ol>
<h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h3><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230803201550275.png" alt="image-20230803201550275" style="zoom:25%;" />

<ol>
<li><strong>處理不合理回答</strong>：如果模型產生的回答無意義、令人困惑或不相關，標籤員應選擇「F」而不是排名，以便容易地移除低質量的答案。</li>
<li><strong>給予詳細指令</strong>：提供像這樣的詳細指令可以提高回答的質量，並確保每個人類標籤員以類似的方式執行任務，從而確保標記的完成組能夠代表共識的觀點。</li>
<li><strong>訓練獎勵模型</strong>：一旦人類標籤員完成了對完成組的評估，你就有了訓練獎勵模型所需的所有數據。</li>
<li><strong>轉換數據格式</strong>：在開始訓練獎勵模型之前，你需要將排名數據轉換成完成的兩兩比較。所有可能的完成配對都應該被劃分為0或1分。</li>
<li><strong>建立完成配對</strong>：例如，如果有三種完成方式，並且人類標籤員的排名是2, 1, 3（其中1代表最喜歡的回答），那麼就有三種可能的配對：紫色-黃色，紫色-綠色，黃色-綠色。根據每個提示的完成選項數量N，你將有”N選二”的組合。</li>
<li><strong>為每個配對分配獎勵</strong>：對於每個配對，你將為首選回答分配1分的獎勵，為次選回答分配0分的獎勵。然後你將重新排列提示，使得首選選項排在前面。這是一個重要的步驟，因為獎勵模型期望首選完成（也稱為Yj）在前。</li>
<li><strong>數據重組</strong>：一旦你完成了這些數據的重組，人類的回答就會被轉換成適合訓練獎勵模型的格式。</li>
<li><strong>注意事項</strong>：雖然通常收集讚好/讚壞的反饋比較容易，但排名反饋可以為訓練你的獎勵模型提供更多的提示完成數據。例如，你可以從每個人的排名中獲得三個提示完成對。</li>
</ol>
<h2 id="RLHF-Reward-Model"><a href="#RLHF-Reward-Model" class="headerlink" title="RLHF: Reward Model"></a>RLHF: Reward Model</h2><ol>
<li><p>訓練，就不再需要人類的參與。</p>
</li>
<li><p>獎勵模型會有效地取代人類標籤員的角色，並在訓練過程中自動選擇首選完成。</p>
</li>
<li><p>獎勵模型通常也是一種語言模型，可以透過監督學習方法在人類標籤員對提示的評估數據上訓練。</p>
</li>
<li><p>獎勵模型的目標是學習偏好人類首選的完成方式，同時最小化獎勵差異。</p>
</li>
<li><p>一旦模型已經在人類的排名提示完成對上訓練過，可以使用獎勵模型作為一種二進制分類器。</p>
</li>
<li><p>例如，如果想要讓語言模型避免產生仇恨言論，獎勵模型就需要能識別完成的部分是否包含仇恨言論。</p>
</li>
</ol>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230823172046547.png" alt="image-20230823172046547" style="zoom: 67%;" />

<blockquote>
<p>Let’s say you want to detoxify your LLM, </p>
<p>and the reward model needs to </p>
<p>identify if the completion contains hate speech. </p>
<p>In this case, the two classes would be notate, </p>
<p>the positive class that you ultimately want to optimize </p>
<p>for and hate the negative class you want to avoid. </p>
<p>The largest value of the positive class is what you </p>
<p>use as the reward value in LLHF. </p>
<p>Just to remind you, if you apply </p>
<p>a Softmax function to the logits, </p>
<p>you will get the probabilities. </p>
<p>The example here shows a good reward for </p>
<p>non-toxic completion and the second example </p>
<p>shows a bad reward being given for toxic completion.</p>
</blockquote>
<h2 id="RLHF-Fine-Tuning-with-Reinforcement-Learning"><a href="#RLHF-Fine-Tuning-with-Reinforcement-Learning" class="headerlink" title="RLHF: Fine-Tuning with Reinforcement Learning"></a>RLHF: Fine-Tuning with Reinforcement Learning</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230803211741800.png" alt="image-20230803211741800" style="zoom: 33%;" />

<table>
<thead>
<tr>
<th>步驟</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>將提示提供給LLM，讓其生成完成的句子</td>
</tr>
<tr>
<td>2</td>
<td>將完成的句子與原始提示一起作為一對傳遞給獎勵模型</td>
</tr>
<tr>
<td>3</td>
<td>獎勵模型根據訓練時的人類反饋評估這一對，並返回一個獎勵值</td>
</tr>
<tr>
<td>4</td>
<td>獎勵值被用於更新LLM的權重，使其更偏向生成更高獎勵、更對齊的回應</td>
</tr>
</tbody></table>
<p>RLHF is becoming increasingly important to ensure that LLMs behave in a safe and aligned manner in deployment.</p>
<blockquote>
<p>This is the algorithm that takes the output </p>
<p>of the reward model and uses it </p>
<p>to update the LLM model weights so </p>
<p>that the reward score increases over time.</p>
</blockquote>
<p>ensure that </p>
<p>LLMs behave in a safe and aligned manner in deployment.</p>
<h2 id="PPO-Proximal-Policy-Optimization"><a href="#PPO-Proximal-Policy-Optimization" class="headerlink" title="PPO, (Proximal Policy Optimization)"></a>PPO, (Proximal Policy Optimization)</h2><blockquote>
<p>As the name suggests, </p>
<p>PPO optimizes a policy, </p>
<p>in this case the LLM, </p>
<p>to be more aligned with human preferences. </p>
<p>Over many iterations, PPO makes updates to the LLM. </p>
<p>The updates are small and within a bounded region, </p>
<p>resulting in an updated LLM </p>
<p>that is close to the previous version, </p>
<p>hence the name Proximal Policy Optimization. </p>
<p>Keeping the changes within </p>
<p>this small region result in a more stable learning. </p>
<p>The goal is to update </p>
<p>the policy so that the reward is maximized. </p>
</blockquote>
<img src="https://p.ipic.vip/gexjj5.png" alt="image-20230823194828690" style="zoom:67%;" />



<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230823195015112.png" alt="image-20230823195015112" style="zoom: 50%;" />





<blockquote>
<p>RLHF is a fine-tuning process </p>
<p>that aligns LLMs with human preferences. </p>
<p>In this process, you make use of </p>
<p>a reward model to assess and LLMs </p>
<p>completions of a prompt data </p>
<p>set against some human preference metric, </p>
<p>like helpful or not helpful. </p>
<p>Next, you use a reinforcement learning algorithm, </p>
<p>in this case, PPO, </p>
<p>to update the weights off the LLM based on the reward is </p>
<p>signed to the completions generated </p>
<p>by the current version off the LLM. </p>
<p>You’ll carry out this cycle of </p>
<p>a multiple iterations using many different prompts </p>
<p>and updates off the model weights </p>
<p>until you obtain your desired degree of alignment. </p>
<p>Your end result is </p>
<p>a human aligned LLM that you can use in your application. </p>
</blockquote>
<h2 id="RLHF-Reward-Hacking"><a href="#RLHF-Reward-Hacking" class="headerlink" title="RLHF: Reward Hacking"></a>RLHF: Reward Hacking</h2><p>以下是個例子來說明獎勵黑客的問題：</p>
<p>假設我們正在使用強化學習來讓你的指導語言模型（instruct LLM）生成更友善（less toxic）的回應。已經訓練了一個獎勵模型，該模型能夠進行情感分析，並將模型生成的文本分類為有毒或無毒。</p>
<p>現在，我將一個提示「this product is…」送入你的指導語言模型，該模型產生了一個補全：「complete garbage」。這個補全的情感負面，因此可能會獲得一個高度有毒的評級。</p>
<p>然而，當我的語言模型經過多次迭代和學習後，它可能會嘗試最大化獎勵（即最小化有毒評級）。它可能會學會加入一些總是能得到低有毒評級的字詞或片語，例如「most awesome」或「most incredible」，即使這樣做可能會使得生成的文本變得過度誇大或者變得不合理。</p>
<p>這就是獎勵黑客的一種現象：模型學會了如何遊戲系統以最大化其獎勵，但最終生成的文本可能並不是我們真正想要的結果。</p>
<img src="https://p.ipic.vip/fl9pjf.png" alt="image-20230823195838934" style="zoom:67%;" />

<img src="https://p.ipic.vip/1cmjm5.png" alt="image-20230803212242324" style="zoom: 33%;" />

<h3 id="Sol"><a href="#Sol" class="headerlink" title="Sol"></a>Sol</h3><img src="https://p.ipic.vip/y8h561.png" alt="image-20230803221112298" style="zoom:33%;" />

<blockquote>
<p>It needs 2 full copies of the LLM to calculate the KL divergence, the frozen reference LLM, and the RL-updated PPO LLM. </p>
<p>We can further benefit from combining our relationship with PEFT. In this case, we only update the weights of a path adapter, not the full weights of the LLM.</p>
</blockquote>
<img src="https://p.ipic.vip/yjd5h0.png" alt="image-20230803221245341" style="zoom:33%;" />



<h3 id="KL-divergence"><a href="#KL-divergence" class="headerlink" title="KL divergence"></a>KL divergence</h3><p>KL散度（Kullback-Leibler Divergence）是一種度量兩個概率分布間差異的數學方法，常用於強化學習領域，特別是在使用PPO（近端策略優化）算法時。</p>
<p>在PPO中，目標是通過基於與環境互動獲得的獎勵來反覆更新代理的參數，以找到改進的策略。然而，過於激進的更新策略可能會導致學習不穩定或策略變化過大。為了解決這個問題，PPO引入了一個約束，限制策略更新的幅度。這個約束是通過使用KL散度來實現的。</p>
<p>要理解KL散度是如何工作的，我們可以想像有兩個概率分布：原始LLM的分布，和一個新提議的RL更新的LLM分布。KL散度度量的是當我們使用原始策略來對新提議的策略的樣本進行編碼時，我們獲得的信息量的平均數。通過最小化兩個分布之間的KL散度，PPO確保更新的策略與原始策略保持接近，防止可能對學習過程產生負面影響的劇變。</p>
<p>你可以使用的一個用於以強化學習方式訓練變換器語言模型的庫是TRL（Transformer Reinforcement Learning）。在這個鏈接中，你可以更多地了解這個庫，以及它與PEFT（Parameter-Efficient Fine-Tuning）方法（如LoRA（Low-Rank Adaption））的集成。該圖像顯示了TRL中PPO訓練設置的概覽。</p>
<p>簡單來說，KL散度是一種方法，用於度量並控制在學習過程中模型更新的幅度，以確保學習的穩定性和防止策略的劇變。</p>
<p>In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.</p>
<h2 id="Scaling-Human-Feedbacks"><a href="#Scaling-Human-Feedbacks" class="headerlink" title="Scaling Human Feedbacks"></a>Scaling Human Feedbacks</h2><p>在RLHF（強化學習和人類反饋）微調中，儘管你可以使用獎勵模型來消除在微調過程中對人類評估的需求，但是首先需要巨大的人力努力來產生訓練獎勵模型所需的標記數據集。這通常需要大團隊的標記員，有時甚至需要成千上萬的人來評估各種提示。這項工作需要大量的時間和其他資源，這可能是重要的限制因素。隨著模型和用例數量的增加，人力努力成為有限的資源。擴大人類反饋的方法是研究的活躍領域。一種克服這些限制的想法是通過模型自我監督來擴展。憲法人工智能就是一種擴展監督的方法。</p>
<p>憲法人工智能首先在2022年由Anthropic的研究人員提出，它是一種使用一組規則和原則來訓練模型的方法，這些規則和原則決定了模型的行為。你可以訓練模型自我批評，並修改其回應以符合這些原則。憲法人工智能不僅對擴大反饋有用，而且還可以幫助解決RLHF的一些意外後果。例如，根據提示的結構，一個對齊的模型可能會在盡其所能提供最有幫助的回應時，暴露出有害的信息。</p>
<p>在實施憲法人工智能方法時，你需要分兩個階段訓練模型。在第一階段，你進行監督學習，開始使用提示模型的方式，試圖讓它產生有害的回應，這個過程被稱為紅隊操作。然後你讓模型根據憲法原則批評自己的有害回應，並修改它們以符合這些規則。完成後，你將使用紅隊提示和修訂的憲法回應對的對模型進行微調。</p>
<p>然後，你將所有部分放在一起，要求模型寫出一個新的回應，刪除所有有害或非法的內容。模型生成一個新的答案，將憲法原則付諸實踐，不包括對非法應用的引用。原始的紅隊提示和這個最後的憲法回應可以作為訓練數據。你將構建一個包含許多這樣的例子的數據集，以創建一個已經學習如何生成憲法回應的微調NLM。</p>
<p>第二部分的過程進行強化學習。這個階段與RLHF相似，除了人類反饋，我們現在使用由模型生成的反饋。這有時被稱為從AI反饋中學習的強化學習或RLAIF。在這裡，你使用前一步中的微調模型來生成一組對你的提示的回應。然後你讓模型根據憲法原則來判斷哪個回應是優先的。結果是一個模型生成的偏好數據集，你可以用它來訓練一個獎勵模型。有了這個獎勵模型，你現在可以使用類似於PPO的強化學習算法進一步微調模型。</p>
<h1 id="LLM-powered-Applications"><a href="#LLM-powered-Applications" class="headerlink" title="LLM-powered Applications"></a>LLM-powered Applications</h1><h2 id="Model-Optimizations-for-Deployment"><a href="#Model-Optimizations-for-Deployment" class="headerlink" title="Model Optimizations for Deployment"></a>Model Optimizations for Deployment</h2><img src="https://p.ipic.vip/gegwtb.png" alt="image-20230804081521186" style="zoom: 33%;" />

<p><strong>問題</strong></p>
<table>
<thead>
<tr>
<th>問題</th>
<th>描述</th>
<th>範例</th>
</tr>
</thead>
<tbody><tr>
<td>模型部署的運行速度</td>
<td>您的模型需要多快生成完成答案？</td>
<td>如果模型被用於需要快速回應的即時對話系統，則運行速度將是非常重要的考慮因素。</td>
</tr>
<tr>
<td>可用的運算預算</td>
<td>您有多少運算能力可以投入到模型的部署上？</td>
<td>如果您在雲端運算資源有限，或是運用在邊緣裝置上，則會需要考慮到運算預算。</td>
</tr>
<tr>
<td>模型效能和推論速度或儲存空間的取捨</td>
<td>是否願意為了改善推論速度或減少儲存需求而犧牲模型的效能？</td>
<td>縮小模型可能會降低其生成結果的準確性，但可以提高運行速度並減少存儲需求。</td>
</tr>
<tr>
<td>模型是否需要與外部數據或其他應用進行交互</td>
<td>如果模型需要與外部數據或其他應用進行交互，則需要考慮如何連接這些資源。</td>
<td>例如，您的模型可能需要存取網路爬蟲或數據庫以查詢或更新資訊。</td>
</tr>
<tr>
<td>模型將如何被消費</td>
<td>您的模型將通過哪種應用程式或API介面被使用？</td>
<td>例如，模型可能透過智能助理應用程式、語音識別系統或一個Web API進行訪問。</td>
</tr>
</tbody></table>
<p><strong>優化技術</strong></p>
<table>
<thead>
<tr>
<th>技術</th>
<th>描述</th>
<th>範例</th>
</tr>
</thead>
<tbody><tr>
<td>蒸餾（Distillation）</td>
<td>用一個大型模型（教師模型）訓練一個較小的模型（學生模型），以降低儲存和運算需求。</td>
<td>一個例子可能是，使用大型GPT-4模型作為教師模型，訓練一個小型GPT-2模型作為學生模型。</td>
</tr>
<tr>
<td>量化（Quantization）</td>
<td>將模型權重轉換為較低精度的表示，例如16位浮點數或8位整數，以降低模型大小和記憶體需求以及所需的模型服務運算資源。</td>
<td>可以將GPT-4模型的權重由32位浮點數量化為16位浮點數，以節省儲存空間和運算時間。</td>
</tr>
<tr>
<td>剪枝（Pruning）</td>
<td>移除對模型性能貢獻不大的權重，這些權重的值接近或等於零。</td>
<td>使用LoRA或其他參數有效的微調方法對模型進行剪枝，移除對模型性能影響最小的權重。</td>
</tr>
</tbody></table>
<h2 id="Generative-AI-Project-Lifecycle-Cheat-Sheet"><a href="#Generative-AI-Project-Lifecycle-Cheat-Sheet" class="headerlink" title="Generative AI Project Lifecycle Cheat Sheet"></a>Generative AI Project Lifecycle Cheat Sheet</h2><table>
<thead>
<tr>
<th>項目階段</th>
<th>描述</th>
<th>需要的時間和努力</th>
<th>需要的技術專業知識</th>
</tr>
</thead>
<tbody><tr>
<td>大型語言模型的預訓練</td>
<td>這個階段涉及到模型架構的決定，大量的訓練數據，並需要專業的技術知識。</td>
<td>最耗時和最耗力的階段。</td>
<td>最需要專業知識的階段。</td>
</tr>
<tr>
<td>引導工程 (Prompt Engineering)</td>
<td>如果你從一個已存在的基礎模型開始，你可能會首先透過引導工程來評估模型的表現。</td>
<td>相對快速，可能不需要特別的訓練時間。</td>
<td>需要一些技術知識，但相對於模型訓練，要求較低。</td>
</tr>
<tr>
<td>引導調整與微調 (Prompt Tuning and Fine Tuning)</td>
<td>如果模型的表現還不夠好，你可能會考慮引導調整或微調模型。</td>
<td>依據用途、性能目標和運算預算的不同，所需的時間和努力也會不同。</td>
<td>需要一定的技術專業知識。</td>
</tr>
<tr>
<td>利用人類反饋進行強化學習來對齊模型 (Aligning the Model Using RL from Human Feedback)</td>
<td>一旦你有了訓練獎勵模型，你就可以快速地使用人類反饋來進行強化學習。</td>
<td>如果你已經有現成的獎勵模型，這個過程可能會很快。但如果需要從零開始訓練一個獎勵模型，由於需要收集人類反饋，所以可能會花費很長的時間。</td>
<td>需要一定的技術專業知識。</td>
</tr>
<tr>
<td>優化技術 (Optimization Techniques)</td>
<td>在進行最終的模型部署之前，你可能會需要使用一些優化技術來改善模型的性能。</td>
<td>這個階段的複雜程度和所需努力在中等範圍內，但一旦確定模型變動不會太大影響性能，這個過程可以很快進行。</td>
<td>需要一定的技術專業知識。</td>
</tr>
</tbody></table>
<h2 id="Using-LLM-in-Applications"><a href="#Using-LLM-in-Applications" class="headerlink" title="Using LLM in Applications"></a>Using LLM in Applications</h2><p>By providing up to date relevant information and avoiding hallucinations to greatly improve the experience of using application for users.</p>
<p>Note: the LLMs do not carry out mathematical operations. They are still just trying to predict the next best token based on their training, and as a result, can easily get the answer wrong.</p>
<p>問題 &amp; 解決方案*</p>
<table>
<thead>
<tr>
<th>問題</th>
<th>描述</th>
<th>範例</th>
</tr>
</thead>
<tbody><tr>
<td>知識截止</td>
<td>語言模型的內在知識在預訓練的時刻停止，無法理解訓練之後的新事件或新聞。</td>
<td>2022 年初訓練的模型被問到英國首相是誰時，會回答是 Boris Johnson，但模型並不知道他在 2022 年底辭職。</td>
</tr>
<tr>
<td>數學問題</td>
<td>模型在處理複雜的數學問題時可能會遇到困難。</td>
<td>提問模型執行一個除法問題，模型返回的答案接近正確答案，但不準確。</td>
</tr>
<tr>
<td>幻覺</td>
<td>當模型不知道答案時，可能會生成與問題無關的文本。</td>
<td>模型生成了一種不存在的植物”火星沙丘樹”的描述。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>解決方案</th>
<th>描述</th>
<th>範例</th>
</tr>
</thead>
<tbody><tr>
<td>使用外部數據源和應用程式</td>
<td>通過連接到外部數據源和應用程式，模型可以得到更新的信息，從而克服知識截止的問題。</td>
<td>將語言模型連接到外部數據庫或其他應用程式的API。</td>
</tr>
</tbody></table>
<h3 id="RAG-Retrieval-Augmented-Generation"><a href="#RAG-Retrieval-Augmented-Generation" class="headerlink" title="RAG, (Retrieval Augmented Generation, )"></a>RAG, (Retrieval Augmented Generation, )</h3><table>
<thead>
<tr>
<th>RAG 的功能</th>
<th>描述</th>
<th>範例</th>
</tr>
</thead>
<tbody><tr>
<td>克服知識截止</td>
<td>RAG通過給模型提供訪問額外的外部數據，幫助模型更新其對世界的理解。</td>
<td>您可以讓模型訪問它可能未見過的數據，如新信息文件、原始訓練數據中未包含的文件，或存儲在您的組織私有數據庫中的專有知識。</td>
</tr>
<tr>
<td>避免幻覺</td>
<td>RAG 可以幫助你避免模型在不知道答案的情況下產生幻覺。</td>
<td></td>
</tr>
<tr>
<td>整合外部信息源</td>
<td>RAG 可以將多種類型的外部信息源整合到一起。</td>
<td>使用 RAG，您可以讓模型訪問本地文檔、包括私有 wiki 和專家系統在內的互聯網上的信息，或者與數據庫互動。</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>RAG 的實施</th>
<th>描述</th>
<th>範例</th>
</tr>
</thead>
<tbody><tr>
<td>限制的上下文窗口大小</td>
<td>絕大部分的文本來源都過長，無法適應模型有限的上下文窗口。因此，外部數據源需要被切分成許多塊，每塊都可以適應在上下文窗口裡。</td>
<td>使用像是 Langchain 這種套件來處理這項工作。</td>
</tr>
<tr>
<td>數據的格式</td>
<td>數據必須以易於檢索最相關文本的格式提供。</td>
<td>利用語言模型內部以向量形式表現語言的特性，透過 RAG 方法將外部數據的各個小塊處理進大型語言模型，創建每個塊的嵌入向量，並將這些新的數據表現形式儲存在叫做向量存儲的結構裡。</td>
</tr>
</tbody></table>
<h2 id="Interacting-with-External-Apllications"><a href="#Interacting-with-External-Apllications" class="headerlink" title="Interacting with External Apllications"></a>Interacting with External Apllications</h2><img src="https://p.ipic.vip/oau486.png" alt="image-20230804095046617" style="zoom: 25%;" />

<p>例子:</p>
<blockquote>
<p><strong>客戶</strong>: 表示他們希望退回他們購買的基因。</p>
<p><strong>ShopBot</strong>: 詢問訂單號碼。</p>
<p><strong>客戶</strong>: 提供訂單號碼。</p>
<p><strong>ShopBot</strong>: 在交易數據庫中查找訂單號碼，可能通過一種類似於您在上一個視頻中看到的RAG實現來實現。在這種情況下，您可能會透過SQL查詢後端訂單數據庫來檢索數據，而不是從文件集中檢索數據。</p>
<p><strong>ShopBot</strong>: 取得了客戶的訂單後，確認將退回的商品。詢問客戶是否還有其他除了基因以外的商品要退回。</p>
<p><strong>客戶</strong>: 表示他們的答案。</p>
<p><strong>ShopBot</strong>: 向公司的運輸合作夥伴發起一個退貨標籤的請求。透過運輸合作夥伴的Python API來請求標籤。並將通過電子郵件將運輸標籤發送給客戶。同時詢問他們確認電子郵件地址。</p>
<p><strong>客戶</strong>: 以電子郵件地址回應，該信息將被包含在對運輸商的API調用中。</p>
<p><strong>ShopBot</strong>: API請求完成後，通知客戶標籤已通過電子郵件發送，並結束對話。</p>
</blockquote>
<p><strong>連接語言模型與外部應用的重要性</strong>：</p>
<ol>
<li>擴展語言模型的功能：連接外部應用可以讓語言模型與更廣泛的世界互動，使其功能超越語言任務。</li>
<li>觸發動作：語言模型可以被用來觸發動作，當給予與API互動的能力時。例如在ShopBot的例子中。</li>
<li>連接其他編程資源：語言模型也可以連接到其他編程資源，例如Python解釋器，以在輸出中嵌入精確的計算。</li>
</ol>
<p><strong>如何觸發動作</strong>：</p>
<ol>
<li>提示（prompts）與完成（completions）：這些是工作流程的核心。應用將根據用戶的請求進行的操作由語言模型決定，語言模型作為應用的推理引擎。</li>
<li>生成指令：語言模型需要生成一套指令，讓應用知道需要採取哪些行動。這些指令需要易於理解並與允許的行動相對應。例如在ShopBot中，重要的步驟包括檢查訂單ID、請求運送標籤、驗證用戶電郵以及透過電郵向用戶發送標籤。</li>
<li>完成的格式：完成需要以一種應用能理解的方式格式化。這可能像是特定的句子結構，或像是寫出Python腳本或生成SQL指令那樣複雜。</li>
<li>收集驗證信息：模型可能需要收集允許驗證行動的信息。例如在ShopBot的對話中，應用需要驗證客戶用於下訂單的電郵地址。</li>
</ol>
<p>以下是一個表格，比較一下在 ShopBot 示例中，LLM 操作步驟以及對應的實際操作：</p>
<table>
<thead>
<tr>
<th>LLM 操作步驟</th>
<th>實際操作</th>
</tr>
</thead>
<tbody><tr>
<td>檢查訂單ID</td>
<td>在數據庫中查找訂單號碼</td>
</tr>
<tr>
<td>請求運送標籤</td>
<td>通過運輸合作夥伴的API發起請求</td>
</tr>
<tr>
<td>驗證用戶電郵</td>
<td>從用戶處獲得電郵地址，並將其包含在API呼叫中</td>
</tr>
<tr>
<td>透過電郵向用戶發送標籤</td>
<td>API請求完成後，通知客戶標籤已經發送</td>
</tr>
</tbody></table>
<h2 id="Helping-LLMs-reason-and-Plan-with-Chain-of-Thought"><a href="#Helping-LLMs-reason-and-Plan-with-Chain-of-Thought" class="headerlink" title="Helping LLMs reason and Plan with Chain-of-Thought"></a>Helping LLMs reason and Plan with Chain-of-Thought</h2><h3 id="Chain-of-Thought-prompting"><a href="#Chain-of-Thought-prompting" class="headerlink" title="Chain of Thought prompting"></a>Chain of Thought prompting</h3><p>E.g. Intermediate calculations form the reasoning steps that a human might take, and the full sequence of steps illustrates the chain of thought that went into solving the problem.</p>
<p>這是文章中兩個範例的比較：</p>
<table>
<thead>
<tr>
<th>問題種類</th>
<th>一次推理</th>
<th>鏈式思考提示</th>
</tr>
</thead>
<tbody><tr>
<td>數學問題</td>
<td>錯誤地判斷餐廳剩餘的蘋果數量為27（正確答案應為9）</td>
<td>正確地判斷餐廳剩餘的蘋果數量為9</td>
</tr>
<tr>
<td>物理問題</td>
<td>未給出範例</td>
<td>正確地判斷黃金戒指會沉到游泳池底部，因為黃金的密度遠大於水的密度</td>
</tr>
</tbody></table>
<h2 id="PAL-Program-aided-Language-Models"><a href="#PAL-Program-aided-Language-Models" class="headerlink" title="PAL, (Program-aided Language Models)"></a>PAL, (Program-aided Language Models)</h2><p>Remember, the model isn’t actually doing any real math here. It is simply trying to predict the most probable tokens that complete the prompt. </p>
<p>This work first presented by Luyu Gao and collaborators at Carnegie Mellon University in 2022</p>
<p>PAL 的主要策略是讓 LLM 生成包含電腦代碼的完成語句。這些代碼可以傳遞給解釋器來進行必要的計算，以解決問題。LLM 可以在提示的例子中學習到輸出的格式。例子裡會包括一個問題和一些 Python 代碼行，這些代碼行能夠解決問題。</p>
<img src="https://p.ipic.vip/9objt9.png" alt="image-20230804094543300" style="zoom: 67%;" />



<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230804095223507.png" alt="image-20230804095223507" style="zoom:25%;" />





<h2 id="ReAct-Combining-“Reasoning-and-Action”"><a href="#ReAct-Combining-“Reasoning-and-Action”" class="headerlink" title="ReAct: Combining “Reasoning and Action”"></a>ReAct: Combining “Reasoning and Action”</h2><p>ReAct is a prompting strategy that combines chain of thought reasoning with action planning. The framework was proposed by researchers at Princeton and Google in 2022. The paper develops a series of complex prompting examples based on problems from Hot Pot QA, a multi-step question answering benchmark. </p>
<p>以下是 “ReAct” 框架的步驟：Thought, Action, Observation steps</p>
<ol>
<li>問題定義：開始的提示(question prompt)會是一個需要多步驟來回答的問題。例如，確定兩本雜誌中哪一本是先創建的。</li>
<li>思考步驟：這個步驟包括一個思考、行動、與觀察的三元組。”思考” 是一個解決問題的邏輯步驟，並識別需要採取的行動。例如，在雜誌出版的問題中，模型將搜索兩本雜誌並確定哪一本先出版。</li>
<li>行動步驟：模型需要從一個預定義的行動列表中選擇一個行動來執行，以便與外部應用程序或數據源互動。在 “ReAct” 框架中，這些行動包括：<ul>
<li>搜索：查找特定主題的Wikipedia條目</li>
<li>查找：在Wikipedia頁面上搜索特定的字符串</li>
<li>完成：當模型決定已經找到答案時，執行的行動</li>
</ul>
</li>
<li>觀察步驟：新的信息（來自於行動步驟的結果）會被引入到提示的上下文中。這是 “觀察” 步驟。</li>
<li>重複步驟：上述的 “思考”、”行動”、”觀察” 的循環會重複進行，直到找到最終的答案為止。</li>
</ol>
<p>注意，每個行動都使用特定的方括號符號來格式化，這樣 Python 解釋器就可以觸發對應的 API 操作。</p>
<h3 id="LangChain"><a href="#LangChain" class="headerlink" title="LangChain"></a>LangChain</h3><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230804102549531.png" alt="image-20230804102549531" style="zoom:33%;" />

<p>LangChain is in active development, and new features are being added all the time, like the ability to examine and evaluate the LLM’s completions throughout the workflow. It’s an exciting framework that can help you with fast prototyping and deployment, and is likely to become an important tool in your generative AI toolbox in the future. </p>
<p>Larger models are generally your best choice for techniques that use advanced prompting, like PAL or ReAct. Smaller models may struggle to understand the tasks in highly structured prompts and may require you to perform additional fine tuning to improve their ability to reason and plan. This could slow down your development process. Instead, if you start with a large, capable model and collect lots of user data in deployment, you may be able to use it to train and fine tune a smaller model that you can switch to at a later time.</p>
<h3 id="ReAct-Paper"><a href="#ReAct-Paper" class="headerlink" title="ReAct Paper"></a>ReAct Paper</h3><p><img src="https://p.ipic.vip/l7w0n9.png" alt="image-20230804103551751"></p>
<blockquote>
<p>The figure provides a comprehensive visual comparison of different prompting methods in two distinct domains. The first part of the figure (1a) presents a comparison of four prompting methods: Standard, Chain-of-thought (CoT, Reason Only), Act-only, and ReAct (Reason+Act) for solving a HotpotQA question. Each method’s approach is demonstrated through task-solving trajectories generated by the model (Act, Thought) and the environment (Obs). The second part of the figure</p>
</blockquote>
<h2 id="LLM-Application-Architectures"><a href="#LLM-Application-Architectures" class="headerlink" title="LLM Application Architectures"></a>LLM Application Architectures</h2><p><img src="https://p.ipic.vip/x8fmj5.png" alt="image-20230804115519625"></p>
<ol>
<li><strong>應用架構的組成元素</strong>：<ul>
<li><strong>基礎設施層</strong>：提供運算，存儲，和網路資源來運行您的LLM，並用於託管您的應用組件。您可以使用您自己的基礎設施，或者選擇使用按需付費的雲服務。</li>
<li><strong>大型語言模型</strong>：在您的應用中使用的LLM，可以包括基礎模型，以及您特定任務的模型。這些模型會部署在適合您推理需求的基礎設施上，例如需要實時或近實時與模型交互的情況。</li>
<li><strong>從外部源獲取信息</strong>：如在增強檢索生成部分所討論的，您的應用可能需要從外部源獲取信息。</li>
<li><strong>輸出保存和反饋收集</strong>：根據您的使用情況，您可能需要實現捕獲和儲存輸出的機制，例如，您可以建立在一個會議中儲存用戶完成內容的能力，以增加您的LLM的固定上下文窗口大小。您也可以收集用戶的反饋，這對於您的應用的進一步微調，對齊或評估可能是有用的。</li>
<li><strong>使用額外的工具和框架</strong>：您可能需要使用一些工具和框架來更輕鬆地實現在本課程中討論的一些技術，例如使用len chain的內置庫來實現像pow react或chain of thought提示等技術。</li>
<li><strong>使用者介面和安全元件</strong>：最後，您通常會有某種用戶界面，例如網站或REST API，讓用戶或者其他應用可以通過它來使用您的應用。在這個層次，您也會包括用於與您的應用交互的安全組件。</li>
</ul>
</li>
<li><strong>模型對齊人類偏好</strong>：本週，您看到了如何通過使用一種名為人類反饋強化學習（RLHF）的技術來微調模型，使其與人類偏好，如有用，無害，誠實等進行對齊。</li>
<li><strong>模型推理優化</strong>：您也看到了重要的技術，通過模型蒸餾，量化，或修剪來縮小模型大小，以減少在生產中運行您的LLM所需要的硬體資源。</li>
<li><strong>模型部署優化</strong>：最後，您探索了通過結構化提示和連接到外部數據源和應用來幫助模型在部署時有更好表現的方法。</li>
<li><strong>LLM的潛力和前景</strong>：LLM可以在應用中發揮驚人的作用，利用其智能來驅動令人興奮的，有用的應用。像len chain這樣的框架使得可以快速構建，部署和測試LLM驅動的應用成為可能，對開發者來說，這是一個非常令人興奮的時期。</li>
</ol>
<h2 id="AWS-Sagemaker-JumpStart"><a href="#AWS-Sagemaker-JumpStart" class="headerlink" title="AWS Sagemaker JumpStart"></a>AWS Sagemaker JumpStart</h2><p>a model hub, Sagemaker JumpStart is a model hub, and it allows you to quickly deploy foundation models that are available within the service, and integrate them into your own applications. The JumpStart service also provides an easy way to fine-tune and deploy models. JumpStart covers many parts of this diagram, including the infrastructure, the LLM itself, the tools and frameworks, and even an API to invoke the model.</p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230804115726032.png" alt="image-20230804115726032" style="zoom:33%;" />

<p>AWS的Sagemaker JumpStart是一種服務，旨在幫助您快速將基於大型語言模型(LLM)的應用程序推向生產並實現規模運作。以下是該服務的主要特點：</p>
<ol>
<li><strong>模型中心</strong>：JumpStart作為一種模型中心，允許您快速部署在該服務中可用的基礎模型，並將其整合到自己的應用程序中。</li>
<li><strong>微調和部署</strong>：JumpStart提供了一種簡單的方式來微調和部署模型。</li>
<li><strong>全面覆蓋</strong>：JumpStart涵蓋了基於LLM應用程序建設的許多部分，包括基礎設施，LLM本身，工具和框架，甚至是調用模型的API。</li>
<li><strong>GPU需求</strong>：與您在實驗室中使用的模型相比，JumpStart模型需要使用GPU進行微調和部署。請注意，這些GPU的價格是按需定價，因此在選擇您要使用的計算設備之前，應參考Sagemaker的定價頁面。</li>
<li><strong>成本監控</strong>：使用JumpStart時，請務必在不使用時刪除Sagemaker模型端點，並遵循成本監控最佳實踐以優化成本。</li>
<li><strong>支持微調</strong>：JumpStart支持模型微調。例如，您可以透過指定訓練和驗證數據集的位置，然後選擇要用於訓練的計算設備的大小來設定微調作業。</li>
<li><strong>參數調整</strong>：您可以快速識別和修改此特定模型的可調參數。</li>
<li><strong>自動生成筆記本</strong>：JumpStart還提供一種選項，可以自動為您生成一個筆記本。如果您更喜歡以程式化的方式與這些模型進行交互，這是一種選擇。</li>
</ol>
<p>除了作為包含基礎模型的模型中心外，JumpStart還提供了許多其他資源，包括博客、視頻和示例筆記本。</p>
<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><h2 id="Reinforcement-Learning-from-Human-Feedback-RLHF"><a href="#Reinforcement-Learning-from-Human-Feedback-RLHF" class="headerlink" title="Reinforcement Learning from Human-Feedback (RLHF)"></a><strong>Reinforcement Learning from Human-Feedback (RLHF)</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/2203.02155.pdf" target="_blank" rel="noopener"><strong>Training language models to follow instructions with human feedback</strong></a> <strong>-</strong> Paper by OpenAI introducing a human-in-the-loop process to create a model that is better at following instructions (InstructGPT).</li>
<li><a href="https://arxiv.org/pdf/2009.01325.pdf" target="_blank" rel="noopener"><strong>Learning to summarize from human feedback</strong></a> - This paper presents a method for improving language model-generated summaries using a reward-based approach, surpassing human reference summaries.</li>
</ul>
<h2 id="Proximal-Policy-Optimization-PPO"><a href="#Proximal-Policy-Optimization-PPO" class="headerlink" title="Proximal Policy Optimization (PPO)"></a><strong>Proximal Policy Optimization (PPO)</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/1707.06347.pdf" target="_blank" rel="noopener"><strong>Proximal Policy Optimization Algorithms</strong></a> - The paper from researchers at OpenAI that first proposed the PPO algorithm. The paper discusses the performance of the algorithm on a number of benchmark tasks including robotic locomotion and game play.</li>
<li><a href="https://arxiv.org/pdf/2305.18290.pdf" target="_blank" rel="noopener"><strong>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</strong></a> - This paper presents a simpler and effective method for precise control of large-scale unsupervised language models by aligning them with human preferences.</li>
</ul>
<h2 id="Scaling-human-feedback"><a href="#Scaling-human-feedback" class="headerlink" title="Scaling human feedback"></a><strong>Scaling human feedback</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/2212.08073.pdf" target="_blank" rel="noopener"><strong>Constitutional AI: Harmlessness from AI Feedback</strong></a> - This paper introduces a method for training a harmless AI assistant without human labels, allowing better control of AI behavior with minimal human input.</li>
</ul>
<h2 id="Advanced-Prompting-Techniques"><a href="#Advanced-Prompting-Techniques" class="headerlink" title="Advanced Prompting Techniques"></a><strong>Advanced Prompting Techniques</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/2201.11903.pdf" target="_blank" rel="noopener"><strong>Chain-of-thought Prompting Elicits Reasoning in Large Language Models</strong></a> -  Paper by researchers at Google exploring how chain-of-thought prompting improves the ability of LLMs to perform complex reasoning.</li>
<li><a href="https://arxiv.org/abs/2211.10435" target="_blank" rel="noopener"><strong>PAL: Program-aided Language Models</strong></a> - This paper proposes an approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps.</li>
<li><a href="https://arxiv.org/abs/2210.03629" target="_blank" rel="noopener"><strong>ReAct: Synergizing Reasoning and Acting in Language Models</strong></a> This paper presents an advanced prompting technique that allows an LLM to make decisions about how to interact with external applications.</li>
</ul>
<h2 id="LLM-powered-application-architectures"><a href="#LLM-powered-application-architectures" class="headerlink" title="LLM powered application architectures"></a><strong>LLM powered application architectures</strong></h2><ul>
<li><a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener"><strong>LangChain Library (GitHub)</strong></a> - This library is aimed at assisting in the development of those types of applications, such as Question Answering, Chatbots and other Agents. You can read the documentation <a href="https://docs.langchain.com/docs/" target="_blank" rel="noopener">here</a>.</li>
<li><a href="https://a16z.com/2023/01/19/who-owns-the-generative-ai-platform/" target="_blank" rel="noopener"><strong>Who Owns the Generative AI Platform?</strong></a> - The article examines the market dynamics and business models of generative AI.</li>
</ul>
<h1 id="Responsible-AI"><a href="#Responsible-AI" class="headerlink" title="Responsible AI"></a>Responsible AI</h1><ol>
<li><strong>毒性（Toxicity）</strong>：LLM可能會產生可能對特定群體，特別是邊緣化群體或受保護群體造成傷害或歧視的語言或內容。為了應對這個問題，一個策略是從訓練數據開始，進行篩選，並訓練守護模型來檢測和過濾訓練數據中的任何不需要的內容。此外，為標注員提供足夠的指導，並確保標注員群體的多樣性也是至關重要的。</li>
<li><strong>幻覺（<em>Hallucinations</em>）</strong>：LLM可能會生成沒有事實依據的信息。這是因為在訓練大型語言模型或神經網絡時，我們往往無法知道模型實際學習了什麼，模型有時會嘗試填補它的數據空白。為了防止這種情況，一種策略是向用戶說明這種技術的現實，並添加任何免責聲明。此外，可以用獨立且經過驗證的信息源來增強大型語言模型，以便可以對獲取的數據進行雙重檢查。</li>
<li><strong>知識產權（Intellectual Property</strong>：使用模型返回的數據可能會侵犯他人的知識產權。因此，我們需要通過技術、政策制定者和其他法律機制來解決這個問題。此外，我們還需要建立一個管理體系，以確保所有利益相關者都在防止這種情況發生方面扮演著他們應有的角色。</li>
</ol>
<p>總的來說，使用生成人工智能模型時，需要明確使用案例，評估風險，評估性能，不斷迭代，並在整個生命週期中實施治理政策和問責措施。</p>
<p>對於目前研究社區正在積極研究的一些令人興奮的主題，如水印和指紋識別，以及創建可以幫助確定是否使用了生成人工智能創建內容的模型，</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/31/LLM/Generative%20AI%20with%20LLM/2%20Fine-Tuning%20LLM/">LLM/Generative AI with LLM/2 Fine-Tuning LLM</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-31</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><div class="content"><h1 id="Fine-Tuning-LLM-with-Instructions"><a href="#Fine-Tuning-LLM-with-Instructions" class="headerlink" title="Fine-Tuning LLM with Instructions"></a>Fine-Tuning LLM with Instructions</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>通過一種名為「指令微調」的過程來調整基礎語言模型，例如FLAN-T5。在此過程中，使用了特定的提示模板和數據集來進行訓練，並利用了如 ROUGE 和 HELM 等評估指標來衡量微調的成功程度。指令微調證明在廣泛的自然語言任務中非常有效，只需幾百個範例，就能根據特定任務進行精細調整。</p>
<p>您還學習了兩種參數高效微調（PEFT）方法——LoRA和提示調整，這些方法可以減少微調模型所需的計算量。在實踐中，將LoRA與量化技術結合（稱為QLoRA）可進一步減少內存使用。PEFT策略大量用於最小化計算和內存資源，進而降低微調成本，讓您可以充分利用計算預算，並加快開發流程。</p>
<ol>
<li>指令 fine-tuning 是讓模型能夠更有效地回應我們的提示或問題。這種方法從大量的互聯網文本學習，並在更小的數據集上進行微調，以學習跟隨指令。然而，需要注意的是慘慘遺忘 (catastrophic forgetting)，即模型可能忘記之前學到的大部分內容。進行指令 fine-tuning 時，需要涵蓋多種不同的指令類型，以防止模型忘記之前學到的內容。</li>
<li>參數效率 fine-tuning (PEFT) 允許開發者將模型特化為特定的應用，而無需對模型的每一個參數進行微調。這種方法可以保持原始模型的權重不變，或者在模型之上添加適應性層，這樣就可以減少記憶體使用，而且仍然可以獲得與全模型 fine-tuning 相似的效果。其中，一種受到廣泛使用的技術是 LoRA，它可以使用低秩矩陣來獲得相當好的性能結果，同時也減少了計算和記憶體需求。</li>
</ol>
<p>兩種方法都有其優點和使用場景，但是在實際操作中，許多開發者會選擇先從 prompting 開始，如果性能達到滿意的水平，就直接使用；如果效果不佳，則使用 PEFT 或 LoRA 等 fine-tuning 技術來提高性能。但需要注意，使用巨大模型的成本是一個值得討論的問題，有時候为特定应用 fine-tuning 一个较小的模型会更有利。</p>
<h2 id="Instruction-Fine-Tuning"><a href="#Instruction-Fine-Tuning" class="headerlink" title="Instruction Fine-Tuning"></a>Instruction Fine-Tuning</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802153201434.png" alt="image-20230802153201434" style="zoom:33%;" />



<h2 id="Prompt-Eng-ICL，-In-context-Learning"><a href="#Prompt-Eng-ICL，-In-context-Learning" class="headerlink" title="Prompt Eng, ICL，(In-context Learning)"></a>Prompt Eng, ICL，(In-context Learning)</h2><table>
<thead>
<tr>
<th>類型</th>
<th>描述</th>
<th>優點</th>
<th>缺點</th>
</tr>
</thead>
<tbody><tr>
<td>零次學習（Zero-shot inference）</td>
<td>模型能識別提示中的指令並正確執行</td>
<td>對於某些模型（通常是較大的模型）來說，能直接完成指令</td>
<td>對於較小的模型，可能無法正確完成任務</td>
</tr>
<tr>
<td>單次學習或少次學習（One-shot or Few-shot inference）</td>
<td>包含一個或多個想要模型完成的示例</td>
<td>可以幫助模型識別任務並生成良好的完成情況</td>
<td>1. 對於較小的模型，即使包含五到六個示例，也不一定能正確完成任務 2. 您在提示中包含的任何示例都會占用上下文窗口中的寶貴空間，減少了您可以包含其他有用信息的空間</td>
</tr>
</tbody></table>
<p>However, this strategy has a couple of drawbacks. </p>
<ol>
<li>First, for smaller models, it doesn’t always work, even when five or six examples are included. </li>
<li>Second, any examples you include in your prompt take up valuable space in the context window, reducing the amount of room you have to include other useful information.</li>
</ol>
<p>所以可以用Fine-Tuning想辦法解決</p>
<h2 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h2><ul>
<li>To improve the performance and adaptability of a pre-trained language model for specific tasks.</li>
</ul>
<table>
<thead>
<tr>
<th>學習方式</th>
<th>描述</th>
<th>優點</th>
<th>缺點</th>
</tr>
</thead>
<tbody><tr>
<td>Prompt-engineering</td>
<td>透過在提示中包含一個或多個示例，來幫助模型識別任務並產生良好的完成內容（也稱為一擊或少數擊推理）</td>
<td>可以在不需要進行微調的情況下改善模型的任務性能</td>
<td>1. 對於較小的模型，即使包含五或六個示例，也不一定總是有效 2. 您在提示中包含的任何示例都會占用寶貴的上下文窗口空間，減少了您包含其他有用信息的空間</td>
</tr>
<tr>
<td>Pre-training</td>
<td>使用大量非結構化的文本數據進行自監督學習來訓練LLM</td>
<td>可以讓模型獲得廣泛的語言知識，理解各種語境</td>
<td>1. 需要大量的數據 2. 對於特定任務，模型的性能可能不佳</td>
</tr>
<tr>
<td><strong>Fine-tuning</strong> (<strong>Instruction Fine-tuning</strong>)</td>
<td>透過監督學習的方式，使用標記過的數據集（提示和完成對）來更新LLM的權重; 利用指令來訓練模型，讓模型學習如何對特定指令進行回應</td>
<td>可以改善模型在特定任務上的表現</td>
<td>需要有標記過的數據集，且這些數據集必須和目標任務相關; 同樣需要有含有特定指令的標記過的數據集</td>
</tr>
</tbody></table>
<p>在「Fine-Tuning」過程中，模型會使用包含特定指令的示例進行訓練。如：若欲改善模型的摘要能力，便需要構建包含 “Summarize the following text” 或類似語句的資料集。</p>
<h3 id="AWS-sample-prompt-instruction-termplates"><a href="#AWS-sample-prompt-instruction-termplates" class="headerlink" title="AWS sample prompt instruction termplates"></a>AWS sample prompt instruction termplates</h3><img src="https://p.ipic.vip/x3o1gs.png" alt="image-20230802154918259" style="zoom: 25%;" />



<img src="https://p.ipic.vip/0sj0it.png" alt="image-20230802155435048" style="zoom:25%;" />



<p>當我們使用LLM，它的輸出是跨令牌的概率分佈。 因此，我可以<u>比較completion的分佈和訓練標籤的分佈</u>，並使<u>用標準交叉熵函數</u><br><u>來計算兩個令牌分佈之間的損失。 然後，我會使用這個計算出的損失來在標準反向傳播過程中更新模型的權重。 我將對許多批次的提示完成對進行這樣的操作，並在幾個時期內更新權重，以提高模型在任務上的性能。</u> 就<u>像標準的監督學習一樣</u>，我可以定義單獨的評估步驟，使用保留的驗證數據集來衡量我們的LLM的性能。 這將為我提供驗證準確性，並且在完成微調後，我可以使用保留的測試數據集進行最終的性能評估。 這將為我提供測試準確性。 微調過程將產生一個新版本的基本模型，通常被稱為指令模型，它更適合我們感興趣的任務。使用指令提示進行微調是當今微調LLM的最常見方法。指的都是「Instruction Fine-Tuning」</p>
<h2 id="On-a-Single-Task-Multi-Tasks"><a href="#On-a-Single-Task-Multi-Tasks" class="headerlink" title="On a Single Task / Multi-Tasks"></a>On a Single Task / Multi-Tasks</h2><table>
<thead>
<tr>
<th>微調方式</th>
<th>描述</th>
<th>優點</th>
<th>缺點</th>
</tr>
</thead>
<tbody><tr>
<td>單任務微調</td>
<td>預訓練模型被微調以改善在特定任務上的性能。例如，使用一個特定任務的範例數據集來進行微調。</td>
<td>對於特定的任務，性能可以被大幅提升。相對少量的範例（500-1000個）就可以得到良好的效果。</td>
<td>這種方法可能導致”災難性遺忘”，使得模型在其他任務上的性能下降。</td>
</tr>
<tr>
<td>多任務微調</td>
<td>在多個任務上同時進行微調，以維持模型的多任務泛化能力。</td>
<td>保持了模型的多任務泛化能力，可避免災難性遺忘。</td>
<td>需要更多的範例（可能需要50-100,000個跨多個任務的範例）和更多的計算資源。</td>
</tr>
<tr>
<td>參數高效微調 PEFT</td>
<td>這是一種只訓練少量任務特定適配層和參數的技術，大部分的預訓練權重都不變。</td>
<td>對災難性遺忘顯示出更大的抵抗力，因為大部分的預訓練權重都沒有改變。</td>
<td>這是一個仍在研究中的方法，可能還未完全成熟或有其它未知的挑戰。</td>
</tr>
</tbody></table>
<h3 id="Multi-Tasks"><a href="#Multi-Tasks" class="headerlink" title="Multi-Tasks"></a>Multi-Tasks</h3><blockquote>
<p>FLAN, which stands for <strong>fine-tuned language net</strong>, is a specific set of instructions used to fine-tune different models. Because they’re FLAN fine-tuning is the last step of the training process the authors of the original paper called it the <strong><em>metaphorical dessert</em></strong> to the main course of pre-training quite a fitting name. </p>
<ul>
<li>FLAN-T5, the FLAN instruct version of the T5 foundation model </li>
<li>FLAN-PALM is the flattening struct version of the palm foundation model. </li>
</ul>
</blockquote>
<h3 id=""><a href="#" class="headerlink" title=""></a><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802162252721.png" alt="image-20230802162252721" style="zoom:33%;" /></h3><table>
<thead>
<tr>
<th></th>
<th>FLAN</th>
<th>FLAN-T5</th>
<th>FLAN-PALM</th>
</tr>
</thead>
<tbody><tr>
<td>定義</td>
<td>Fine-tuned Language Net，是一個透過多任務指令微調的模型系列。不同的 FLAN 模型依據微調過程中使用的數據集和任務有所不同。</td>
<td>FLAN-T5 是基於T5基礎模型的 FLAN 模型，它總共在 473 個數據集上進行了微調，涵蓋了 146 種任務類別。</td>
<td>FLAN-PALM 是基於PALM基礎模型的 FLAN 模型。</td>
</tr>
<tr>
<td>用途</td>
<td>用於對不同的基礎模型進行微調。</td>
<td>是一個適合通用的指令模型，展示出在多種任務上的良好性能。</td>
<td>類似於FLAN-T5，對不同的任務有不同的性能表現。</td>
</tr>
<tr>
<td>需要的範例數量</td>
<td>取決於微調的任務數量和種類，可能需要大量數據。</td>
<td>依據不同任務的需求，可能需要大量數據。</td>
<td>依據不同任務的需求，可能需要大量數據。</td>
</tr>
</tbody></table>
<h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><table>
<thead>
<tr>
<th></th>
<th>SAMSum</th>
<th>DialogSum</th>
</tr>
</thead>
<tbody><tr>
<td>定義</td>
<td>SAMSum 是一個專門用於對話摘要的數據集，包含16,000個類似於即時通訊的對話與摘要。</td>
<td>DialogSum 是一個用於客服對話摘要的數據集，包含超過13,000個客服聊天對話和摘要。</td>
</tr>
<tr>
<td>用途</td>
<td>訓練語言模型如何總結對話，特別是友誼或日常生活中的對話。</td>
<td>訓練語言模型如何總結客服對話，並理解顧客的需求和問題。</td>
</tr>
<tr>
<td>對話內容</td>
<td>主要是朋友之間的日常對話。</td>
<td>主要是客戶與客服之間的對話。</td>
</tr>
<tr>
<td>設計目標</td>
<td>提供語言學家編寫的高品質對話和摘要數據集，以訓練語言模型。</td>
<td>提供實際的客服對話數據，以便語言模型能學習如何摘要對話並理解顧客的需求。</td>
</tr>
</tbody></table>
<blockquote>
<p>SAMSum 是 “A System-Agnostic Method for Dialogue Summarization” 的縮寫。這個名字來自於創建該數據集的相關研究論文的標題。這個名字反映了數據集的設計目標，即為不同的對話摘要系統提供訓練資料，這些系統可以獨立於特定的對話系統進行運作。這意味著，無論對話是通過什麼媒介（例如即時消息，電子郵件等）或在什麼上下文中（例如客服，個人聊天等）進行，SAMSum 都應該能夠提供有用的訓練數據。</p>
</blockquote>
<h2 id="Model-Evaluation"><a href="#Model-Evaluation" class="headerlink" title="Model Evaluation"></a>Model Evaluation</h2><table>
<thead>
<tr>
<th></th>
<th>ROUGE</th>
<th>BLEU</th>
</tr>
</thead>
<tbody><tr>
<td><strong>全稱</strong></td>
<td>Recall-Oriented Understudy for Gisting Evaluation</td>
<td>Bilingual Evaluation Understudy</td>
</tr>
<tr>
<td><strong>使用情境</strong></td>
<td>主要用於評估自動生成的摘要的品質</td>
<td>主要用於評估機器翻譯的品質</td>
</tr>
<tr>
<td><strong>衡量方式</strong></td>
<td>1. ROUGE-N: 計算預測和參考文本間的 n-gram 重疊數 <br> 2. ROUGE-L: 計算最長共享子序列 (LCS) <br> 3. ROUGE-S: 計算跳躍的二元組（Skip-bigram）的重叠程度</td>
<td>1. BLEU: 針對多個不同大小的 n-grams 計算精確度 (precision)，並使用特定的平均方式進行整合</td>
</tr>
<tr>
<td><strong>優點</strong></td>
<td>提供多種衡量方式（如單詞重疊、最長共享子序列等），更適合評估摘要生成的效果</td>
<td>考慮了多個不同大小的 n-grams 的精確度，更適合評估翻譯的品質</td>
</tr>
<tr>
<td><strong>缺點</strong></td>
<td>可能會對高頻詞給予過高的權重，並可能無法有效捕捉詞序信息</td>
<td>無法考慮語句的語義和語境，並可能對詞序變動的語句給予過低的評分</td>
</tr>
</tbody></table>
<h3 id="ROUGE-Recall-Oriented-Understudy-for-Gisting-Evaluation）"><a href="#ROUGE-Recall-Oriented-Understudy-for-Gisting-Evaluation）" class="headerlink" title="ROUGE, (Recall-Oriented Understudy for Gisting Evaluation）"></a>ROUGE, (Recall-Oriented Understudy for Gisting Evaluation）</h3><p>ROUGE 是一種評估文本摘要（如從長篇文章中生成簡短摘要）的質量的方法。</p>
<p>ROUGE的重要概念包括「召回率」（Recall）、「精確度」（Precision）和「F1分數」（F1 score）。以下是這三種評估方法的基本概念：</p>
<ol>
<li>召回率：生成摘要與原文參考摘要中共享的詞語（或n-gram，即n個連續詞語的序列）的數量除以參考摘要中的詞語（或n-gram）的數量。</li>
<li>精確度：生成摘要與原文參考摘要中共享的詞語（或n-gram）的數量除以生成摘要中的詞語（或n-gram）的數量。</li>
<li>F1分數：召回率和精確度的調和平均數，它平衡了召回率和精確度的重要性。</li>
</ol>
<p>讓我們來看一個例子。假設我們有以下參考摘要（由人類撰寫）：</p>
<p>參考摘要：”我喜歡喝茶。”</p>
<p>然後模型生成了以下摘要：</p>
<p>生成摘要：”我真的很喜歡喝茶。”</p>
<p>在這種情況下，我們將會看到以下結果：</p>
<ol>
<li>召回率：所有的參考摘要中的詞（我，喜歡，喝，茶）都在生成摘要中出現了，因此召回率為 4/4 = 1。</li>
<li>精確度：生成摘要中的四個詞（我，真的，很喜歡，喝，茶）中，有四個詞出現在參考摘要中，因此精確度為 4/5 = 0.8。</li>
<li>F1分數：F1分數是召回率和精確度的調和平均數，其公式為 2<em>(Recall * Precision) / (Recall + Precision)。在這個例子中，F1分數為 2</em>(1 * 0.8) / (1 + 0.8) = 0.88。</li>
</ol>
<p>以上就是 ROUGE 指標在單詞級別（或稱為unigram）的運作方式，這通常被稱為 ROUGE-1。但是，ROUGE 也可以在更大的文本塊（如二元組，也就是兩個連續的詞，或者更大的n-gram）上運作。在這些情況下，我們將計算生成摘要和參考摘要中共享的n-gram的數量，並相應地計算召回率、精確度和F1分數。這被稱為 ROUGE-N，其中N代表n-gram的大小。例如，對於二元組（bigrams），我們會使用 ROUGE-2。此外，還有一種稱為 ROUGE-L 的評價指標，它計算生成摘要與參考摘要之間的最長公共子序列（LCS）。</p>
<h3 id="BLEU-Bilingual-Evaluation-Understudy"><a href="#BLEU-Bilingual-Evaluation-Understudy" class="headerlink" title="BLEU, (Bilingual Evaluation Understudy)"></a>BLEU, (Bilingual Evaluation Understudy)</h3><p>BLEU（Bilingual Evaluation Understudy）評分是一種常用來評估機器翻譯系統的效能的方法。BLEU評分的主要思想是：如果機器翻譯的結果越接近人類翻譯的結果，那麼該翻譯的品質就越高。</p>
<p>BLEU評分主要依賴於n-gram精確度。n-gram是一種語言模型，用於預測句子中的下一個詞。n-gram中的”n”表示我們要考慮的詞的數量。例如，”bigram”（二元組）考慮兩個相鄰的詞，”trigram”（三元組）考慮三個相鄰的詞，以此類推。</p>
<p>值得注意的是，BLEU評分還包括一個稱為”brevity penalty”（簡短處罰）的因素。如果機器翻譯的結果比參考翻譯短，那麼這個處罰因素會降低BLEU評分，以反映出這種短度差異。</p>
<p>舉一個例子，假設我們有一句英文 “The cat sat on the mat”，並且有一個參考翻譯為法語的 “Le chat s’est assis sur le tapis”。如果機器翻譯的結果是 “Le chat sur le tapis”，那麼這個翻譯的BLEU評分將會較低，因為它缺少了 “s’est assis”（坐下）這個動作。</p>
<h2 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h2><table>
<thead>
<tr>
<th>基準測試 (Benchmark)</th>
<th>年份</th>
<th>優點</th>
<th>缺點</th>
</tr>
</thead>
<tbody><tr>
<td>GLUE (General Language Understanding Evaluation)</td>
<td>2018</td>
<td>能夠評估模型在多種任務上的通用能力。</td>
<td>由於該基準測試涵蓋範疇廣泛，可能無法針對特定領域的任務進行深入評估。</td>
</tr>
<tr>
<td>SuperGLUE</td>
<td>2019</td>
<td>包含更具挑戰性的任務，可以評估模型在更複雜情境下的表現。</td>
<td>雖然難度提高，但可能仍無法完全捕捉到某些具有高度專業知識或創造力要求的任務的表現。</td>
</tr>
<tr>
<td>MMLU (Massive Multitask Language Understanding)</td>
<td>2021</td>
<td>能夠評估模型在多個學科領域的知識和問題解決能力。</td>
<td>由於範圍非常廣泛，可能對某些特定領域的深度評估仍然有限。</td>
</tr>
<tr>
<td>BIG-bench</td>
<td>2022</td>
<td>提供極其多樣化的任務，包括語言學、數學、物理學等多個領域。</td>
<td>由於範疇龐大，可能難以精確定位模型在特定任務上的弱點。此外，進行這種大型基準測試可能需要較高的計算成本。</td>
</tr>
<tr>
<td>HELM (Holistic Evaluation of Language Models)</td>
<td>N/A</td>
<td>能夠全面評估模型，並提供了對公平性、偏見和有害行為的評估。</td>
<td>雖然它全面，但可能仍然無法完全捕捉到模型在特定任務或場景下的特定優點或缺點。</td>
</tr>
</tbody></table>
<h1 id="PEFT-Parameter-efficient-fine-tuning"><a href="#PEFT-Parameter-efficient-fine-tuning" class="headerlink" title="PEFT, (Parameter efficient fine-tuning)"></a>PEFT, (Parameter efficient fine-tuning)</h1><blockquote>
<p>Full fine-tuning results in a new version of the model for every task you train on. Each of these is the same size as the original model, so it can create an expensive storage problem if you’re fine-tuning for multiple tasks. Let’s see how you can use PEFT to improve the situation. With parameter efficient fine-tuning, you train only a small number of weights, which results in a much smaller footprint overall, as small as megabytes depending on the task. The new parameters are combined with the original LLM weights for inference. The PEFT weights are trained for each task and can be easily swapped out for inference, allowing efficient adaptation of the original model to multiple tasks. There are several methods you can use for parameter efficient fine-tuning, each with trade-offs on parameter efficiency, memory efficiency, training speed, model quality, and inference costs. </p>
</blockquote>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802200511131.png" alt="image-20230802200511131" style="zoom: 25%;" />

<p><strong>表格1: 完全微調的挑戰</strong></p>
<table>
<thead>
<tr>
<th>挑戰</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>計算需求</td>
<td>完全微調需要大量的計算資源來訓練模型</td>
</tr>
<tr>
<td>記憶體需求</td>
<td>需要記憶體來儲存模型和其他訓練過程中的參數</td>
</tr>
<tr>
<td>儲存問題</td>
<td>每個任務的微調都會產生一個新的模型，可能會產生大量的儲存需求</td>
</tr>
<tr>
<td>災難性遺忘</td>
<td>完全微調可能導致模型遺忘先前訓練的任務</td>
</tr>
</tbody></table>
<p><strong>表格2: PEFT與完全微調的比較</strong></p>
<table>
<thead>
<tr>
<th>屬性</th>
<th>完全微調</th>
<th>PEFT</th>
</tr>
</thead>
<tbody><tr>
<td>參數更新</td>
<td>更新所有模型參數</td>
<td>只更新一部分模型參數</td>
</tr>
<tr>
<td>記憶體需求</td>
<td>高</td>
<td>相對較低</td>
</tr>
<tr>
<td>訓練速度</td>
<td>慢</td>
<td>相對較快</td>
</tr>
<tr>
<td>多任務適應</td>
<td>需要對每個任務都訓練一個新的模型</td>
<td>可以輕易地在推理中替換訓練的權重以適應多種任務</td>
</tr>
</tbody></table>
<p><strong>表格3: PEFT的三大類型</strong></p>
<table>
<thead>
<tr>
<th>方法類型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>選擇性方法</td>
<td>只對原始LLM參數的一部分進行微調</td>
</tr>
<tr>
<td>再參數化方法</td>
<td>使用原始LLM參數，但通過創建原始網絡權重的新的低秩變換，減少了需要訓練的參數數量</td>
</tr>
<tr>
<td>添加法</td>
<td>保持所有原始LLM權重的凍結並引入新的可訓練組件</td>
</tr>
</tbody></table>
<h2 id="PEFT-tech1-LoRA-Low-Rank-Adaption-of-LLMs"><a href="#PEFT-tech1-LoRA-Low-Rank-Adaption-of-LLMs" class="headerlink" title="PEFT tech1: LoRA (Low Rank Adaption of LLMs)"></a>PEFT tech1: LoRA (Low Rank Adaption of LLMs)</h2><p><strong>LoRA的工作原理</strong>：</p>
<table>
<thead>
<tr>
<th>步驟</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>1. 凍結原始模型參數</td>
<td>LoRA首先將所有的原始模型參數凍結，防止在微調過程中被更新。</td>
</tr>
<tr>
<td>2. 注入秩分解矩陣</td>
<td>LoRA在原始權重旁注入一對秩分解矩陣，這對較小的矩陣將在後續步驟中進行訓練。</td>
</tr>
<tr>
<td>3. 設定矩陣維度</td>
<td>這對較小的矩陣的維度被設定為它們的乘積是一個與它們修改的權重相同維度的矩陣。</td>
</tr>
<tr>
<td>4. 訓練小矩陣</td>
<td>在保持原始LLM（大語言模型）的權重凍結的情況下，使用之前見過的監督式學習過程訓練這些較小的矩陣。</td>
</tr>
<tr>
<td>5. 矩陣相乘</td>
<td>在推理時，這兩個低秩矩陣被相乘以創建一個與凍結權重相同維度的矩陣。</td>
</tr>
<tr>
<td>6. 更新權重</td>
<td>然後將這個新創建的矩陣加到原始權重上，並將模型中的這些原始權重替換為這些更新的值。</td>
</tr>
</tbody></table>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802192846899.png" alt="image-20230802192846899" style="zoom: 20%;" />

<table>
<thead>
<tr>
<th>步驟</th>
<th>說明</th>
</tr>
</thead>
<tbody><tr>
<td>輸入處理</td>
<td>輸入提示轉換為代碼，然後轉換為嵌入向量，並傳遞到Transformer的編碼器和/或解碼器部分。</td>
</tr>
<tr>
<td>Transformer組件</td>
<td>編碼器和解碼器有兩種類型的神經網路：自我關注和前饋網路。這些網路的權重在預訓練期間學習。</td>
</tr>
<tr>
<td>完全微調</td>
<td>在完全微調期間，更新這些層的所有參數。</td>
</tr>
<tr>
<td>LoRA策略</td>
<td>LoRA通過凍結原始模型的所有參數，然後將一對秩分解矩陣注入到原始權重旁邊，減少微調期間需要訓練的參數數量。</td>
</tr>
<tr>
<td>保持原權重</td>
<td>保持LLM的原始權重凍結，並使用之前看到的相同的監督學習過程訓練較小的矩陣。</td>
</tr>
<tr>
<td>推斷</td>
<td>在推斷時，將兩個低秩矩陣相乘以創建一個與凍結權重具有相同維度的矩陣。然後將其添加到原始權重並替換模型中的這些更新值。</td>
</tr>
<tr>
<td>LoRA微調模型</td>
<td>您現在有一個可以執行特定任務的LoRA微調模型。由於該模型的參數數量與原始模型相同，因此對推斷延遲幾乎沒有影響。</td>
</tr>
<tr>
<td>應用LoRA</td>
<td>研究人員發現，僅將LoRA應用於模型的自我注意層通常就足夠進行任務的微調並實現性能增益。然而，原則上，您也可以在前饋層等其他組件上使用LoRA。但是，由於LLMs的大部分參數都在注意層中，因此通過將LoRA應用於這些權重矩陣，您可以獲得最大的訓練參數節省。</td>
</tr>
</tbody></table>
<h2 id="PEFT-tech2-Soft-Prompt-also-called-Prompt-Tuning"><a href="#PEFT-tech2-Soft-Prompt-also-called-Prompt-Tuning" class="headerlink" title="PEFT tech2: Soft Prompt, also called Prompt Tuning"></a>PEFT tech2: Soft Prompt, also called Prompt Tuning</h2><img src="https://p.ipic.vip/2tgwuh.png" alt="image-20230802195050954" style="zoom: 25%;" />

<img src="https://p.ipic.vip/nhzm0k.png" alt="image-20230802195125915" style="zoom:25%;" />

<img src="https://p.ipic.vip/633jbo.png" alt="image-20230802195351887" style="zoom: 33%;" />







<h1 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h1><h2 id="Multi-task-instruction-fine-tuning"><a href="#Multi-task-instruction-fine-tuning" class="headerlink" title="Multi-task, instruction fine-tuning"></a><strong>Multi-task, instruction fine-tuning</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/2210.11416.pdf" target="_blank" rel="noopener"><strong>Scaling Instruction-Finetuned Language Models</strong></a> - Scaling fine-tuning with a focus on task, model size and chain-of-thought data.</li>
<li><a href="https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html" target="_blank" rel="noopener"><strong>Introducing FLAN: More generalizable Language Models with Instruction Fine-Tuning</strong></a> - This blog (and article) explores instruction fine-tuning, which aims to make language models better at performing NLP tasks with zero-shot inference.</li>
</ul>
<h2 id="Model-Evaluation-Metrics"><a href="#Model-Evaluation-Metrics" class="headerlink" title="Model Evaluation Metrics"></a><strong>Model Evaluation Metrics</strong></h2><ul>
<li><a href="https://crfm.stanford.edu/helm/latest/" target="_blank" rel="noopener"><strong>HELM - Holistic Evaluation of Language Models</strong></a> - HELM is a living benchmark to evaluate Language Models more transparently. </li>
<li><a href="https://openreview.net/pdf?id=rJ4km2R5t7" target="_blank" rel="noopener"><strong>General Language Understanding Evaluation (GLUE) benchmark</strong></a> - This paper introduces GLUE, a benchmark for evaluating models on diverse natural language understanding (NLU) tasks and emphasizing the importance of improved general NLU systems.</li>
<li><a href="https://super.gluebenchmark.com/" target="_blank" rel="noopener"><strong>SuperGLUE</strong></a> - This paper introduces SuperGLUE, a benchmark designed to evaluate the performance of various NLP models on a range of challenging language understanding tasks.</li>
<li><a href="https://aclanthology.org/W04-1013.pdf" target="_blank" rel="noopener"><strong>ROUGE: A Package for Automatic Evaluation of Summaries</strong></a> - This paper introduces and evaluates four different measures (ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S) in the ROUGE summarization evaluation package, which assess the quality of summaries by comparing them to ideal human-generated summaries.</li>
<li><a href="https://arxiv.org/pdf/2009.03300.pdf" target="_blank" rel="noopener"><strong>Measuring Massive Multitask Language Understanding (MMLU)</strong></a> - This paper presents a new test to measure multitask accuracy in text models, highlighting the need for substantial improvements in achieving expert-level accuracy and addressing lopsided performance and low accuracy on socially important subjects.</li>
<li><a href="https://arxiv.org/pdf/2206.04615.pdf" target="_blank" rel="noopener"><strong>BigBench-Hard - Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models</strong></a> - The paper introduces BIG-bench, a benchmark for evaluating language models on challenging tasks, providing insights on scale, calibration, and social bias.</li>
</ul>
<h2 id="Parameter-efficient-fine-tuning-PEFT"><a href="#Parameter-efficient-fine-tuning-PEFT" class="headerlink" title="Parameter- efficient fine tuning (PEFT)"></a><strong>Parameter- efficient fine tuning (PEFT)</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/2303.15647.pdf" target="_blank" rel="noopener"><strong>Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</strong></a> - This paper provides a systematic overview of Parameter-Efficient Fine-tuning (PEFT) Methods in all three categories discussed in the lecture videos.</li>
<li><a href="https://arxiv.org/pdf/2211.15583.pdf" target="_blank" rel="noopener"><strong>On the Effectiveness of Parameter-Efficient Fine-Tuning</strong></a> - The paper analyzes sparse fine-tuning methods for pre-trained models in NLP.</li>
</ul>
<h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a><strong>LoRA</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank" rel="noopener"><strong>LoRA Low-Rank Adaptation of Large Language Models</strong></a> -  This paper proposes a parameter-efficient fine-tuning method that makes use of low-rank decomposition matrices to reduce the number of trainable parameters needed for fine-tuning language models.</li>
<li><a href="https://arxiv.org/pdf/2305.14314.pdf" target="_blank" rel="noopener"><strong>QLoRA: Efficient Finetuning of Quantized LLMs</strong></a> - This paper introduces an efficient method for fine-tuning large language models on a single GPU, based on quantization, achieving impressive results on benchmark tests.</li>
</ul>
<h2 id="Prompt-tuning-with-soft-prompts"><a href="#Prompt-tuning-with-soft-prompts" class="headerlink" title="Prompt tuning with soft prompts"></a><strong>Prompt tuning with soft prompts</strong></h2><ul>
<li><a href="https://arxiv.org/pdf/2104.08691.pdf" target="_blank" rel="noopener"><strong>The Power of Scale for Parameter-Efficient Prompt Tuning</strong></a> - The paper explores “prompt tuning,” a method for conditioning language models with learned soft prompts, achieving competitive performance compared to full fine-tuning and enabling model reuse for many tasks.</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2023/07/29/LLM/Generative%20AI%20with%20LLM/1%20Generative%20AI%20with%20LLM/">LLM/Generative AI with LLM/1 Generative AI with LLM</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-07-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span><div class="content"><h1 id="Generative-AI-w-LLM"><a href="#Generative-AI-w-LLM" class="headerlink" title="Generative AI w/ LLM"></a>Generative AI w/ LLM</h1><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><h2 id="Generative-AI-w-LLM-1"><a href="#Generative-AI-w-LLM-1" class="headerlink" title="Generative AI w/ LLM"></a>Generative AI w/ LLM</h2><p>2017 年，谷歌和多倫多大學發表了《Attention is All You Need》這篇論文後，一切都發生了變化。 Transformer 架構已經到來。 這種新穎的方法開啟了我們今天看到的生成人工智能的進步。 </p>
<p>它可以</p>
<ul>
<li>有效地擴展以使用多核 GPU</li>
<li>並行處理輸入數據，利用更大的訓練數據集，</li>
<li>最重要的是，它能夠學會關注正在處理的單詞的含義。 您所需要的就是關注。 </li>
</ul>
<p>Ambiguity: Bank</p>
<p>These are <code>homonyms</code>. In this case, it’s only with the context of the sentence that we can see what kind of <code>bank</code> is meant. </p>
<p>Syntactic ambiguity: whose Book?</p>
<p>Words within a sentence structures can be <code>ambiguous</code> or have what we might call <code>syntactic ambiguity</code>. Take for example this sentence, “The teacher taught the students with the book.” Did the teacher teach using the book or did the student have the book, or was it both?</p>
<img src="https://p.ipic.vip/edg63s.png" alt="image-20230802125204881" style="zoom: 33%;" />



<h2 id="Transformer-Architecture"><a href="#Transformer-Architecture" class="headerlink" title="Transformer Architecture"></a>Transformer Architecture</h2><p>sequence2sequence + self-attention</p>
<img src="https://p.ipic.vip/jrujlb.png" alt="img" style="zoom: 77%;" />

<blockquote>
<p>“Attention is All You Need” is a research paper published in 2017 by Google researchers, which introduced the Transformer model, a novel architecture that revolutionized the field of natural language processing (NLP) and became the basis for the LLMs we now know - such as GPT, PaLM and others. The paper proposes a neural network architecture that replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with an entirely attention-based mechanism. </p>
<p>The Transformer model uses self-attention to compute representations of input sequences, which allows it to capture long-term dependencies and parallelize computation effectively. The authors demonstrate that their model achieves state-of-the-art performance on several machine translation tasks and outperform previous models that rely on RNNs or CNNs.</p>
<p>The Transformer architecture consists of an encoder and a decoder, each of which is composed of several layers. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a feed-forward neural network. The multi-head self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network applies a point-wise fully connected layer to each position separately and identically. </p>
<p>The Transformer model also uses residual connections and layer normalization to facilitate training and prevent overfitting. In addition, the authors introduce a positional encoding scheme that encodes the position of each token in the input sequence, enabling the model to capture the order of the sequence without the need for recurrent or convolutional operations.</p>
</blockquote>
<p>Each token ID in the vocabulary is matched to a multi-dimensional vector, and the <u>intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence</u>.</p>
<p>原理概要: </p>
<ol>
<li><p>字被轉為token，有其standard，可能照一個word或是一個word的部份(我們知道”部分”一般有其函義，如er), 在生成時要用一樣的分詞器喲</p>
<ul>
<li>回顧word2vec paper的概念。然後transformer paper裡用的是512的vector</li>
</ul>
</li>
<li><p>放入嵌入層，結合位置層的info以保留字間的關係</p>
</li>
<li><p>attention 允許模型關注輸入序列的不同部分，以更好地捕獲單詞之間的上下文依賴關係。 在訓練期間學習並存儲在這些層中的自註意力權重反映了重要性，該輸入序列中每個單詞相對於序列中所有其他單詞的比例</p>
</li>
<li><p>這會有多頭的attention，每個關注可能不同特性，如人物、實體、壓韻等等</p>
</li>
<li><p>最後傳給FC 是個logits 向量，與 tokenizer 字典中每個 token 的概率得分成正比。</p>
</li>
<li><p>然後，您可以將這些 logits 傳遞到最終的 softmax 層，在其中將它們標準化為每個單詞的概率得分。，然後出去給softmax</p>
</li>
</ol>
<p>架構概要:</p>
<ul>
<li>“Attention is All You Need” 是由 Google 研究員在 2017 年發表的一篇研究論文，其中引入了 Transformer 模型，這是一種革新性的架構，徹底改變了自然語言處理（NLP）領域，並成為我們現在所知道的如 GPT、PaLM 等大型語言模型 (LLMs) 的基礎。這篇論文提出一種神經網絡架構，用全注意力機制取代傳統的遞歸神經網絡（RNNs）和卷積神經網絡（CNNs）。</li>
<li>Transformer 模型使用自我注意力來計算輸入序列的表示，這使它能有效地捕捉長期依賴性並並行計算。作者證明他們的模型在多個機器翻譯任務上達到了最先進的性能，並超越了依賴 RNNs 或 CNNs 的先前模型。</li>
<li>Transformer 架構由編碼器和解碼器組成，每個部分都由多個層組成。每個層都由兩個子層組成：多頭自我注意力機制和前饋神經網絡。多頭自我注意力機制使模型能夠關注輸入序列的不同部分，而前饋網絡則對每個位置分別且相同地應用全連接層。</li>
<li>Transformer 模型還使用殘差連接和層正規化來促進訓練並防止過度擬合。此外，作者引入了一種位置編碼方案，對輸入序列中每個 token 的位置進行編碼，使模型能夠在無需遞歸或卷積操作的情況下捕捉序列的順序。</li>
</ul>
<img src="https://p.ipic.vip/9tgj64.png" alt="image-20230801112600856" style="zoom: 25%;" />

<img src="https://p.ipic.vip/ybxbno.png" alt="image-20230801112636111" style="zoom:25%;" />





<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230801113212896.png" alt="image-20230801113212896" style="zoom:25%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230801113302281.png" alt="image-20230801113302281" style="zoom:25%;" />



<img src="https://p.ipic.vip/pc6kc2.png" alt="image-20230801113514240" style="zoom:25%;" />

<img src="https://p.ipic.vip/hs81dx.png" alt="image-20230801113706004" style="zoom:25%;" />



<h2 id="Gen-text-w-transformer"><a href="#Gen-text-w-transformer" class="headerlink" title="Gen text w/ transformer"></a>Gen text w/ transformer</h2><p>此時，離開編碼器的數據是輸入序列的結構和含義的深度表示<br>離開編碼器的數據是輸入序列的結構和含義的深度表示。 該表示被插入到解碼器的中間以影響解碼器的自註意力機制。 接下來，將序列開始標記添加到解碼器的輸入。 這會觸發解碼器預測下一個令牌，它是根據編碼器提供的上下文理解來預測下一個令牌的。 解碼器自註意力層的輸出通過解碼器前饋網絡並通過最終的 softmax 輸出層。 此時，我們有了第一個令牌。</p>
<p>可以通過多種方式使用 softmax 層的輸出來預測下一個標記。 這些會影響生成的文本的創意程度。</p>
<p><img src="https://p.ipic.vip/4f1c29.png" alt="image-20230801120748737"></p>
<h2 id="Promp-Eng-In-Context-Learning"><a href="#Promp-Eng-In-Context-Learning" class="headerlink" title="Promp Eng. (In-Context Learning)"></a>Promp Eng. (In-Context Learning)</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230801145113929.png" alt="image-20230801145113929" style="zoom:15%;" />



<h2 id="Generative-Configuration"><a href="#Generative-Configuration" class="headerlink" title="Generative Configuration"></a>Generative Configuration</h2><img src="https://p.ipic.vip/nrlcgd.png" alt="image-20230801145459701" style="zoom:25%;" />



<table>
<thead>
<tr>
<th>參數名稱</th>
<th>解釋</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td><strong>max_new_tokens</strong></td>
<td>限制模型生成的最大token數量。這可以被視為對模型選擇過程的最大次數設定一個上限。</td>
<td>假設 max_new_tokens 被設定為 100，150，或 200，模型生成的token數將不會超過這些數值。</td>
</tr>
<tr>
<td><strong>sample_top_k</strong></td>
<td>給模型指定只從具有最高概率的 k 個token中選擇。這可以幫助模型有一定的隨機性，同時避免選擇高度不可能的完成詞。</td>
<td>如果 k 被設定為 3，模型將只從最可能的三個選項中選擇。</td>
</tr>
<tr>
<td><strong>sample_top_p</strong></td>
<td>限制隨機抽樣只用於總概率不超過 p 的預測。也就是說，所有選擇的概率加起來不會超過 p。</td>
<td>如果 p 被設定為 0.3，模型會選擇概率為 0.2 和 0.1 的 token，因為他們的概率和為 0.3。</td>
</tr>
<tr>
<td><strong>temperature</strong></td>
<td>控制模型輸出隨機性的參數。它影響模型計算下一個token的概率分佈的形狀。溫度值越高，隨機性越高；溫度值越低，隨機性越低。</td>
<td>假設溫度值低於1，這將會使 softmax 層的概率分佈更為集中，大多數概率集中在少數幾個詞上；如果溫度值設定為高於1，則模型將計算出一個較為平坦的概率分佈，概率會在各個token間更為均勻。</td>
</tr>
</tbody></table>
<blockquote>
<p>這四種參數（max_new_tokens，sample_top_k，sample_top_p，以及 temperature）都與 softmax 層的輸出有關，並且主要影響模型在推斷階段生成文本的方式</p>
</blockquote>
<h2 id="Proj-Lifecycle"><a href="#Proj-Lifecycle" class="headerlink" title="Proj Lifecycle"></a>Proj Lifecycle</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230801152653222.png" alt="image-20230801152653222" style="zoom: 33%;" />



<h2 id="LAB1-FlanT5-prompt-eng"><a href="#LAB1-FlanT5-prompt-eng" class="headerlink" title="LAB1 - FlanT5, prompt eng."></a>LAB1 - FlanT5, prompt eng.</h2><ul>
<li>general model, prompt eng.</li>
</ul>
<p>Flan-T5</p>
<p>用不到的instructions prompt, 看model是不是有表現得比較好？</p>
<p>one-shot ，用一個完整的例子，包含「這個人類覺得的answer」，然後給第二個例子，讓它作出如summary的answer</p>
<p>not cheating, we’re helping the model itself , inexpensive way to try out models! </p>
<ul>
<li>實驗摘要：</li>
</ul>
<ol>
<li>實驗開始於使用Python 3和特定的程式庫進行設定，包括 PyTorch, Torch data 和 Transformers 等。</li>
<li>實驗的目標是用語言模型（在此為FLAN-T5模型）對對話進行摘要，用於比如客服對話等場景。</li>
<li>實驗使用的資料集名為 Dialogue sum，其中包含人與人之間的對話以及對話的人工摘要，作為基線比較。</li>
<li>首先，直接傳遞原始對話給模型進行摘要，結果並不理想，許多細節並未被捕捉到。</li>
<li>為了提高摘要的質量，實驗引入了 “prompt engineering” 的概念，即傳遞特定的指令來改變模型的行為。這包括 “zero-shot inference with an instruction”、 “one-shot learning” 和 “few-shot learning”。雖然有些改善，但效果仍然不是很好。</li>
<li>在 “one-shot learning” 中，給模型提供一個完整的對話範例（包括人工摘要），然後給出第二個需要模型進行摘要的對話。這比zero-shot有些許改善。</li>
<li>在 “few-shot learning” 中，給模型提供多個完整的對話範例（包括人工摘要），然後給出另一個需要模型進行摘要的對話。結果顯示，與one-shot相比，這並沒有太大的改善，顯示增加更多範例可能並不會提高摘要的質量。</li>
<li>最後，實驗將探索如何調整生成模型的參數，如調整溫度以影響模型生成的創新程度。例如，增加溫度可以讓模型生成更有創意的回應，而降低溫度會讓模型的回應更為保守。</li>
</ol>
<ul>
<li>比較不同的 “shot”：</li>
</ul>
<ol>
<li>“Zero-shot”：直接傳遞原始對話給模型進行摘要，並沒有給模型任何對話範例。此種情況下，模型並不能很好地摘要對話，很多細節並未被捕捉到。</li>
<li>“One-shot”：給模型提供一個完整的對話範例（包括人工摘要），然後給出第二個需要模型進行摘要的對話。此種情況下，模型的摘要有所改善。</li>
<li>“Few-shot”：給模型提供多個完整的對話範例（包括人工摘要），然後給出另一個需要模型進行摘要的對話。結果顯示，與one-shot相比，這並沒有太大的改善，顯示增加更多範例可能並不會提高摘要的質量。</li>
</ol>
<h1 id="LLM-Pretraining-amp-Scaling-laws"><a href="#LLM-Pretraining-amp-Scaling-laws" class="headerlink" title="LLM Pretraining &amp; Scaling laws"></a>LLM Pretraining &amp; Scaling laws</h1><h2 id="Pretraining-LLMs"><a href="#Pretraining-LLMs" class="headerlink" title="Pretraining LLMs"></a>Pretraining LLMs</h2><p>Model Hubs</p>
<p><img src="https://p.ipic.vip/9ueya9.png" alt="image-20230801224427672"></p>
<p>LLMs encode a deep statistical representation of language. </p>
<ol>
<li><p>LLM 的初始訓練過程通常被稱為預訓練階段。在此階段，模型從大量的非結構化文本數據中學習，以編碼深度統計語言表示。這個學習過程需要大量的計算資源和 GPU。</p>
</li>
<li><p>預訓練階段的目標是最小化訓練目標的損失。在此過程中，模型權重會被更新，編碼器會為每個令牌生成一個嵌入或向量表示。</p>
</li>
<li><p>有三種 Transformer 模型的變體：只有編碼器的模型、只有解碼器的模型和包含編碼器和解碼器的模型。每一種都有不同的訓練目標，因此學習執行不同的任務。</p>
</li>
<li><ul>
<li>只有編碼器的模型（如BERT和RoBERTa）是使用遮蔽語言建模進行預訓練的，適合從雙向上下文中受益的任務，例如情感分析或命名實體識別。</li>
<li>只有解碼器的模型（如GBT和BLOOM）是使用因果語言建模進行預訓練的，常用於文本生成。這些模型通過預測下一個令牌來建立語言的統計表示。</li>
<li>序列到序列模型（如T5和BART）同時使用編碼器和解碼器進行預訓練，適用於翻譯、摘要和問答等任務。T5 的全名是 “Text-to-Text Transfer Transformer”。這是一個通用的大型語言模型，由 Google Research 的團隊開發</li>
</ul>
</li>
<li><p>儘管更大的模型通常更能有效地完成任務，但訓練這些巨大的模型非常困難且成本高昂，因此不斷訓練更大和更大的模型可能是不可行的。對此，下一段視頻將進一步探討訓練大型模型所面臨的挑戰</p>
</li>
</ol>
<h3 id="Encode-only"><a href="#Encode-only" class="headerlink" title="Encode-only"></a>Encode-only</h3><img src="https://p.ipic.vip/dc5zn5.png" alt="image-20230801230449054" style="zoom: 33%;" />





<h3 id="Decode-only"><a href="#Decode-only" class="headerlink" title="Decode-only"></a>Decode-only</h3><img src="https://p.ipic.vip/pcrzmg.png" alt="image-20230801230817344" style="zoom:33%;" />



<h3 id="Encode-Decode-sequence2sequence"><a href="#Encode-Decode-sequence2sequence" class="headerlink" title="Encode+Decode, sequence2sequence"></a>Encode+Decode, sequence2sequence</h3><img src="https://p.ipic.vip/2iq460.png" alt="image-20230801231356176" style="zoom: 33%;" />



<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary!"></a>Summary!</h3><img src="https://p.ipic.vip/bmhcxf.png" alt="image-20230801231840733" style="zoom:33%;" />

<table>
<thead>
<tr>
<th>模型類型</th>
<th>訓練目標</th>
<th>應用場景</th>
<th>是否有利於大模型</th>
<th>代表模型</th>
</tr>
</thead>
<tbody><tr>
<td>Encoder-only (自編碼模型)</td>
<td>使用蒙版語言建模，隨機遮蔽輸入序列中的 tokens，並試圖預測遮蔽的 tokens</td>
<td>句子分類任務（例如情感分析），或 token 級任務（例如命名實體識別或單詞分類）</td>
<td>一般</td>
<td>BERT, RoBERTa</td>
</tr>
<tr>
<td>Decoder-only (自回歸模型)</td>
<td>使用因果語言建模，基於前面的一系列 tokens 來預測下一個 token</td>
<td>文本生成；對於大型模型，展示出強大的零樣本推理能力，並能很好地完成各種任務</td>
<td>對於大模型有利</td>
<td>GPT, BLOOM</td>
</tr>
<tr>
<td>Encoder-Decoder (序列對序列模型)</td>
<td>使用跨度腐敗（Span Corruption）進行訓練，隨機遮蔽輸入 token 的序列</td>
<td>翻譯、摘要、問答</td>
<td>一般</td>
<td>T5, BART</td>
</tr>
</tbody></table>
<p>請注意，儘管大型的 Decoder-only 模型在應用場景上更為廣泛，但是訓練這些模型的資源需求、訓練難度和成本也相對較高。</p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230801232210879.png" alt="image-20230801232210879" style="zoom: 25%;" />



<h2 id="Computational-Challenges-of-training-LLMs"><a href="#Computational-Challenges-of-training-LLMs" class="headerlink" title="Computational Challenges of training LLMs"></a>Computational Challenges of training LLMs</h2><blockquote>
<p>OutOfMemoryError: CUDA out of memory..!</p>
</blockquote>
<img src="https://p.ipic.vip/qgeop8.png" alt="image-20230802013617666" style="zoom:25%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802013643585.png" alt="image-20230802013643585" style="zoom:25%;" />



<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802013853436.png" alt="image-20230802013853436" style="zoom:25%;" />





<table>
<thead>
<tr>
<th>數據類型</th>
<th>bits</th>
<th>指數</th>
<th>分數</th>
<th>記憶體需求</th>
</tr>
</thead>
<tbody><tr>
<td>FP32</td>
<td>32</td>
<td>8</td>
<td>23</td>
<td>4 bytes</td>
</tr>
<tr>
<td>FP16</td>
<td>16</td>
<td>5</td>
<td>10</td>
<td>2 bytes</td>
</tr>
<tr>
<td>BFLOAT16, *FLAN-T5用這個</td>
<td>16</td>
<td>8</td>
<td>7</td>
<td>2 bytes</td>
</tr>
<tr>
<td>INT8</td>
<td>8</td>
<td>N/A</td>
<td>N/A</td>
<td>1 byte</td>
</tr>
</tbody></table>
<p>*注意：對於INT8，並不會有指數和分數的表示，因為它是一個整數類型。</p>
<p>此外，這裡也是一些重要的摘要點：</p>
<ol>
<li>量化的目標是通過降低模型權重的精度來減少存儲和訓練模型所需的記憶體。</li>
<li>量化將原始的32位浮點數統計投影到使用基於原始32位浮點數範圍計算的縮放因子的更低精度空間。</li>
<li>現代的深度學習框架和庫支持量化感知訓練 Quantization-aware training(QAT)，這在訓練過程中學習量化縮放因子。</li>
<li>量化可以用來減少訓練過程中模型的記憶體占用。</li>
<li>BFLOAT16已經成為深度學習中的一種熱門精度選擇，因為它保持了FP32的動態範圍，但將記憶體占用減半。</li>
<li>許多大型語言模型（LLMs），包括FLAN-T5，都已經用BFLOAT16預訓練過。</li>
</ol>
<p>存1Billion的FP32需要4GB的RAM</p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802014842420.png" alt="image-20230802014842420" style="zoom: 25%;" />



<h2 id="Training-Across-GPUs"><a href="#Training-Across-GPUs" class="headerlink" title="Training Across GPUs"></a>Training Across GPUs</h2><h3 id="DDP，-Distributed-Data-Parallel"><a href="#DDP，-Distributed-Data-Parallel" class="headerlink" title="DDP，(Distributed Data Parallel)"></a>DDP，(Distributed Data Parallel)</h3><p><img src="https://p.ipic.vip/1c4szn.png" alt="image-20230802112224890"></p>
<h3 id="ZeRO-Zero-Redundancy-Optimizer"><a href="#ZeRO-Zero-Redundancy-Optimizer" class="headerlink" title="ZeRO, (Zero Redundancy Optimizer)"></a>ZeRO, (Zero Redundancy Optimizer)</h3><p>ZeRO 是由 Microsoft 的深度速度團隊開發的一種技術，該技術主要針對模型參數、優化器狀態和梯度進行優化，以減少 GPU 記憶體的使用並提高訓練大型模型的效率。</p>
<p>以下是一些關於 ZeRO 的更多信息：</p>
<ol>
<li><strong>模型參數、優化器狀態和梯度的分片</strong>：ZeRO 的核心思想是分片模型參數、優化器狀態和梯度。這意味著這些元素被均勻地分散在所有可用的 GPU 中，而不是在每個 GPU 上存儲完整的副本。這消除了冗餘的記憶體使用，並允許在 GPU 記憶體有限的情況下訓練更大的模型。</li>
<li><strong>優化器狀態分片（ZeRO-1）</strong>：這是減少記憶體使用量的第一步，優化器狀態通常比模型參數需要更多的記憶體，因此分片它們可以節省大量記憶體。</li>
<li><strong>梯度分片（ZeRO-2）</strong>：當應用 ZeRO-1 的同時，也將梯度分片，可以進一步減少記憶體使用量。</li>
<li><strong>模型參數分片（ZeRO-3）</strong>：此階段將模型參數分片，進一步節省記憶體。這使得模型的規模可以超過單個 GPU 的記憶體限制。</li>
<li><strong>通訊優化</strong>：ZeRO 還包括一種被稱為 “重疊通訊與計算” 的技術，這種技術可以在計算過程中進行數據的交換，從而減少等待時間並提高效率。</li>
<li><strong>兼容性</strong>：ZeRO 與 PyTorch 和 TensorFlow 等深度學習框架都相容。具體實現包括 DeepSpeed（一個由微軟開發的深度學習優化庫）和 FairScale（Facebook 的一個開源專案）。</li>
<li><strong>與其他分散式訓練策略的組合</strong>：ZeRO 可以與 Data Parallelism 和 Model Parallelism 等其他分散式訓練策略結合使用，以進一步提高訓練大型模型的效率。例如，微軟的 DeepSpeed 團隊提出了一種稱為 “ZeRO-Offload” 的技術，該技術將一部分的優化器狀態和梯度計算卸載到 CPU，進一步減少 GPU 記憶體的使用並允許在有限的硬體資源下訓練更大的模型。</li>
</ol>
<p>這些特性使 ZeRO 成為一種非常強大的工具，尤其是對於訓練大型深度學習模型的場景，它可以有效地解決記憶體限制問題。</p>
<h3 id="FSDP-Fully-Sharded-Data-Parallelism"><a href="#FSDP-Fully-Sharded-Data-Parallelism" class="headerlink" title="FSDP, (Fully Sharded Data Parallelism)"></a>FSDP, (Fully Sharded Data Parallelism)</h3><ol>
<li><strong>FSDP 概述</strong>：FSDP (Fully Sharded Data Parallelism) 是一種分散式訓練技術，它結合了數據平行訓練（如 DDP）與 ZeRO 的模型狀態分片技術。當使用 FSDP 時，不僅數據分佈在多個 GPU 中，模型參數、梯度和優化器狀態也跨 GPU 節點進行分片。</li>
<li><strong>操作過程</strong>：與 DDP 不同，FSDP 在執行前向和反向傳播時需要收集所有 GPU 的數據。每個 CPU 都會按需從其他 GPU 獲取數據，將分片的數據臨時轉換為未分片的數據以進行操作。操作完成後，將非本地的未分片數據釋放回其他 GPU，或者可以選擇在後續操作（例如反向傳播）中保留它。</li>
<li><strong>梯度同步</strong>：反向傳播完成後，FSDP 會以與 DDP 相同的方式跨 GPU 同步梯度。</li>
<li><strong>模型分片</strong>：FSDP 允許進行模型分片，從而減少總體的 GPU 記憶體使用。你還可以選擇讓 FSDP 把部分訓練計算卸載到 GPU，以進一步減少 GPU 記憶體使用。</li>
<li><strong>分片級別的配置</strong>：你可以使用 FSDP 的分片因子來管理性能和記憶體使用之間的權衡。分片因子為1就意味著去除分片並複製完整模型，這與 DDP 類似。如果將分片因子設置為最大的 GPU 數量，則開啟完全分片，這樣可以節省最多的記憶體，但會增加 GPU 之間的通訊量。設定在這兩者之間的任何分片因子則可啟用超級分片。</li>
<li><strong>FSDP vs DDP 性能比較</strong>：根據實驗數據，當模型大小超過22.8億參數時，DDP 會遇到記憶體不足的問題，而 FSDP 則可以輕鬆處理這種大小的模型，並在降低模型精度至16位時達到更高的 teraflops。然而，當模型在越來越多的 GPU 上分佈時，通訊量的增加會開始影響性能，使計算變慢。</li>
<li><strong>適用場景</strong>：你可以在小型和大型模型上使用 FSDP，並無縫地將模型訓練擴展到多個 GPU。然而，由於跨 GPU 訓練模型的成本和技術複雜性，一些研究者正在尋找能夠用更小模型達到更好性能的方法。</li>
</ol>
<p>：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">FSDP - Full Sharding</th>
<th align="center">FSDP - Hyper Sharding</th>
<th align="center">FSDP - Full Replication</th>
<th align="center">DDP</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>611 Million Parameters (Model 1-25)</strong></td>
<td align="center">Similar Performance to DDP</td>
<td align="center">Not Provided</td>
<td align="center">Not Provided</td>
<td align="center">Similar Performance to FSDP</td>
</tr>
<tr>
<td align="center"><strong>2.28 Billion Parameters (Model 1-25)</strong></td>
<td align="center">Similar Performance to DDP</td>
<td align="center">Not Provided</td>
<td align="center">Not Provided</td>
<td align="center">Similar Performance to FSDP</td>
</tr>
<tr>
<td align="center"><strong>11.3 Billion Parameters (Model 25)</strong></td>
<td align="center">Can Handle and Achieve Higher Teraflops with 16-bit Precision</td>
<td align="center">Not Provided</td>
<td align="center">Not Provided</td>
<td align="center">Out-of-Memory Error</td>
</tr>
<tr>
<td align="center"><strong>Impact of Increasing GPU from 8 to 512 (11 Billion T5 Model)</strong></td>
<td align="center">7% Decrease in per GPU Teraflops</td>
<td align="center">Not Provided</td>
<td align="center">Not Provided</td>
<td align="center">Not Provided</td>
</tr>
</tbody></table>
<blockquote>
<ul>
<li><p>FSDP - Full Replication 跟    DDP 意義上有什麼差別呀？如下</p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>FSDP - Full Replication</th>
<th>DDP</th>
</tr>
</thead>
<tbody><tr>
<td>Data Parallelism</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Model Replication</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Memory Management</td>
<td>More efficient (due to model sharding)</td>
<td>Standard (full model on each GPU)</td>
</tr>
<tr>
<td>Sharding Control</td>
<td>Flexible (allows trade-off between performance and memory usage)</td>
<td>No flexibility</td>
</tr>
<tr>
<td>Suitable for Large Models</td>
<td>More suitable (due to model sharding and flexible control)</td>
<td>Less suitable for extremely large models</td>
</tr>
</tbody></table>
</li>
</ul>
</blockquote>
<h3 id="Scaling-Laws-amp-Compute-Optimal-Models"><a href="#Scaling-Laws-amp-Compute-Optimal-Models" class="headerlink" title="Scaling Laws &amp; Compute-Optimal Models"></a>Scaling Laws &amp; Compute-Optimal Models</h3><p>每秒一千萬萬次的浮點運算（petaFLOP/s）相當於八個NVIDIA V100 GPU滿載運行一整天，或者兩個NVIDIA A100 GPU滿載運行一整天。</p>
<p>根據這個比例，我們可以計算出下表：</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Parameter Quantity</th>
<th>Compute Budget (petaFLOP/s-days)</th>
<th>Equivalent No. of NVIDIA V100 GPUs (1 day)</th>
<th>Equivalent No. of NVIDIA A100 GPUs (1 day)</th>
</tr>
</thead>
<tbody><tr>
<td>T5 XL</td>
<td>3 billion</td>
<td>100</td>
<td>800</td>
<td>200</td>
</tr>
<tr>
<td>GPT-3</td>
<td>175 billion</td>
<td>3700</td>
<td>29600</td>
<td>7400</td>
</tr>
</tbody></table>
<p>需要注意的是，這裡的計算僅僅是個概略的估算，真實情況可能會因為其他因素（例如具體的訓練實現、軟體優化等）而有所不同。</p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802121714590.png" alt="image-20230802121714590" style="zoom:33%;" />

<p><img src="https://p.ipic.vip/ysdk4x.png" alt="image-20230802122024515"></p>
<p> Jordan Hoffmann, Sebastian Borgeaud 和 Arthur Mensch 在 2022 年發表的研究。該研究的目標是找出給定計算預算下的最佳參數數量和訓練數據量，並產生了一個被稱為 “Chinchilla” 的計算最優模型。以下是文章的主要摘要和比較：</p>
<ol>
<li><strong>過度參數化與訓練不足</strong>：許多具有 1000 億參數的大型語言模型（如 GPT-3）可能實際上是過度參數化的，這意味著它們的參數數量超過了獲得良好語言理解的需要，並且訓練不足，對更多的訓練數據有利。作者假設，如果在更大的數據集上訓練，較小的模型可能能夠達到與大型模型相同的性能。</li>
<li><strong>訓練數據集的最佳大小</strong>：Chinchilla 的一個重要發現是，給定模型的最佳訓練數據集大小約為模型參數數量的 20 倍。例如，對於一個有 70 億參數的模型，最佳的訓練數據集包含 1.4 兆個 tokens。</li>
<li><strong>Chinchilla 模型的優勢</strong>：在大範圍的下游評價任務上，計算最優的 Chinchilla 模型優於非計算最優的模型，如 GPT-3。有了 Chinchilla 研究的結果，團隊已經開始開發相似或更好的模型，而這些模型比以非最優方式訓練的大型模型更小。</li>
<li><strong>模型設計的優化趨勢</strong>：未來可以期待看到更多團隊或開發者像你一樣開始優化他們的模型設計，從而偏離過去幾年的 “越大越好” 趨勢。</li>
<li><strong>Bloomberg GPT 模型</strong>：最後一個顯示在這張幻燈片上的 Bloomberg GPT 模型是一個很有趣的模型。該模型按照 Chinchilla 的方式進行了計算最優的訓練，因此，儘管參數數量為 500 億，但仍實現了良好的性能。此外，該模型也是一個從頭開始訓練模型以實現良好任務性能的有趣例子。</li>
</ol>
<h2 id="Pretraining-for-Domain-Adaptation"><a href="#Pretraining-for-Domain-Adaptation" class="headerlink" title="Pretraining for Domain Adaptation"></a>Pretraining for Domain Adaptation</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20230802124542725.png" alt="image-20230802124542725" style="zoom: 33%;" />

<h4 id="BloombergGPT"><a href="#BloombergGPT" class="headerlink" title="BloombergGPT"></a>BloombergGPT</h4><p>BloombergGPT 是一種只有解碼器的大型語言模型，特別是為金融領域做預訓練。Bloomberg 的研究人員選擇結合金融數據和通用稅務數據進行預訓練，該模型在金融基準測試上獲得最佳成績，同時在通用語言模型基準測試上也表現競爭力。因此，研究人員選擇的數據由51%的金融數據和49%的公共數據組成。</p>
<p>在訓練 BloombergGPT 的過程中，作者使用了 Chinchilla 縮放規則來指導模型中的參數數量和訓練數據的體積（以 token 衡量）。Chinchilla 的建議在圖像中由 Chinchilla-1、Chinchilla-2 和 Chinchilla-3 的線條表示，我們可以看到 BloombergGPT 跟它們很接近。</p>
<p>雖然為團隊可用的訓練計算預算推薦的配置是 500 億個參數和 1.4 兆個 token，但在金融領域獲取 1.4 兆個 token 的訓練數據證明具有挑戰性。因此，他們構建了一個只包含 7000 億個 token 的數據集，少於計算最佳值。此外，由於早期停止，訓練過程在處理了 5690 億個 token 後結束。</p>
<p>BloombergGPT 項目是一個很好的例子，說明了增加模型特定領域性的預訓練，以及可能迫使模型和訓練配置作出與計算最佳模型相對的權衡的挑戰。</p>
<h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><p>Which of the following statements about pretraining scaling laws are correct? Select all that apply:</p>
<ul>
<li><p>To scale our model, we need to jointly increase dataset size and model size, or they can become a bottleneck for each other.</p>
<ul>
<li>Incorrect</li>
</ul>
</li>
<li><p>There is a relationship between model size (in number of parameters) and the optimal number of tokens to train the model with.</p>
<ul>
<li><p>Correct</p>
<p>This relationship is describe in the Chinchilla paper, that shows that many models might even be overparametrized according to the relationship they found.</p>
</li>
</ul>
</li>
<li><p>When measuring compute budget, we can use “PetaFlops per second-Day” as a metric.</p>
<ul>
<li><p>Correct</p>
<p>Petaflops per second-day is a useful measure for computing budget as it reflects the both hardware and time required to train the model.</p>
</li>
</ul>
</li>
<li><p>You should always follow the recommended number of tokens, based on the chinchilla laws, to train your model.</p>
<ul>
<li>This should not be selected<ul>
<li>Although compute optimization is important, it can be challenging to obtain a sufficient amount of data tokens. In the case of BloombergGPT, they had a limited token count and even used fewer tokens due to early stopping. While chinchilla laws offer valuable guidance, they should not be strictly followed as rigid rules.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="HW"><a href="#HW" class="headerlink" title="HW"></a>HW</h1><p>What is the <em>self-attention</em> that powers the transformer architecture?</p>
<p>1 point</p>
<p>v A mechanism that allows a model to focus on different parts of the input sequence during computation.</p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/82/">82</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2024 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>