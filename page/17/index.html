<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.1.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">188</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">22</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Awaken Desparado</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CVs/2017-12-04-SSH%20publickey%20on%20gitllab%20&amp;%20conda/">CVs/2017-12-04-SSH publickey on gitllab &amp; conda</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/DevOps/">DevOps</a></span><div class="content"><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1g9oejag0wxj31hr0u0gre.jpg" alt="image-20191207195200870"></p>
<p><img src="https://tva1.sinaimg.cn/large/006tNbRwly1g9oekmmtasj30md019t96.jpg" alt="image-20191207195318751"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> git@git.xxxxxx</span><br></pre></td></tr></table></figure>



<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ conda create -n joe_py36_hpc1 python=3.6</span><br><span class="line">Fetching package metadata .....</span><br></pre></td></tr></table></figure>



<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">532  python video_creater.py</span><br><span class="line">533  pip install Pillow</span><br><span class="line">534  python video_creater.py</span><br><span class="line">535  pip install moviepy</span><br><span class="line">536  python video_creater.py</span><br><span class="line">537  pip install ffmpy3</span><br><span class="line">538  python video_creater.py</span><br><span class="line">539  pip install opencv</span><br><span class="line">540  pip install opencv-python</span><br><span class="line">541  python video_creater.py</span><br><span class="line">542  pip install sklearn</span><br><span class="line">543  python video_creater.py</span><br><span class="line">544  <span class="built_in">history</span> | tail -20</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CVs/2019-12-12-Problems%20Collection%20from%20EGN/">CVs/2019-12-12-Problems Collection from EGN</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/DevOps/">DevOps</a></span><div class="content"><h3 id="LC-46-Permutation"><a href="#LC-46-Permutation" class="headerlink" title="LC 46 - Permutation"></a>LC 46 - Permutation</h3></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CVs/2016-12-10-ML-Regularization/">CVs/2016-12-10-ML-Regularization</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/ML/">ML</a></span><div class="content"><h2 id="1-The-Problem-of-Overfitting"><a href="#1-The-Problem-of-Overfitting" class="headerlink" title="1. The Problem of Overfitting"></a>1. The Problem of Overfitting</h2><p><a href="https://images0.cnblogs.com/blog/663864/201411/081949249876584.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081949392689088.png" alt="image"></a></p>
<h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>还是来看预测房价的这个例子，我们先对该数据做线性回归，也就是左边第一张图。</p>
<p>如果这么做，我们可以获得拟合数据的这样一条直线，但是，实际上这并不是一个很好的模型。我们看看这些数据，很明显，随着房子面积增大，住房价格的变化趋于稳定或者说越往右越平缓。因此线性回归并没有很好拟合训练数据。</p>
<blockquote>
<p><strong>我们把此类情况称为欠拟合(underfitting)，或者叫作叫做高偏差(bias)。</strong></p>
</blockquote>
<p>这两种说法大致相似，都表示没有很好地拟合训练数据。高偏差这个词是 machine learning 的研究初期传下来的一个专业名词，具体到这个问题，意思就是说如果用线性回归这个算法去拟合训练数据，那么该算法实际上会产生一个非常大的偏差或者说存在一个很强的偏见。</p>
<p>第二幅图，我们在中间加入一个二次项，也就是说对于这幅数据我们用二次函数去拟合。自然，可以拟合出一条曲线，事实也证明这个拟合效果很好。</p>
<p>另一个极端情况是，如果在第三幅图中对于该数据集用一个四次多项式来拟合。因此在这里我们有五个参数θ0到θ4，这样我们同样可以拟合一条曲线，通过我们的五个训练样本，我们可以得到如右图的一条曲线。</p>
<p>一方面，我们似乎对训练数据做了一个很好的拟合，因为这条曲线通过了所有的训练实例。但是，这实际上是一条很扭曲的曲线，它不停上下波动。因此，事实上我们并不认为它是一个预测房价的好模型。</p>
<blockquote>
<p><strong>所以，我们把这类情况叫做过拟合(overfitting)，也叫高方差(variance)。</strong></p>
</blockquote>
<p>与高偏差一样，高方差同样也是一个历史上的叫法。从第一印象上来说，如果我们拟合一个高阶多项式，那么这个函数能很好的拟合训练集（能拟合几乎所有的训练数据），但这也就面临函数可能太过庞大的问题，变量太多。</p>
<p><strong>同时如果我们没有足够的数据集（训练集）去约束这个变量过多的模型，那么就会发生过拟合。</strong></p>
<h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>过度拟合的问题通常发生在变量（特征）过多的时候。这种情况下训练出的方程总是能很好的拟合训练数据，也就是说，我们的代价函数可能非常接近于 0 或者就为 0。</p>
<p>但是，这样的曲线千方百计的去拟合训练数据，这样会导致它无法泛化到新的数据样本中，以至于无法预测新样本价格。在这里，<strong>术语”泛化”指的是一个假设模型能够应用到新样本的能力。</strong>新样本数据是指没有出现在训练集中的数据。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081950012995058.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081950152375432.png" alt="image"></a><br>之前，我们看到了线性回归情况下的过拟合。类似的情况也适用于逻辑回归。</p>
<h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><p>那么，如果发生了过拟合问题，我们应该如何处理？</p>
<p>过多的变量（特征），同时只有非常少的训练数据，会导致出现过度拟合的问题。因此为了解决过度拟合，有以下两个办法。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081950204873029.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081950339879703.png" alt="image"></a></p>
<blockquote>
<p>*<em>方法一：尽量减少选取变量的数量<br>*</em></p>
</blockquote>
<p>具体而言，我们可以人工检查每一项变量，并以此来确定哪些变量更为重要，然后，保留那些更为重要的特征变量。至于，哪些变量应该舍弃，我们以后在讨论，这会涉及到模型选择算法，这种算法是可以自动选择采用哪些特征变量，自动舍弃不需要的变量。这类做法非常有效，但是其缺点是当你舍弃一部分特征变量时，你也舍弃了问题中的一些信息。例如，也许所有的特征变量对于预测房价都是有用的，我们实际上并不想舍弃一些信息或者说舍弃这些特征变量。</p>
<blockquote>
<p><strong>方法二：正则化</strong></p>
</blockquote>
<p>正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。</p>
<p>这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。</p>
<p>接下来我们会讨论怎样应用正则化和什么叫做正则化均值，然后将开始讨论怎样使用正则化来使学习算法正常工作，并避免过拟合。</p>
<h2 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h2><p><a href="https://images0.cnblogs.com/blog/663864/201411/081950392065543.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081950460499853.png" alt="image"></a></p>
<h3 id="1-1"><a href="#1-1" class="headerlink" title="1"></a>1</h3><p>在前面的介绍中，我们看到了如果用一个二次函数来拟合这些数据，那么它给了我们一个对数据很好的拟合。然而，如果我们用一个更高次的多项式去拟合，最终我们可能会得到一个曲线，它能很好地拟合训练集，但却并不是一个好的结果，因为它过度拟合了数据，因此，一般性并不是很好。</p>
<p>让我们考虑下面的假设，我们想要加上<strong>惩罚项</strong>，从而使参数 θ3 和 θ4 足够的小。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081950493939765.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081950538152248.png" alt="image"></a></p>
<p>这里我的意思就是，上图的式子是我们的优化目标，也就是说我们需要尽量减少代价函数的均方误差。</p>
<p>对于这个函数我们对它添加一些项，加上 1000 乘以 θ3 的平方，再加上 1000 乘以 θ4 的平方，</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081950595497528.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081951047539138.png" alt="image"></a></p>
<p>1000 只是我随便写的某个较大的数字而已。现在，如果我们要最小化这个函数，那么为了最小化这个新的代价函数，我们要让 θ3 和 θ4 尽可能小。因为，如果你在原有代价函数的基础上加上 1000 乘以 θ3 这一项 ，那么这个新的代价函数将变得很大，所以，当我们最小化这个新的代价函数时， 我们将使 θ3 的值接近于 0，同样 θ4 的值也接近于 0，就像我们忽略了这两个值一样。如果我们做到这一点（ θ3 和 θ4 接近 0 ），那么我们将得到一个近似的二次函数。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081951193621142.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081951300032292.png" alt="image"></a></p>
<p>因此，我们最终恰当地拟合了数据，我们所使用的正是二次函数加上一些非常小，贡献很小项（因为这些项的 θ3、 θ4 非常接近于0）。显然，这是一个更好的假设。</p>
<h3 id="2-1"><a href="#2-1" class="headerlink" title="2"></a>2</h3><p><a href="https://images0.cnblogs.com/blog/663864/201411/081951469871136.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081951588935799.png" alt="image"></a></p>
<p>更一般地，这里给出了正规化背后的思路。这种思路就是，如果我们的参数值对应一个较小值的话（参数值比较小），那么往往我们会得到一个形式更简单的假设。</p>
<p>在我们上面的例子中，我们惩罚的只是 θ3 和 θ4 ，使这两个值均接近于零，从而我们得到了一个更简单的假设，实际上这个假设大抵上是一个二次函数。</p>
<p>但更一般地说，如果我们像惩罚 θ3 和 θ4 这样惩罚其它参数，那么我们往往可以得到一个相对较为简单的假设。</p>
<blockquote>
<p>*<em>实际上，这些参数的值越小，通常对应于越光滑的函数，也就是更加简单的函数。因此 就不易发生过拟合的问题。<br>*</em></p>
</blockquote>
<p>我知道，为什么越小的参数对应于一个相对较为简单的假设，对你来说现在不一定完全理解，但是在上面的例子中使 θ3 和 θ4 很小，并且这样做能给我们一个更加简单的假设，这个例子至少给了我们一些直观感受。</p>
<p>来让我们看看具体的例子，对于房屋价格预测我们可能有上百种特征，与刚刚所讲的多项式例子不同，我们并不知道 θ3 和 θ4 是高阶多项式的项。所以，如果我们有一百个特征，我们并不知道如何选择关联度更好的参数，如何缩小参数的数目等等。</p>
<p>因此在正则化里，我们要做的事情，就是把减小我们的代价函数（例子中是线性回归的代价函数）所有的参数值，因为我们并不知道是哪一个或哪几个要去缩小。</p>
<p>因此，我们需要修改代价函数，在这后面添加一项，就像我们在方括号里的这项。当我们添加一个额外的正则化项的时候，我们收缩了每个参数。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081952091127076.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081952152064015.png" alt="image"></a></p>
<p>顺便说一下，按照惯例，我们没有去惩罚 θ0，因此 θ0 的值是大的。这就是一个约定从 1 到 n 的求和，而不是从 0 到 n 的求和。但其实在实践中<br>这只会有非常小的差异，无论你是否包括这 θ0 这项。但是按照惯例，通常情况下我们还是只从 θ1 到 θn 进行正则化。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081952259717893.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081952330495431.png" alt="image"></a></p>
<p>下面的这项就是一个正则化项</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081952338934773.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081952345967158.png" alt="image"></a><br>并且 λ 在这里我们称做正则化参数。</p>
<blockquote>
<p><strong>λ 要做的就是控制在两个不同的目标中的平衡关系。</strong></p>
</blockquote>
<p><strong>第一个目标就是我们想要训练，使假设更好地拟合训练数据。</strong>我们希望假设能够很好的适应训练集。</p>
<p><strong>而第二个目标是我们想要保持参数值较小。（通过正则化项）</strong></p>
<p>而 λ 这个正则化参数需要控制的是这两者之间的平衡，即平衡拟合训练的目标和保持参数值较小的目标。从而来保持假设的形式相对简单，来避免过度的拟合。</p>
<p>对于我们的房屋价格预测来说，我们之前所用的非常高的高阶多项式来拟合，我们将会得到一个非常弯曲和复杂的曲线函数，现在我们只需要使用正则化目标的方法，那么你就可以得到一个更加合适的曲线，但这个曲线不是一个真正的二次函数，而是更加的流畅和简单的一个曲线。这样就得到了对于这个数据更好的假设。</p>
<p>再一次说明下，这部分内容的确有些难以明白，为什么加上参数的影响可以具有这种效果？但如果你亲自实现了正规化，你将能够看到这种影响的最直观的感受。</p>
<h3 id="3-1"><a href="#3-1" class="headerlink" title="3"></a>3</h3><p><a href="https://images0.cnblogs.com/blog/663864/201411/081952576123726.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081953140499371.png" alt="image"></a></p>
<p>在正则化线性回归中，如果正则化参数值 λ 被设定为非常大，那么将会发生什么呢？</p>
<p>我们将会非常大地惩罚参数θ1 θ2 θ3 θ4 … 也就是说，我们最终惩罚θ1 θ2 θ3 θ4 … 在一个非常大的程度，那么我们会使所有这些参数接近于零。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081953329569128.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081953465963762.png" alt="image"></a></p>
<p>如果我们这么做，那么就是我们的假设中相当于去掉了这些项，并且使我们只是留下了一个简单的假设，这个假设只能表明房屋价格等于 θ0 的值，那就是类似于拟合了一条水平直线，对于数据来说这就是一个欠拟合 (underfitting)。这种情况下这一假设它是条失败的直线，对于训练集来说这只是一条平滑直线，它没有任何趋势，它不会去趋向大部分训练样本的任何值。</p>
<p>这句话的另一种方式来表达就是这种假设有过于强烈的”偏见” 或者过高的偏差 (bais)，认为预测的价格只是等于 θ0 。对于数据来说这只是一条水平线。</p>
<p>因此，为了使正则化运作良好，我们应当注意一些方面，应该去选择一个不错的正则化参数 λ 。当我们以后讲到多重选择时我们将讨论一种方法来自动选择正则化参数 λ ，为了使用正则化，接下来我们将把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。</p>
<h2 id="3-Regularized-Linear-Regression"><a href="#3-Regularized-Linear-Regression" class="headerlink" title="3. Regularized Linear Regression"></a>3. Regularized Linear Regression</h2><p>之前我们已经介绍过，岭回归的代价函数如下：</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081953500814930.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081953557536697.png" alt="image"></a><br>对于线性回归(的求解)，我们之前运用了两种学习算法，一种基于梯度下降，一种基于正规方程。</p>
<h3 id="1-2"><a href="#1-2" class="headerlink" title="1"></a>1</h3><p>梯度下降，如下：</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081954132218927.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081954274561745.png" alt="image"></a></p>
<h3 id="2-2"><a href="#2-2" class="headerlink" title="2"></a>2</h3><p>正规方程，如下：</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081954343629568.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081954443152563.png" alt="image"></a></p>
<h3 id="3-2"><a href="#3-2" class="headerlink" title="3"></a>3</h3><p><a href="https://images0.cnblogs.com/blog/663864/201411/081954502994300.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081954592215008.png" alt="image"></a></p>
<p>现在考虑 M（即样本量）， 比 N（即特征的数量）小或等于N。</p>
<p>通过之前的博文，我们知道如果你只有较少的样本，导致特征数量大于样本数量，那么矩阵 XTX 将是不可逆矩阵或奇异（singluar）矩阵，或者用另一种说法是这个矩阵是退化（degenerate）的，那么我们就没有办法使用正规方程来求出 θ 。</p>
<p>幸运的是，正规化也为我们解决了这个问题，具体的说只要正则参数是严格大于零，实际上，可以证明如下矩阵：</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081955003621092.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081955014244135.png" alt="image"></a></p>
<p>将是可逆的。因此，使用正则还可以照顾任何 XTX 不可逆的问题。</p>
<p>所以，你现在知道如何实现岭回归，利用它，你就可以避免过度拟合，即使你在一个相对较小的训练集里有很多特征。这应该可以让你在很多问题上更好的运用线性回归。</p>
<p>在接下来的视频中，我们将把这种正则化的想法应用到 Logistic 回归，这样我们就可以让 logistic 回归也避免过度拟合，从而表现的更好。</p>
<h2 id="4-Regularized-Logistic-Regression"><a href="#4-Regularized-Logistic-Regression" class="headerlink" title="4. Regularized Logistic Regression"></a>4. Regularized Logistic Regression</h2><p>Regularized Logistic Regression 实际上与 Regularized Linear Regression 是十分相似的。</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081955184567964.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081955311908741.png" alt="image"></a></p>
<p>同样使用梯度下降：</p>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081955407211863.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081955489245956.png" alt="image"></a></p>
<p>如果在高级优化算法中，使用正则化技术的话，那么对于这类算法我们需要自己定义costFunction。</p>
<blockquote>
<p>For those methods what we needed to do was to define the function that’s called the cost function.</p>
</blockquote>
<p><a href="https://images0.cnblogs.com/blog/663864/201411/081956439096258.png" target="_blank" rel="noopener"><img src="https://images0.cnblogs.com/blog/663864/201411/081957037373100.png" alt="image"></a></p>
<p>这个我们自定义的 costFunction 的输入为向量 θ ，返回值有两项，分别是代价函数 jVal 以及 梯度gradient。</p>
<p>总之我们需要的就是这个自定义函数costFunction，针对Octave而言，我们可以将这个函数作为参数传入到 fminunc 系统函数中（fminunc 用来求函数的最小值，将@costFunction作为参数代进去，注意 @costFunction 类似于C语言中的函数指针），fminunc返回的是函数 costFunction 在无约束条件下的最小值，即我们提供的代价函数 jVal 的最小值，当然也会返回向量 θ 的解。</p>
<p>上述方法显然对正则化逻辑回归是适用的。</p>
<h2 id="5-尾声"><a href="#5-尾声" class="headerlink" title="5. 尾声"></a>5. 尾声</h2><p>通过最近的几篇文章，我们不难发现，无论是线性回归问题还是逻辑回归问题都可以通过构造多项式来解决。但是，你将逐渐发现其实还有更为强大的非线性分类器可以用来解决多项式回归问题。下篇文章中，我们将会讨论。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CVs/2019-12-11-CNN%20Review%20&amp;%20Prep/">CVs/2019-12-11-CNN Review &amp; Prep</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/AI/">AI</a></span><div class="content"><h3 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h3><h3 id="YOLO-v1"><a href="#YOLO-v1" class="headerlink" title="YOLO v1"></a>YOLO v1</h3><h3 id="YOLO-v2"><a href="#YOLO-v2" class="headerlink" title="YOLO v2"></a>YOLO v2</h3><pre><code>* Explanation
* Has RPN?</code></pre><h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><h3 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h3><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><ul>
<li><h3 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h3></li>
<li><p>Explanation:</p>
</li>
</ul>
<h3 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet v2"></a>MobileNet v2</h3><h3 id="ShuffleNet"><a href="#ShuffleNet" class="headerlink" title="ShuffleNet"></a>ShuffleNet</h3><h3 id="1x1"><a href="#1x1" class="headerlink" title="1x1"></a>1x1</h3><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><ul>
<li>To solve what problem?</li>
<li>Explanation:</li>
<li>Keywords:<ul>
<li>Downsampling</li>
</ul>
</li>
</ul>
<h3 id="Mask-RCNN"><a href="#Mask-RCNN" class="headerlink" title="Mask RCNN"></a>Mask RCNN</h3><h3 id="RF-Receptive-Field"><a href="#RF-Receptive-Field" class="headerlink" title="RF (Receptive Field)"></a>RF (Receptive Field)</h3><ul>
<li>Top-Down (Easy!)</li>
<li>Bottom Up (Hard..)</li>
</ul>
<h3 id="Gradient-Vanishing"><a href="#Gradient-Vanishing" class="headerlink" title="Gradient Vanishing"></a>Gradient Vanishing</h3><h3 id="Gradient-Explosion"><a href="#Gradient-Explosion" class="headerlink" title="Gradient Explosion"></a>Gradient Explosion</h3><p>ROI Pooling vs ROI Align</p>
<hr>
<h2 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h2><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><ul>
<li><p>Explanation</p>
<p>The delta btw y_hat and y will be fed into another weak classifier </p>
</li>
</ul>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p><strong>SVM本身是一个二值分类器</strong></p>
<p>　　SVM算法最初是为二值分类问题设计的，当处理多类问题时，就需要构造合适的多类分类器。</p>
<p>　　目前，构造SVM多类分类器的方法主要有两类</p>
<p>　　（1）直接法，直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类。这种方法看似简单，但其计算复杂度比较高，实现起来比较困难，只适合用于小型问题中；</p>
<p>　　（2）间接法，主要是通过组合多个二分类器来实现多分类器的构造，常见的方法有one-against-one和one-against-all两种。</p>
<h3 id="RF-Random-Forest"><a href="#RF-Random-Forest" class="headerlink" title="RF (Random Forest)"></a>RF (Random Forest)</h3><h3 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h3><ul>
<li>Entropy definition</li>
</ul>
<h3 id="Training-Imbalance"><a href="#Training-Imbalance" class="headerlink" title="Training Imbalance"></a>Training Imbalance</h3><ul>
<li><p>Undersampling</p>
</li>
<li><p>Oversampling</p>
</li>
<li><p>SMOTE</p>
</li>
</ul>
<h3 id="Why-Regularization-example"><a href="#Why-Regularization-example" class="headerlink" title="Why Regularization ? example?"></a>Why Regularization ? example?</h3></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CVs/2019-12-04-OpenCV%20Arrangement/">CVs/2019-12-04-OpenCV Arrangement</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/CV/">CV</a></span><div class="content"><h3 id="Install-sh"><a href="#Install-sh" class="headerlink" title="Install.sh"></a>Install.sh</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mkdir opencv</span><br><span class="line"><span class="built_in">cd</span> opencv</span><br><span class="line">cp /root/3.4.7.zip ./</span><br><span class="line">unzip 3.4.7.zip</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install -y cmake</span><br><span class="line">apt-get install -y build-essential libgtk2.0-dev libavcodec-dev libavformat-dev libjpeg.dev libtiff4.dev libswscale-dev libjasper-dev</span><br><span class="line"><span class="built_in">cd</span> opencv-3.4.7</span><br><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> ..</span><br><span class="line">make install -j8</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CV%20notes/2019-08-04-Pose-Generation/">CV notes/2019-08-04-Pose-Generation</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/">Deep-Learning</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/AI/">AI</a></span><div class="content"><p>(NOT MY NOTE, ref from colleaque’s)</p>
<h3 id="Deformable-GANs-for-Pose-based-Human-Image-Generation"><a href="#Deformable-GANs-for-Pose-based-Human-Image-Generation" class="headerlink" title="Deformable GANs for Pose-based Human Image Generation."></a>Deformable GANs for Pose-based Human Image Generation.</h3><p><a href="https://github.com/AliaksandrSiarohin/pose-gan" target="_blank" rel="noopener">https://github.com/AliaksandrSiarohin/pose-gan</a>.  cvpr2018</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6hp3mtcvuj30rp0aq42d.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6hp4oxs5gj30gd0adwh2.jpg" alt=""></p>
<h3 id="Progressive-Pose-Attention-for-Person-Image-Generation"><a href="#Progressive-Pose-Attention-for-Person-Image-Generation" class="headerlink" title="Progressive Pose Attention for Person Image Generation"></a>Progressive Pose Attention for Person Image Generation</h3><p><a href="https://github.com/tengteng95/Pose-Transfer" target="_blank" rel="noopener">https://github.com/tengteng95/Pose-Transfer</a>.  cvpr2019, </p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6gthtcaz5j30o70b2wfd.jpg" alt="整体流程图"></p>
<p>整体结构中含有多个Pose  Attentional Block，其作用是对输入的image pathway和pose pathway按照Pose Mask进行更新，图中Mt即为Pose Mask，它引导网络将图片中人物的不同的部分按照目标姿态进行像素块迁移。</p>
<p>将最后一个Block中Image Pathway的数据经过解码网络，即得到了最终的生成图像。</p>
<h3 id="Unsupervised-Person-Image-Generation-with-Semantic-Parsing-Transformation"><a href="#Unsupervised-Person-Image-Generation-with-Semantic-Parsing-Transformation" class="headerlink" title="Unsupervised Person Image Generation with Semantic Parsing Transformation"></a>Unsupervised Person Image Generation with Semantic Parsing Transformation</h3><p><a href="https://github.com/SijieSong/person_generation_spt" target="_blank" rel="noopener">https://github.com/SijieSong/person_generation_spt</a>. cvpr2019</p>
<p>只有测试代码，测试输入还需要semantic parsing</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6lf6q884ej311s0mb0vb.jpg" alt="">  </p>
<p>###Dense Intrinsic Appearance Flow for Human Pose Transfer</p>
<p><a href="https://github.com/ly015/intrinsic_flow" target="_blank" rel="noopener">https://github.com/ly015/intrinsic_flow</a>      cvpr2019</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4tdrihzj31dk0lsgrt.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4v9xer1j31bg0j40u9.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4vecqfjj30o407udg5.jpg" alt=""></p>
<p>###Disentangled Person Image Generation</p>
<p><a href="https://github.com/charliememory/Disentangled-Person-Image-Generation" target="_blank" rel="noopener">https://github.com/charliememory/Disentangled-Person-Image-Generation</a> cvpr2018</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4zyq9zhj31pa0n076v.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g6m4xvk2woj31l00l8di5.jpg" alt=""></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CV%20notes/2019-11-06-EGNet%20Eval/">CV notes/2019-11-06-EGNet Eval</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Graphic/">Graphic</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Deep-Learning/">Deep-Learning</a></span><div class="content"><h2 id="EGNet-Evaluation"><a href="#EGNet-Evaluation" class="headerlink" title="EGNet Evaluation"></a>EGNet Evaluation</h2><p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ol6oj5msj307h0b4wfe.jpg" alt="img"></p>
<h3 id="1-GPU-运行时间测试"><a href="#1-GPU-运行时间测试" class="headerlink" title="1. GPU 运行时间测试"></a><strong>1. GPU 运行时间测试</strong></h3><table>
<thead>
<tr>
<th>Times</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody><tr>
<td>preprocess time / s</td>
<td>0.0016</td>
<td>0.0016</td>
<td>0.0023</td>
<td>0.0019</td>
<td>0.0016</td>
<td>0.0023</td>
<td>0.0027</td>
<td>0.0028</td>
<td>0.0024</td>
<td>0.0022</td>
</tr>
<tr>
<td>model time / s</td>
<td>0.1007</td>
<td>0.1011</td>
<td>0.0982</td>
<td>0.1076</td>
<td>0.0945</td>
<td>0.0970</td>
<td>0.0997</td>
<td>0.0948</td>
<td>0.1021</td>
<td>0.1015</td>
</tr>
<tr>
<td>postprocess time / s</td>
<td>0.0017</td>
<td>0.0020</td>
<td>0.0021</td>
<td>0.0022</td>
<td>0.0019</td>
<td>0.0019</td>
<td>0.0027</td>
<td>0.0018</td>
<td>0.0021</td>
<td>0.0022</td>
</tr>
</tbody></table>
<p><strong>preprocessing average time: 0.00214</strong></p>
<p><strong>model average time: 0.09972</strong></p>
<p><strong>postprocessing average time: 0.00206</strong></p>
<h3 id="2-CPU-Time"><a href="#2-CPU-Time" class="headerlink" title="**2. CPU Time"></a>**2. CPU Time</h3><table>
<thead>
<tr>
<th>Times</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody><tr>
<td>preprocess time / s</td>
<td>0.0027</td>
<td>0.0029</td>
<td>0.0017</td>
<td>0.0022</td>
<td>0.0016</td>
<td>0.0017</td>
<td>0.0028</td>
<td>0.0024</td>
<td>0.0031</td>
<td>0.0016</td>
</tr>
<tr>
<td>model time / s</td>
<td>8.9851</td>
<td>9.6686</td>
<td>8.6931</td>
<td>9.7251</td>
<td>8.9071</td>
<td>8.9628</td>
<td>8.9946</td>
<td>8.9697</td>
<td>8.9452</td>
<td>9.0649</td>
</tr>
<tr>
<td>postprocess time / s</td>
<td>0.0017</td>
<td>0.0017</td>
<td>0.0023</td>
<td>0.0017</td>
<td>0.0017</td>
<td>0.0018</td>
<td>0.0017</td>
<td>0.0017</td>
<td>0.0018</td>
<td>0.0016</td>
</tr>
</tbody></table>
<p><strong>preprocessing average time: 0.00227</strong></p>
<p><strong>model average time: 9.09162</strong></p>
<p><strong>postprocessing average time: 0.00177</strong></p>
<ol start="3">
<li>Results</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ol8on2b6j307h0b4ab1.jpg" alt="img"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g8ol8t9igaj307h0b4wed.jpg" alt="img"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/CV%20notes/2019-09-06-Face-Keypoint-Detection/">CV notes/2019-09-06-Face-Keypoint-Detection</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/">Deep-Learning</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Deep-Learning/AI/">AI</a></span><div class="content"><p>Ref from: <a href="https://niuyuanyuanna.github.io/2018/11/08/computer_version/face-keypoint-detection/" target="_blank" rel="noopener">https://niuyuanyuanna.github.io/2018/11/08/computer_version/face-keypoint-detection/</a></p>
<h1 id="人脸关键点检测"><a href="#人脸关键点检测" class="headerlink" title="人脸关键点检测"></a>人脸关键点检测</h1><p>关于人脸识别和表情分类的一些<a href="https://zhuanlan.zhihu.com/p/31638581" target="_blank" rel="noopener">论文</a></p>
<h2 id="人脸关键点检测介绍"><a href="#人脸关键点检测介绍" class="headerlink" title="人脸关键点检测介绍"></a>人脸关键点检测介绍</h2><p>人脸关键点检测也称为人脸关键点检测、定位或者人脸对齐，是指给定人脸图像，定位出人脸面部的关键区域位置，包括眉毛、眼睛、鼻子、嘴巴、脸部轮廓等。</p>
<p>关键点的集合称作形状(shape)，形状包含了关键点的位置信息，而这个位置信息一般可以用两种形式表示，第一种是关键点的位置相对于整张图像，第二种是关键点的位置相对于人脸框(标识出人脸在整个图像中的位置)。把第一种形状称作绝对形状，它的取值一般介于 [0∼h][0∼h]或[0∼w][0∼w]，第二种形状我们称作相对形状，它的取值一般介于 0 到 1。这两种形状可以通过人脸框转换。</p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>人脸关键点检测分为三种：</p>
<ul>
<li>基于ASM(Active Shape Model)和AAM (Active Appearnce Model) 的传统方法、参数化方法</li>
<li>基于CSR(Cascaded Shape Regression)的方法、非参数化方法</li>
<li>基于深度学习、非参数化方法</li>
</ul>
<p>基于参数化形状模型的方法可依据其外观模型的不同，可进一步分为，基于局部的方法和基于全局的方法；对于非参数化进一步可分为基于样例的方法、基于图模型的方法、基于级联回归的方法和基于深度学习的方法。</p>
<h3 id="人脸关键点评价标准"><a href="#人脸关键点评价标准" class="headerlink" title="人脸关键点评价标准"></a>人脸关键点评价标准</h3><p>目前主要的衡量标准是算法所获取的关键点位置与真实关键点位置之间的偏差。在评价偏差时，由于不同人脸图像的实际大小难免会有所差异，为便于在同样的尺度下比较算法性能，需要采用一定的数据归一化策略。 目前主流的方法是基于两眼间的距离进行人脸大小的标准化：<br>$$<br>ex=∣∣x^−xGT∣∣DIODex=DIOD∣∣x^−xGT∣∣<br>$$<br>其中分子DIODDIOD表示估计值与真实值的欧式距离，分母∣∣x^−xGT∣∣∣∣x^−xGT∣∣表示双眼距离，即两眼中心的欧式距离。也有采用边界框对角线作为归一化因子来评价偏差。</p>
<h3 id="人脸常用数据库"><a href="#人脸常用数据库" class="headerlink" title="人脸常用数据库"></a>人脸常用数据库</h3><p>数据库可以分为两类：主动式捕获的数据和被动式捕获的数据。主动式捕获的数据是在实验室里，对光照变化、遮挡、头部姿态和面部表情可控的情况下，对固定人员进行照片采集。被动式捕获的数据则是在社交网站等一些环境不可控的条件下采集而得。</p>
<ul>
<li>主动式数据</li>
</ul>
<ol>
<li>CMU Multi-PIE：在2004年10月至2005年3月的四次会议中收集的，支持在姿态、光照和表情变化条件下识别人脸的算法的开发。 该数据库包含337个主题和超过750,000个305GB数据的图像。 共记录了六种不同的表情：中性，微笑，惊奇，斜视，厌恶和尖叫。 在15个视图和19个不同照明条件下记录受试者，这个数据库的一个子集被标记为68点或39点。</li>
<li>XM2VTS：收集了295人的2360个彩色图像，声音文件和3D人脸模型，这2360个彩色图像标有68个关键点。</li>
<li>AR：包含超过4000个彩色图像，对应126人（70名男性和56名女性）的脸部。图像是在可控的条件下，以不同的面部表情，光照条件和遮挡（太阳镜和围巾）拍摄的。Ding and Martinez手动为每张脸部图像标注了130个关键点。</li>
<li>IMM：包含240张40个人的彩色图像（7名女性和33名男性）。 每张图像都对眉毛、眼睛、鼻子、嘴巴和下巴进行标注，共计58个标记点。</li>
<li>MUCT：由276个人的3755张图像组成，每张图像有76个关键点。 这个数据库中的面孔在不同的光照、不同的年龄和不同的种族的条件下拍摄。</li>
<li>PUT：采集了部分光照条件可控的100个人，且沿着俯仰角和偏航角旋转的9971张高分辨率图像（2048×1536），每张图像都标有30个关键点。</li>
</ol>
<ul>
<li>被动式数据</li>
</ul>
<ol>
<li>BioID：记录在室内实验室环境中，但使用“真实世界”的条件。 该数据库包含23个主题的1521个灰度人脸图像，每张图像标记20个关键点。</li>
<li>LFW：包含从网上收集的5724个人的13,233幅面部图像，其中1680人在数据集中有两张或更多的照片。虽然，这个数据库没有提供标记点，但可以从其余网站上获取。</li>
<li>AFLW(Annotated Facial Landmarks in the Wild) ：是一个大规模、多视角和真实环境下的人脸数据库。图像是从图片分享网站Flickr上收集，该数据库共包含25,993张图像，每张图像标有21个关键点。</li>
<li>LFPW(Labeled Face Parts in the Wild)：由1400个面部图像（1100作为训练集，其他300个图像作为测试集）组成。所有数据均从google, Flickr和Yahoo上获取，每张图像标记35个关键点，但在文献中，通常采用29个关键点。</li>
<li>AFW(Annotated Faces in the Wild)：包含205个图像，特点是：背景高度混乱，人脸比例和姿势都有很大的变化，每张图像均有6个关键点和边界框。</li>
<li>300-W(300 Faces in-the-Wild Challenge)：一个混合数据库，由多个已发布数据库（LFPW，Helen，AFW和XM2VTS）的面部图像和一个新收集的数据库IBUG组成。 所有这些图像都重新标注了68个关键点。</li>
</ol>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><h4 id="ASM检测方法"><a href="#ASM检测方法" class="headerlink" title="ASM检测方法"></a>ASM检测方法</h4><p>ASM(Active Shape Model)是由Cootes于1995年提出的经典的人脸关键点检测算法，主动形状模型即通过形状模型对目标物体进行抽象，ASM是一种基于点分布模型（Point Distribution Model, PDM）的算法。在PDM中，外形相似的物体，例如人脸、人手、心脏、肺部等的几何形状可以通过若干关键点（landmarks）的坐标依次串联形成一个形状向量来表示。ASM算法需要通过人工标定的方法先标定训练集，经过训练获得形状模型，再通过关键点的匹配实现特定物体的匹配。</p>
<p>其检测过程主要分为两步：</p>
<ul>
<li>训练<ul>
<li>图像预处理：<ul>
<li>搜集n个训练样本（n=400）， 手动标记脸部关键点；</li>
<li>将训练集中关键点的坐标串成特征向量</li>
<li>对形状进行归一化和对齐（对齐采用Procrustes方法）</li>
<li>对齐后的形状特征做PCA处理</li>
</ul>
</li>
<li>为关键点构建局部特征（在每次迭代搜索过程中每个关键点可以寻找新的位置）：<ul>
<li>局部特征一般用梯度特征，以防光照变化</li>
<li>有的方法沿着边缘的法线方向提取，有的方法在关键点附近的矩形区域提取</li>
</ul>
</li>
</ul>
</li>
<li>搜索<ul>
<li>计算眼睛（或者眼睛和嘴巴）的位置，做简单的尺度和旋转变化，对齐人脸</li>
<li>在对齐后的各个点附近搜索，匹配每个局部关键点（常采用马氏距离），得到初步形状</li>
<li>用平均人脸（形状模型）修正匹配结果</li>
<li>迭代直到收敛</li>
</ul>
</li>
</ul>
<p>优点：模型简单直接，架构清晰明确，易于理解和应用，而且对轮廓形状有着较强的约束</p>
<p>缺点：其近似于穷举搜索的关键点定位方式在一定程度上限制了其运算效率</p>
<h4 id="AAM"><a href="#AAM" class="headerlink" title="AAM"></a>AAM</h4><p>AAM（Active Appearance Models）。1998年，Cootes对ASM进行改进，不仅采用形状约束，而且又加入整个脸部区域的纹理特征，提出了AAM算法[2]。AAM于ASM一样，主要分为两个阶段，模型建立阶段和模型匹配阶段。其中模型建立阶段包括对训练样本分别建立形状模型(Shape Model)和纹理模型(Texture Model)，然后将两个模型进行结合，形成AAM模型。</p>
<h4 id="CPR"><a href="#CPR" class="headerlink" title="CPR"></a>CPR</h4><p>2010年，Dollar提出CPR（Cascaded Pose Regression, 级联姿势回归），CPR通过一系列回归器将一个指定的初始预测值逐步细化，每一个回归器都依靠前一个回归器的输出来执行简单的图像操作，整个系统可自动的从训练样本中学习。<br>人脸关键点检测的目的是估计向量</p>
<p>S=(x1,x2,⋅⋅⋅,xk,⋅⋅⋅,xK)∈R2KS=(x1,x2,⋅⋅⋅,xk,⋅⋅⋅,xK)∈R2K</p>
<p>其中K表示关键点的个数，由于每个关键点有横纵两个坐标，所以S的长度为2K。CPR检测一共有T个阶段，在每个阶段中首先进行特征提取，得到</p>
<p>ft=ϕ(I,St)ft=ϕ(I,St)</p>
<p>这里特征使用的是shape-indexed features，也可以使用诸如HOG、SIFT等人工设计的特征，或者其他可学习特征（learning based features），然后通过训练得到的回归器rtrt来估计增量ΔSΔS( update vector)</p>
<p>ΔS=rt(ϕ(I,St))ΔS=rt(ϕ(I,St))</p>
<p>把ΔSΔS加到前一个阶段的SS上得到新的SS，这样通过不断的迭代即可以得到最终的S(shape)。</p>
<p>St+1=St+ΔSSt+1=St+ΔS</p>
<h4 id="DCNN"><a href="#DCNN" class="headerlink" title="DCNN"></a>DCNN</h4><p>2013 年，Sun 等人 首次将 CNN 应用到人脸关键点检测，提出一种级联的 CNN（拥有三个层级）——DCNN(Deep Convolutional Network)，此种方法属于级联回归方法。作者通过精心设计拥有三个层级的级联卷积神经网络，不仅改善初始不当导致陷入局部最优的问题，而且借助于 CNN 强大的特征提取能力，获得更为精准的关键点检测。</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/06230452.png" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/06230452.png" alt="img"></a></p>
<p>如上图所示，DCNN由三个level构成。level-1是3个CNN网络；level-2是10个CNN块构成，每个关键点采用两个CNN；level-3结构类似于level-2，都为10个CNN。</p>
<p>Level-1的三个CNN分别为 F1（Face 1）、EN1（Eye，Nose）、NM1（Nose，Mouth）。F1 输入尺寸为 39<em>39，输出 5 个关键点的坐标；EN1 输入尺寸为 39</em>31，输出是 3 个关键点的坐标；NM11 输入尺寸为 39*31，输出是 3 个关键点；Level-1 的输出为三个 CNN 输出取平均。</p>
<p>Level-2，由 10 个 CNN 构成，输入尺寸均为 15*15，每两个组成一对，一对 CNN 对一个关键点进行预测，预测结果同样是采取平均。</p>
<p>Level-3 与 Level-2 一样，由 10 个 CNN 构成，输入尺寸均为 15*15，每两个组成一对。Level-2 和 Level-3 是对 Level-1 得到的粗定位进行微调，得到精细的关键点定位。</p>
<p>Level-1 之所以比 Level-2 和 Level-3 的输入要大，是因为作者认为，由于人脸检测器的原因，边界框的相对位置可能会在大范围内变化，再加上面部姿态的变化，最终导致输入图像的多样性，因此在 Level-1 应该需要有足够大的输入尺寸。Level-1 与 Level-2 和 Level-3 还有一点不同之处在于，Level-1 采用的是局部权值共享（Lcally Sharing Weights），作者认为传统的全局权值共享是考虑到，某一特征可能在图像中任何位置出现，所以采用全局权值共享。然而，对于类似人脸这样具有固定空间结构的图像而言，全局权值共享就不奏效了。因为眼睛就是在上面，鼻子就是在中间，嘴巴就是在下面的。所以作者采用局部权值共享，通过实验证明了局部权值共享给网络带来性能提升。</p>
<p>DCNN 采用级联回归的思想，从粗到精的逐步得到精确的关键点位置，不仅设计了三级级联的卷积神经网络，还引入局部权值共享机制，从而提升网络的定位性能。最终在数据集 BioID 和 LFPW 上均获得当时最优结果。速度方面，采用 3.3GHz 的 CPU，每 0.12 秒检测一张图片的 5 个关键点。</p>
<h4 id="Face-DCNN"><a href="#Face-DCNN" class="headerlink" title="Face++ DCNN"></a>Face++ DCNN</h4><p>2013 年，Face++在 DCNN 模型上进行改进，提出从粗到精的人脸关键点检测算法，实现了 68 个人脸关键点的高精度定位。该算法将人脸关键点分为内部关键点和轮廓关键点，内部关键点包含眉毛、眼睛、鼻子、嘴巴共计 51 个关键点，轮廓关键点包含 17 个关键点。</p>
<p>针对内部关键点和外部关键点，该算法并行的采用两个级联的 CNN 进行关键点检测，网络结构如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/92897228.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/92897228.jpg" alt="img"></a></p>
<p>针对内部 51 个关键点，采用四个层级的级联网络进行检测。其中，Level-1 主要作用是获得面部器官的边界框；Level-2 的输出是 51 个关键点预测位置，这里起到一个粗定位作用，目的是为了给 Level-3 进行初始化；Level-3 会依据不同器官进行从粗到精的定位；Level-4 的输入是将 Level-3 的输出进行一定的旋转，最终将 51 个关键点的位置进行输出。针对外部 17 个关键点，仅采用两个层级的级联网络进行检测。Level-1 与内部关键点检测的作用一样，主要是获得轮廓的 bounding box；Level-2 直接预测 17 个关键点，没有从粗到精定位的过程，因为轮廓关键点的区域较大，若加上 Level-3 和 Level-4，会比较耗时间。最终面部 68 个关键点由两个级联 CNN 的输出进行叠加得到。</p>
<p>算法创新点：</p>
<ol>
<li>把人脸的关键点定位问题，划分为内部关键点和轮廓关键点分开预测，有效的避免了 loss 不均衡问题</li>
<li>在内部关键点检测部分，并采用 DCNN 的方法，将每个关键点采用两个 CNN 进行预测，而是每个器官采用一个 CNN 进行预测，从而减少计算量</li>
<li>相比于 DCNN，没有直接采用人脸检测器返回的结果作为输入，而是增加一个边界框检测层（Level-1），可以大大提高关键点粗定位网络的精度。</li>
</ol>
<h4 id="TCDNN"><a href="#TCDNN" class="headerlink" title="TCDNN"></a>TCDNN</h4><p>2014 年，Zhang 等人将 MTL（Multi-Task Learning）应用到人脸关键点检测中，提出 TCDCN（Tasks-Constrained Deep Convolutional Network）。作者认为，在进行人脸关键点检测任务时，结合一些辅助信息可以帮助更好的定位关键点，这些信息如，性别、是否带眼镜、是否微笑和脸部的姿势等等。作者将人脸关键点检测（5 个关键点）与性别、是否带眼镜、是否微笑及脸部的姿势这四个子任务结合起来构成一个多任务学习模型，模型框架如图所示。</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/15813947.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/15813947.jpg" alt="img"></a></p>
<p>网络输入为40×40的灰度图，通过CNN后得到2×2×64的特征图，再通过一层含有100个神经元的全连接层输出最终提取到的共享特征。该特征为所有任务共享，对于关键点检测问题，就采用线性回归模型；对于分类问题，就采用逻辑回归。</p>
<p>在传统 MLT 中，各任务重要程度是一致的，其目标方程如下：</p>
<p>arg⁡min⁡∑t−1T∑i=1Nℓ(yit,f(xit;wt))+Φ(wt){wt}argmint−1∑Ti=1∑Nℓ(yit,f(xit;wt))+Φ(wt){wt}t=1T</p>
<p>其中，</p>
<ul>
<li>f(xit;wt)f(xit;wt)表示输入矩阵xitxit与权值矩阵$ \mathbf{w}^t$运算后得到的输出</li>
<li>ℓ(.)ℓ(.)表示损失函数</li>
<li>Φ(wt)Φ(wt)表示正则化</li>
</ul>
<p>对于各任务 t 而言，其重要性是相同的，但是在多任务学习中，往往不同任务的学习难易程度不同，若采用相同的损失权重，会导致学习任务难以收敛。文章针对多任务学习中，不同学习难度问题进行了优化，提出带权值的目标函数：</p>
<p>wr,{wa}  ∑i=1Nℓ(yit,f(xit;wt))+∑i=1N∑a∈Aλaℓ(yia,f(xia;wa))argminwr,{wa}a∈A  i=1∑Nℓ(yit,f(xit;wt))+i=1∑Na∈A∑λaℓ(yia,f(xia;wa))</p>
<p>式中前一项为人脸关键点检测的损失函数，第二项表示其他任务的损失函数，λaλa为任务aa的重要程度。在论文中，四个子任务分别为：性别、是否带眼镜、微笑、脸部姿势，因此，优化目标函数为：</p>
<p>wr,{wa}  12∑i=1N∥(yit,f(xit;wt)∥2+∑i=1N∑a∈Aλayialog⁡(p(yia∣xia;wa))+∑t=1T∥wa∥22argminwr,{wa}a∈A  21i=1∑N∥(yit,f(xit;wt)∥2+i=1∑Na∈A∑λayialog(p(yia∣xia;wa))+t=1∑T∥wa∥22</p>
<p>分类任务采用交叉熵损失函数。</p>
<p>针对多任务学习的另外一个问题——各任务收敛速度不同，本文提出一种新的提前停止（Early Stopping）方法。当某个子任务达到最好表现以后，这个子任务就对主任务已经没有帮助，就可以停止这个任务。</p>
<p>TCDCN 采用多任务学习方法对人脸关键点进行检测，针对多任务学习在人脸关键点检测任务中的两个主要问题：不同任务学习难易程度不同以及不同任务收敛速度不同，分别提出了新目标函数和提前停止策略加以改进，最终在 AFLW 和 AFW 数据集上获得领先的结果。同时对比于级联 CNN 方法，在 Intel Core i5 cpu 上，级联 CNN 需要 0.12s，而 TCDCN 仅需要 17ms，速度提升七倍有余。</p>
<h4 id="MTCNN"><a href="#MTCNN" class="headerlink" title="MTCNN"></a>MTCNN</h4><p>2016 年，Zhang 等人提出一种多任务级联卷积神经网络（MTCNN, Multi-task Cascaded Convolutional Networks）用以同时处理人脸检测和人脸关键点定位问题。作者认为人脸检测和人脸关键点检测两个任务之间往往存在着潜在的联系，然而大多数方法都未将两个任务有效的结合起来，本文为了充分利用两任务之间潜在的联系，提出一种多任务级联的人脸检测框架，将人脸检测和人脸关键点检测同时进行。</p>
<p>MTCNN 包含三个级联的多任务卷积神经网络，分别是 Proposal Network (P-Net)、Refine Network (R-Net)、Output Network (O-Net)，每个多任务卷积神经网络均有三个学习任务，分别是人脸分类、边框回归和关键点定位。网络结构如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/14009657.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/14009657.jpg" alt="img"></a></p>
<p>MTCNN 实现人脸检测和关键点定位分为三个阶段。首先由 P-Net 获得了人脸区域的候选窗口和边界框的回归向量，并用该边界框做回归，对候选窗口进行校准，然后通过非极大值抑制（NMS）来合并高度重叠的候选框。然后将 P-Net 得出的候选框作为输入，输入到 R-Net，R-Net 同样通过边界框回归和 NMS 来去掉那些 false-positive 区域，得到更为准确的候选框；最后，利用 O-Net 输出 5 个关键点的位置。</p>
<p>在具体训练过程中，作者就多任务学习的损失函数计算方式进行相应改进。在多任务学习中，当不同类型的训练图像输入到网络时，有些任务是不进行学习的，因此相应的损失应为 0。例如，当训练图像为背景（Non-face）时，边界框和关键点的 loss 应为 0，文中提供计算公式自动确定 loss 的选取，公式为：</p>
<p>min⁡∑i=1N∑j∈{det,box,landmark}αjβijLijmini=1∑Nj∈{det,box,landmark}∑αjβijLij</p>
<p>其中</p>
<ul>
<li>αjαj表示第jj个任务的重要程度，在P-Net中，αdet=1αdet=1，αbox=0.5αbox=0.5，αlandmark=0.5αlandmark=0.5；在R-Net中αdet=1αdet=1，αbox=0.5αbox=0.5，αlandmark=1αlandmark=1。在R-Net中将$\alpha_{landmark} $增大，因为需要对关键点进行检测，所以相应增大任务重要性</li>
<li>βij∈(0,1)βij∈(0,1)作为样本类型指示器</li>
</ul>
<p>为了提升网络性能，需要挑选出困难样本（Hard Sample），传统方法是通过研究训练好的模型进行挑选，而本文提出一种能在训练过程在线挑选困难样本的方法。在 mini-batch 中，对每个样本的损失进行排序，挑选前 70% 较大的损失对应的样本作为困难样本，同时在反向传播时，忽略那 30% 的样本，因为那 30% 样本对更新作用不大。</p>
<h4 id="TCNN"><a href="#TCNN" class="headerlink" title="TCNN"></a>TCNN</h4><p>TCNN（Tweaked Convolutional Neural Networks）2016 年，Wu 等人研究了 CNN 在人脸关键点定位任务中到底学习到的是什么样的特征，在采用 GMM（Gaussian Mixture Model, 混合高斯模型）对不同层的特征进行聚类分析，发现网络进行的是层次的，由粗到精的特征定位，越深层提取到的特征越能反应出人脸关键点的位置。针对这一发现，提出了 TCNN（Tweaked Convolutional Neural Networks），其网络结构如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/89918024.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/89918024.jpg" alt="img"></a></p>
<p>左边为Vanilla CNN，针对 FC5FC5得到的特征进行 K 个类别聚类，将训练图像按照所分类别进行划分，用以训练所对应的 FC6KFC6K。测试时，图片首先经过 Vanilla CNN 提取特征，即 FC5FC5的输出。将 FC5FC5输出的特征与 K 个聚类中心进行比较，将 FC5FC5输出的特征划分至相应的类别中，然后选择与之相应的 FC6FC6进行连接，最终得到输出。</p>
<p>作者对 Vanilla CNN 中间各层特征进行聚类分析，并统计出关键点在各层之间的变化程度。越深层提取到的特征越紧密，因此越深层提取到的特征越能反应出人脸关键点的位置。作者在采用 K=64 时，对所划分簇的样本进行平均后绘图如下：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/62921355.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/62921355.jpg" alt="img"></a></p>
<p>从图上可发现，每一个簇的样本反应了头部的某种姿态，甚至出现了表情和性别的差异。因此可推知，人脸关键点的位置常常和人脸的属性相关联。因此为了得到更准确的关键点定位，作者使用具有相似特征的图片训练对应的回归器，最终在人脸关键点检测数据集 AFLW,AFW 和 300W 上均获得当时最佳效果。</p>
<h2 id="DAN"><a href="#DAN" class="headerlink" title="DAN"></a>DAN</h2><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><p>DAN是一种人脸对齐的方法，采用级联神经网络结构，充分利用人脸的全局信息，而不是局部信息，避免局部最小化。</p>
<ol>
<li>参考CSR框架，通过前向传播提取特征，训练前向传播的backbone网络得到关键点位的偏差，替代CSR中的回归器</li>
<li>用级联结构来实现CSR中的迭代</li>
<li>利用人脸所有信息T(I)T(I)、H(I)H(I)、F(I)F(I)作为输入，得到关键点偏差</li>
<li>构造级联网络结构</li>
<li>分级训练网络，每一级网络loss不收敛后再训练下一级网络</li>
</ol>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>2017 年，Kowalski 等人提出一种新的级联深度神经网络——DAN（Deep Alignment Network），以往级联神经网络输入的是图像的某一部分，与以往不同，DAN 各阶段网络的输入均为整张图片。当网络均采用整张图片作为输入时，DAN 可以有效的克服头部姿态以及初始化带来的问题，从而得到更好的检测效果。之所以 DAN 能将整张图片作为输入，是因为其加入了关键点热图（Landmark Heatmaps），关键点热图的使用是本文的主要创新点。DAN 基本框架如图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/55049944.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/55049944.jpg" alt="img"></a></p>
<p>DAN 包含多个阶段，每一个阶段含三个输入和一个输出，输入分别是被矫正过的图片、关键点热图和由全连接层生成的特征图，输出是面部形状（Face Shape）。其中，CONNECTION LAYER 的作用是将本阶段得输出进行一系列变换，生成下一阶段所需要的三个输入，具体操作如下图所示：</p>
<p><a href="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/83789038.jpg" target="_blank" rel="noopener"><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/83789038.jpg" alt="img"></a></p>
<p>第一阶段的输入仅有原始图片和 S0S0。面部关键点的初始化即为 S0S0，是由所有关键点取平均得到，第一阶段输出 S1S1。对于第二阶段， S1S1经第一阶段的 CONNECTION LAYERS 进行转换，分别得到转换后图片 T2(I)T2(I)、 S1S1所对应的热图 H2H2和第一阶段 fc1fc1层输出，这三个正是第二阶段的输入。如此周而复始，直到最后一个阶段输出 SNSN。</p>
<p>DAN 要做的“IMAGE TRANSFORM“，就是图片矫正， DAN 对姿态变换具有很好的适应能力，或许就得益于这个“IMAGE TRANSFORM“。StSt公式为：</p>
<p>St=Tt−1(Tt(St−1)+ΔSt)St=Tt−1(Tt(St−1)+ΔSt)</p>
<p>Feed forward NN网络参数为：</p>
<table>
<thead>
<tr>
<th align="center">Name</th>
<th align="center">Shape-in</th>
<th align="center">Shape-out</th>
<th align="center">Kernel</th>
</tr>
</thead>
<tbody><tr>
<td align="center">conv1a</td>
<td align="center">112×112×1</td>
<td align="center">112×112×64</td>
<td align="center">3×3×1,1</td>
</tr>
<tr>
<td align="center">conv1b</td>
<td align="center">112×112×64</td>
<td align="center">112×112×64</td>
<td align="center">3×3×64,1</td>
</tr>
<tr>
<td align="center">pool1</td>
<td align="center">112×112×64</td>
<td align="center">56×56×64</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">conv2a</td>
<td align="center">56×56×64</td>
<td align="center">56×56×128</td>
<td align="center">3×3×64,1</td>
</tr>
<tr>
<td align="center">conv2b</td>
<td align="center">56×56×128</td>
<td align="center">56×56×128</td>
<td align="center">3×3×128,1</td>
</tr>
<tr>
<td align="center">pool2</td>
<td align="center">56×56×128</td>
<td align="center">28×28×128</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">conv3a</td>
<td align="center">28×28×128</td>
<td align="center">28×28×256</td>
<td align="center">3×3×128,1</td>
</tr>
<tr>
<td align="center">conv3b</td>
<td align="center">28×28×256</td>
<td align="center">28×28×256</td>
<td align="center">3×3×256,1</td>
</tr>
<tr>
<td align="center">pool3</td>
<td align="center">28×28×256</td>
<td align="center">14×14×256</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">conv4a</td>
<td align="center">14×14×256</td>
<td align="center">14×14×512</td>
<td align="center">3×3×256,1</td>
</tr>
<tr>
<td align="center">conv4b</td>
<td align="center">14×14×512</td>
<td align="center">14×14×512</td>
<td align="center">3×3×512,1</td>
</tr>
<tr>
<td align="center">pool4</td>
<td align="center">14×14×512</td>
<td align="center">7×7×512</td>
<td align="center">2×2×1,2</td>
</tr>
<tr>
<td align="center">fc1</td>
<td align="center">7×7×512</td>
<td align="center">1×1×256</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">fc2</td>
<td align="center">1×1×256</td>
<td align="center">1×1×136</td>
<td align="center">-</td>
</tr>
</tbody></table>
<p>Feed Forward NN的输入是经过“IMAGE TRANSFORM“之后得到的偏移量ΔStΔSt，它是在新特征空间下的偏移量，在经过偏移后再经过反变换Tt−1(⋅)Tt−1(⋅)，将其还原到原始空间。</p>
<p>关键点热度图的计算就是一个中心衰减，关键点处值最大，越远则值越小，公式如下：</p>
<p>H(x,y)=11+Si∈Tt(St−1)  ∥(x,y)−Si∥H(x,y)=1+minSi∈Tt(St−1)  ∥(x,y)−Si∥1</p>
<p>从fc1fc1层生成特种图的目的是人为给 CNN 增加上一阶段信息。总而言之，DAN 是一个级联思想的关键点检测方法，通过引入关键点热图作为补充，DAN 可以从整张图片进行提取特征，从而获得更为精确的定位。</p>
<h2 id="EmotionalDAN"><a href="#EmotionalDAN" class="headerlink" title="EmotionalDAN"></a>EmotionalDAN</h2><p>Author: NYY</p>
<p>Link: <a href="http://yoursite.com/2018/11/08/computer_version/face-keypoint-detection/">http://yoursite.com/2018/11/08/computer_version/face-keypoint-detection/</a></p>
<p>Copyright Notice: All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a>unless stating additionally.</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/C++/2017-12-19-.a,%20.so,%20and%20DLL/">C++/2017-12-19-.a, .so, and DLL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/C/">C++</a></span><div class="content"><p>Migration from my OneNote in my local.</p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g99dbljar7j31ks0qk11d.jpg" alt="image-20191124194400743"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g99d99683mj316u0u07wh.jpg" alt="image-20191124194420542"></p>
<p><img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g99d9ia4rpj31kg092aig.jpg" alt="image-20191124194435445"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/12/15/C++/2018-02-14-Dynamic%20vs%20Static%20Binding/">C++/2018-02-14-Dynamic vs Static Binding</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-12-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/C/">C++</a></span><div class="content"><p>Ref: <a href="http://notes.maxwi.com/3516/08/19/static-dynamic-binding/" target="_blank" rel="noopener">http://notes.maxwi.com/3516/08/19/static-dynamic-binding/</a></p>
<ul>
<li>静态类型：就是对象声明时采用的类型，一旦确定就无法更改，编译期已经确定</li>
<li>动态类型：通常是指一个指针或引用在调用时所指向的类型，可以理解为赋值号右侧对象的类型（当然采用直接赋值就是括号中的对象的类型），可以在运行时更改，在运行期决定</li>
<li>静态绑定（static binding）：又名前期绑定（early binding），绑定的是对象的静态类型，发生在编译期，即程序编译完成后就已经确定</li>
<li>动态绑定（dynamic binding）：又名后期绑定（late binding），绑定的是对象的动态类型，发生在运行期，即在运行期由当前的动态类型决定所需要调用的函数或属性</li>
</ul>
<p>可以简单理解为，通常我们定义的非虚函数都是静态绑定，即静态类型与动态类型都指同一种类型，在编译期就已经确定，那么运行时调用的函数也就是相应类型对象的函数。<br>而虚函数可以发生动态绑定，即在运行时，根据指针或引用所指对象的类型来决定调用相应的虚函数。如当定义父类的指针，指向派生类的对象时，该指针的动态类型与静态类型就不一致,<br>动态类型即派生类，静态类型为父类，那么在使用此指针在运行时调用虚函数时会首先调用动态类型对象（即派生类对象）的虚函数，如果动态类型中没有重写（override）该虚函数，<br>则调用的还是基类中的虚函数。<br>举例说明：<br>所有的讲解都是注释中，请认真阅读每一句注释</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">void</span>)</span>  <span class="comment">//普通的非虚函数，只能静态绑定</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"A::func"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">vfunc1</span><span class="params">()</span>   <span class="comment">//虚函数，可以在运行时，根据动态类型不同而发生动态绑定</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"A::vfunc1"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">vfunc2</span><span class="params">(<span class="keyword">int</span> i = <span class="number">2</span>)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"A::vfunc2: "</span> &lt;&lt; i &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> :</span> <span class="keyword">public</span> A &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">func</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"B::func"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">vfunc1</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"B::vfunc1"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">vfunc2</span><span class="params">(<span class="keyword">int</span> i = <span class="number">5</span>)</span>  <span class="comment">//注意B改变了继承而来的虚函数的默认参数</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"B::vfunc2: "</span> &lt;&lt; i &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">C</span> :</span> <span class="keyword">public</span> B &#123;  <span class="comment">//注意C继承自B而不是A</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    B* pb = <span class="keyword">new</span> B();  <span class="comment">//pb的静态类型即是它声明的类型B，动态类型是它指向的类型B，即动态类型与静态类型一致</span></span><br><span class="line">    C* pc = <span class="keyword">new</span> C(); <span class="comment">//pc的静态类型与动态类型一致</span></span><br><span class="line">    A* pa = pc;   <span class="comment">//pa的静态类型是A，动态类型是C</span></span><br><span class="line">    pa = pb; <span class="comment">//将pa的动态类型改为B</span></span><br><span class="line">    B *pbnull = <span class="literal">nullptr</span>;  <span class="comment">//定义B指向空指针，它将在编译期发生静态绑定</span></span><br><span class="line">    pa-&gt;func();  <span class="comment">//A::func，虽然pa的静态类型与动态类型不一样，但func是不是虚函数，只有在通过指针或引用调用虚函数时，才会有运行时发生动态绑定，即pa优先调用其动态类型中的虚函数，所以此时pa只能调用它自己的func</span></span><br><span class="line">    pa-&gt;vfunc1(); <span class="comment">//B::vfunc1，此时pa发生了动态绑定，即调用它动态类型B的虚函数vfunc1</span></span><br><span class="line">    pb-&gt;func(); <span class="comment">//B::func，pb的func将隐藏继承自A中的函数func，所有非虚函数的调用都由静态类型决定，即编译期就已经确定了</span></span><br><span class="line">    pb-&gt;vfunc2(); <span class="comment">//B::vfunc2:5，pb正常通过静态绑定调用自己的函数vfunc2</span></span><br><span class="line">    pa-&gt;vfunc2(); <span class="comment">//B::vfunc2:2，发现pa此时动态绑定调用的是类型B的成员函数，但形参去是A自己的形参，因为C++为了执行效率，缺省参数值都是静态绑定的，所以永远不要重新定义一个继承而来的virtual函数的缺省参数值</span></span><br><span class="line">    pc-&gt;func(); <span class="comment">//B::func，pc正常的静态绑定，调用继承自B的函数func</span></span><br><span class="line">    pa = pc;  <span class="comment">//将pa的动态类型改为pc</span></span><br><span class="line">    pa-&gt;vfunc1();  <span class="comment">//B::vfunc1，此时pa实际上发生了动态绑定，被动态绑定到pc，将调用其动态类型C中的虚函数，但由于C中没有重写继承自B的虚函数，所以只能调用其父类B中的虚函数，注意不是A中的</span></span><br><span class="line">    pbnull-&gt;func();  <span class="comment">//B::func，虽然pbnull此时指向的是空指针，但其静态绑定是在编译期确定的，也就是说编译完成之后，pbnull就已经存在pbnull-&gt;func()的调用了。</span></span><br><span class="line">    <span class="comment">// pbnull-&gt;vfunc1(); //编译没有问题，但将发生运行时错误，linux下运行报错为Segmentation fault。因为在运行时发生动态绑定时，pbnull是指向空指针的，显然空指针不属于任何类型，pbnull也就无法正常绑定。之所以编译器无法检查到这类问题应该就是动态导致用户可以随时将pbnull绑定到一个其派生类的对象上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放动态内存</span></span><br><span class="line">    <span class="keyword">delete</span> pb;</span><br><span class="line">    <span class="keyword">delete</span> pc;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的注释中已经讲解的非常清楚了，最后两句指向空指针的类对象也可以调用其静态绑定的函数需要特别注意，这是一种极不推荐的做法</p>
<p>注意</p>
<ul>
<li>永远不要重新定义继承而来的非虚函数，这样的函数由对象声明时的静太类型确定，没有多态性，将给程序留下不可预知的隐患</li>
<li>永远不要重新定义一个继承而来的虚函数的缺省参数值，因为缺省参数值都是静态绑定的。</li>
<li>接口基类不应该有非虚函数，即全部使用纯虚函数</li>
</ul>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/16/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><span class="page-number current">17</span><a class="page-number" href="/page/18/">18</a><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/18/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>