<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-180692466-1', 'auto');
ga('send', 'pageview');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">386</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">66</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Awaken Desparado</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2021/06/17/NodeJS/0_nvm_VS_npm/">NodeJS/0_nvm_VS_npm</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-06-17</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/NodeJS/">NodeJS</a></span><div class="content"><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> nvm use v16.13.2</span></span><br><span class="line">Now using node v16.13.2 (npm v8.1.2)</span><br><span class="line">(base)</span><br><span class="line"><span class="meta">#</span><span class="bash"> joe @ J-M1-Pro-16 <span class="keyword">in</span> ~ [16:23:32]</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> nvm ls</span></span><br><span class="line">       v12.18.0</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash">     v16.13.2</span></span><br><span class="line">         system</span><br><span class="line">default -&gt; v12.18.0</span><br><span class="line">node -&gt; stable (-&gt; v16.13.2) (default)</span><br><span class="line">stable -&gt; 16.13 (-&gt; v16.13.2) (default)</span><br><span class="line">iojs -&gt; N/A (default)</span><br><span class="line">unstable -&gt; N/A (default)</span><br><span class="line">lts/* -&gt; lts/gallium (-&gt; N/A)</span><br><span class="line">lts/argon -&gt; v4.9.1 (-&gt; N/A)</span><br><span class="line">lts/boron -&gt; v6.17.1 (-&gt; N/A)</span><br><span class="line">lts/carbon -&gt; v8.17.0 (-&gt; N/A)</span><br><span class="line">lts/dubnium -&gt; v10.24.1 (-&gt; N/A)</span><br><span class="line">lts/erbium -&gt; v12.22.12 (-&gt; N/A)</span><br><span class="line">lts/fermium -&gt; v14.19.3 (-&gt; N/A)</span><br><span class="line">lts/gallium -&gt; v16.15.1 (-&gt; N/A)</span><br><span class="line">(base)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> joe @ J-M1-Pro-16 <span class="keyword">in</span> ~ [16:28:44]</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">which</span> npm</span></span><br><span class="line">/Users/joe/.nvm/versions/node/v16.13.2/bin/npm</span><br><span class="line">(base)</span><br><span class="line"><span class="meta">#</span><span class="bash"> joe @ J-M1-Pro-16 <span class="keyword">in</span> ~ [16:28:48]</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">which</span> node</span></span><br><span class="line">/Users/joe/.nvm/versions/node/v16.13.2/bin/node</span><br><span class="line">(base)</span><br><span class="line"><span class="meta">#</span><span class="bash"> joe @ J-M1-Pro-16 <span class="keyword">in</span> ~ [16:29:07]</span></span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> joe @ J-M1-Pro-16 <span class="keyword">in</span> ~/side_projects/sallyer-projects/serverless on git:develop x [16:50:45] C:130</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mv .env .env.local</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> joe @ J-M1-Pro-16 <span class="keyword">in</span> ~/side_projects/sallyer-projects/serverless on git:develop x [16:50:54]</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> npm run dev</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> sallyer-next@0.0.0 dev</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> next dev -p 3005</span></span><br><span class="line"></span><br><span class="line">ready - started server on 0.0.0.0:3005, url: http://localhost:3005</span><br><span class="line">info  - Loaded env from /Users/joe/side_projects/sallyer-projects/serverless/.env.local</span><br><span class="line">event - compiled client and server successfully in 1018 ms (1148 modules)</span><br><span class="line">wait  - compiling / (client and server)...</span><br></pre></td></tr></table></figure></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/01/08/BigData_java/2021-1-8%20Watermark%20in%20Structure-streaming%20&amp;%20Kafka%20as%20output%20sink/">BigData_java/2021-1-8 Watermark in Structure-streaming &amp; Kafka as output sink</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-01-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/BigData-java/">BigData_java</a></span><div class="content"><h4 id="缺條腿方案"><a href="#缺條腿方案" class="headerlink" title="缺條腿方案"></a>缺條腿方案</h4><p>只有動態的接收者，沒有動態的處理者</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1gom2jms4yvj21440aqwih.jpg" alt="image-20210316213105971" style="zoom:67%;" />



<h4 id="完整的流！"><a href="#完整的流！" class="headerlink" title="完整的流！"></a>完整的流！</h4><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1gom2j1rh7lj21540aqgpc.jpg" alt="image-20210316213032834" style="zoom:50%;" />

<p>透過structure-streaming可以實時處理，還可以被下個kfk topic收集，可以透過kfk console看接受到了哪些新的data，或是下游連的另個收集方讀取著。</p>
<ul>
<li>input 就是一串data的組合</li>
<li>Schema相對於kfk consumer會多一點<ul>
<li>key</li>
<li>Value</li>
<li>topic 會標明這條msg是從哪個topic來的</li>
<li>partition：當接收到data時，會有個recorded metadata, 之前有個callback去讀，知道當前的msg在哪個topic的哪個partition的哪個offset。<ul>
<li>但這邊不用再去解析，這邊直接給</li>
</ul>
</li>
<li>offset　↑</li>
<li>timestamp: 到我kfk的時間；而eventtime還是得去msg的編碼裡面找</li>
<li>timestampType: Integer；Epoch time, 是大多數情況，10位就是in secs; 13位就是mili-seconds</li>
</ul>
</li>
<li>key是：<ul>
<li>Default不設為None時就是round-robin</li>
<li>就是往kfk發數據時，有兩個field, 一個就是key，可以按指定hashvalue 方式發去指定的kfk cluster上；key就是個record的key；只要是一樣的key，就是去到相同機器</li>
</ul>
</li>
<li>value是: 就是真的內容<ul>
<li>實際pull到的那些msg</li>
<li>如整個 Json的tweet，就是整個msg！</li>
<li>需要通過Spark datatype 進行解碼；序列化　（kfk的自帶API會自己帶，但現在得自己手動）<ul>
<li><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gomw6v6anhj30ae00r0sz.jpg" alt="image-20210317143646216" style="zoom: 67%;" /></li>
<li>轉化類型當中，做了反序列化操作</li>
<li>要是直接讀進來print的row，會有topic, partition, 但key, value都是二進制encoding</li>
</ul>
</li>
<li>解碼也不夠，因為是個json obj，個人的id, retweet這類的<ul>
<li>我還需要extract json，去轉成了json obj再操作</li>
</ul>
</li>
<li>每次對應的obj可能是多行row… 不如直接用spark方法直接parse, 可進行結構化定義<ul>
<li><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gomwdvlo67j30kw062wh3.jpg" alt="image-20210317144332554" style="zoom:50%;" /></li>
<li>可以對增量操作而不是每條額外的操作</li>
<li>我的input已經是個純結構化的了，json就是結構了，我不需要再把一整行再轉化出來，我只需要去extract當中需要的三個column，剩下的就可以在讀取並轉化的過程中扔了！讓整個obj也變得非常輕便</li>
<li>要是json是好幾層，也完全是可以的；structure type就得要多define幾層</li>
</ul>
</li>
</ul>
</li>
<li>相同點 (kfk consumer VS structure-streaming consumer)<ul>
<li>之前的consumer app會有auto offset reset &lt;&gt; startingOffset (會去拉最早的data而不是把最早的給忽略了)</li>
<li>Topic &lt;&gt; subscribe; 我想要subscribe的　是哪個topic</li>
<li>Bootstrap.servers &lt;&gt; kafka.bootstrap.server</li>
<li>consumer.poll &lt;&gt; maxOffsetPerTrigger 就是每次的micro-batch我要拉幾條，通過這樣的設定，就可以明顯設這次的拉出幾條</li>
</ul>
</li>
<li>不同點<ul>
<li>ss不支持group_id；我要是自己控制group_id還要知道哪幾台機器在pull什麼data，還要控制consumer app的個數 &lt;= partition 個數，不然就浪費；所以ss就自己內部管理；SS會根據job數去管理、平衡。<ul>
<li>官網doc上會說我也是可以硬 overwrite group_id <ul>
<li>１但大家都很不推薦自己define group_id</li>
<li>２因為沒有如kfk那套所對應的key的serializer &amp; deserializer, SS用的是自己的一套</li>
<li>３SS會忽略kfk的auto commit，這類的async當時用kfk consumer時那套得自己處理；它有自己的commit log，SS會在executor, driver紀錄自己處理到了哪個kfk partition的第幾個offset。所以SS就直接忽略kfk的auto commit。省掉了auto commit和我們要實現那auto commit的難處</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Structure-streaming’s-output-sink"><a href="#Structure-streaming’s-output-sink" class="headerlink" title="Structure-streaming’s output sink"></a>Structure-streaming’s output sink</h2><p>TODO: 要把kfk API consumer改成structure-streaming consumer</p>
<ul>
<li>之前在 kfk裡的，而ss包了的：<ul>
<li>processID:<ul>
<li>Itempotency</li>
<li>only commit once, if jbo failure, 要確認要做的不在 processID裡</li>
</ul>
</li>
<li>ctrl + C時<ul>
<li>該處理一堆在consumer cache裡的，elegantly 善後</li>
</ul>
</li>
<li>管理 commit<ul>
<li>ss 處理完才commit</li>
<li>由於 spark 本質就是只處理commit 完的 msg, 所以仍是依賴spark自己的 offset, 決定當有 Ctrl + C時怎麼commit msg，而不是當被強迫退出了，還是去commit已pull下來的msg</li>
</ul>
</li>
</ul>
</li>
<li>poll也被ss的getStreamingData取代了</li>
<li>curID也是for idempotency的</li>
</ul>
<ul>
<li><p>producer打印出來，到json online parser看下，才知道data什麼樣，我在ss裡define才不會define錯</p>
</li>
<li><p>Nested field:</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38ab3ff1pj20fd03z3yk.jpg" alt="image-20210318163250985" style="zoom: 67%;" />
</li>
<li><p>input 有個增量，是個已被CAST過的value, input有個col裡面是 json obj：</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38ab5sch7j20mn04w751.jpg" alt="image-20210318163832389" style="zoom:67%;" />
</li>
<li><p>as data　要是沒有的話↓</p>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goo5dat8ugj30gl02ct9i.jpg" alt="image-20210318163953648"></p>
</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38ab8myhwj20pi01kjrp.jpg" alt="image-20210318175941292" style="zoom:67%;" />



<ul>
<li>run producer</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goo7zussqmj30q202uwgd.jpg" alt="image-20210318181047094"></p>
<ul>
<li>Flatten into data frame, 所以就不需要 StringIntoJsonObj這個function<ul>
<li>就得到了 transformedTweet, 而且會發現也不需要 ExtractObjIntoDataframe了！</li>
<li>刪除沒有ID的Tweet, 並把當前的Tweet簡單地clean up!</li>
</ul>
</li>
<li>withColumn: lit<ul>
<li>填一col, 如當前的cur time stamp<ul>
<li><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38abbxe0kj20wi07w0tg.jpg" alt="image-20210328235740991" style="zoom:50%;" /></li>
</ul>
</li>
<li>repalce<ul>
<li><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gp02fx9333j315q0380t7.jpg" alt="image-20210329000527131" style="zoom:50%;" />





</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Nested Schema    <img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38abctu5ej20na0863yu.jpg" alt="image-20210329001059779" style="zoom:67%;" /></li>
</ul>
<p>Hank</p>
<ol>
<li>mySQL –&gt; aqoformat(?!) –&gt; HDFS</li>
<li>mySQL –&gt; Enterprise Warehouse (Hive)</li>
<li>Platform engineer; Infra Engineer</li>
</ol>
<ul>
<li>DS: query 已經生成的Data, which is generated by platform eng. or Infra eng.，</li>
<li>生成後存在HDFS/DB裡；Stat 建模</li>
</ul>
<p>說是Best Practice(?!)</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/01/03/BigData_java/2021-1-3%20Structured%20Streaming%20Continue/">BigData_java/2021-1-3 Structured Streaming Continue</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-01-03</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/BigData-java/">BigData_java</a></span><div class="content"><ul>
<li>Default: Micro-batch != Real-Time</li>
</ul>
<p>如何trigger-steaming job &lt;== depends, 我是可以define trigger模式的。</p>
<p>每次的batch特別小，這個就是micro-batch, 我想實時看到結果</p>
<ul>
<li>Fixed interval: still micro-batch, with user-defined interval, 如就是hdfs，我就配合30分鐘就好啦！<ul>
<li>是上個開始和下個開始的時間之間的差別<ul>
<li>要是上個特長呢？做完上個超時完後，再接著下個job</li>
<li>我設interval一定是知道freq 很低了，大概率，我做research的，就是知道10分鐘不會進來太多，就是我這個micro-batch能搞完的</li>
</ul>
</li>
<li>用的人可能會出錯…</li>
</ul>
</li>
<li>Once mode: 做一次而已，就是Batch Spark job, 做完就自己關掉了！</li>
</ul>
<ul>
<li>Continuous mode:  別用…Latency: 100ms，雖然不是真的實時，但已經滿足大多數的scenario；Flink可以真的實時。。。近乎0<ul>
<li>兩年前開始實驗此一持續性模式，但support 功能還很陽春，因為底層還是batch。不然就是全翻掉重做了。。</li>
</ul>
</li>
</ul>
<ul>
<li>WINDOW! <ul>
<li>要維護的不僅僅是當下的window,  還有之前的也要記得維護著；更像是一個Window History, 會去紀錄下來我上個interval、這個interval的WINDOWs各出現了什麼</li>
<li>變深的就是當前的data frame裡我更改的變化</li>
<li>在res table不斷地update 的；res 就是要統計、計算過的所有數據所生成的結果是什麼，而這結果都是存去disk上</li>
<li><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38aa4tyksj21em0p6td1.jpg" alt="image-20210315164555094" style="zoom:67%;" />



</li>
</ul>
</li>
</ul>
<ul>
<li><p>很久如兩天前的要維護嗎？</p>
<ul>
<li>要是用戶選了complete mode，他還有個下游，他的下游把最後return的complete數據再extract出來最新的 time window，我當然還是維護</li>
<li>但要是 append/update mode，就不需要維護了…</li>
</ul>
</li>
<li><p>不理想的狀態下，什麼情況我還要去維護已經處理過的window 呢？</p>
<ul>
<li>如需要排序這類的時候，這時候就要用到舊的data；<ul>
<li>同理我還是會有個window，只要去處理window裡的排序，對window外的就不去CARE, 但是↓<ul>
<li>很常見的一個現象跟問題就是希望Streaming app可以去解決的，這個問題就是</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>理想狀態定義：</p>
<ul>
<li>沒有 Late Data, 就是 8:10 event time的會如時過來<ul>
<li>但是！Spark 有 Fault Tolerance特性，要是有個executor掛了，要有另個executor快去頂上，把還沒搞完的data快速搞完</li>
<li>要是user要groupby time-window, 對structure-streaming而言，他就是要groupby, 他只關心這一個小時裡的topk的增量</li>
</ul>
</li>
<li>手機上被trigger了10/1，到kafka是10/30! Why? 有些地方網路差，過了半小時一小時後才出現；手機上操作很多今天蒐了地址，手機就發了很多個log去kfk，又不想打車了，就划掉了app，手機把payload cache著，半年後又想打車時，所有東西就重新發送。。就late了半年。。。<ul>
<li>但我又想保證正確性，意味，過去的state完全保存在memory當中，result table一直增長，但是distributed，就一直存在disk很ok., 但state很危險！＜＝＝ input table有個state, 紀當前增量到了哪個位置，記一個增量的卡點；window也有state也全是在memory裡．</li>
<li>要是所有的window都存在memory裡，就很容易OOM</li>
</ul>
</li>
<li><strong>Watermark 解決 OOM – late data</strong><ul>
<li>並保證最終結果的穩定</li>
<li>memory裡只保存 watermark所指定的時間範圍內的data</li>
</ul>
</li>
<li>就是只維護要的window，而不是維護所有的windows在memory裡</li>
</ul>
</li>
<li><p>最後寫去的，還是那三個地方</p>
<ul>
<li>kfk &lt;== update complete模式都很好，就是無限append的操作，就是無腦intsert, 無腦update</li>
<li>hdfs &lt;== 不建議；讀是沒問題的，但寫的話。。。就是複雜，它的data就是不能update的，只能replace就是重寫了，不然就是append當前data<ul>
<li>如我要write到當前的folder時，我不會update當前folder的文件所對應的data，而只會去無腦append；這樣我這樣用hdfs跟用kfk就沒有區別，因為我也沒有按col作更新，我就只是寫入一組data，然後我又來了新的data，就生出新的文件說什麼又更新了一下，hdfs它的small file會很多，name space就會out of quota, (HDFS 就是一個namespace 下就是20w個文件)；這時要是batch又沒有設置interval，這樣namespace很快就會用完。</li>
<li>就是要先想清楚用的場景、跟怎麼用</li>
<li>所以最好是「流對流」、「batch 對 batch」</li>
<li>Kfk retention時間 default是一週，很多公司會縮到２、３天。大多數都是 log</li>
<li>然後一般再按照data partition dump去HDFS上，HDFS上還可以設一個retention<ul>
<li>好處一：一次地dump，文件的大小、個數是由injection job決定的，就是很長時間內都不會有 small file問題, 如就每天200個文件</li>
<li>好處二: HDFS可以設retention；分析用的data，也不可能去存十年陳年老data。一般就是２年。所以就是可以 date partition + retention</li>
</ul>
</li>
</ul>
</li>
<li>console</li>
</ul>
</li>
</ul>
<ul>
<li>Input 不再是words, 而是我在模擬機器發送一個完整的log, 就是要有event time，是event time，不是land到kfk的時間；kfk會給land到kfk的時間。所以手機上發去的時間戳是payload過去的。</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38aa87dnvj20sy02kq2y.jpg" alt="image-20210316154559107" style="zoom:67%;" />





<h3 id="Kafka-Consumer"><a href="#Kafka-Consumer" class="headerlink" title="Kafka Consumer"></a>Kafka Consumer</h3><p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1golui5db3fj21480am0z1.jpg" alt="image-20210316165252703"></p>
<p>那時用了很多kfk的API做了kfk的consumer</p>
<h3 id="Structure-streaming-Consumer"><a href="#Structure-streaming-Consumer" class="headerlink" title="Structure-streaming Consumer"></a>Structure-streaming Consumer</h3><p>取代了kfk consumer，這傢伙有自己的consumer，只是跟kfk自帶的api不同，</p>
<p>不再用之前說的kfk自帶的api, 如kfk consumer的pull , offset, commit這類的</p>
<p>structured-streaming有自己的流程</p>
<h2 id="Twitter-Consumer-的問題們"><a href="#Twitter-Consumer-的問題們" class="headerlink" title="Twitter Consumer 的問題們"></a>Twitter Consumer 的問題們</h2><ol>
<li>Itempotency<ul>
<li>kfk 以前可以保證exactly once，前提是得用kfk streaming，其他的第三方的application，都只能保證 <strong><em>at most</em> or <em>at least</em>　once</strong> , 就是無法保證 exactly once；<ul>
<li>所以，導致，要是我模型不能保證itempotency，我就設at most once，因為我不怕丟data丟失</li>
<li>我要是怕丟失，它重複讀了進來或retry 很多次，就… 一直加到爆</li>
</ul>
</li>
</ul>
</li>
<li>之前不想rely on 自己的commit offset，我自己做得過程出問題了，雖然它覺得已經commit成功了，我不想要自己的auto commit –&gt; 實時處理 process_id，手動的同步操作，只能blocking，等做完，才能commit 這個offset。</li>
</ol>
<p>我展示的還是batch process，只有我把它integrate到一個streaming app，才能保證我真的<strong>實時</strong></p>
<p>實時處理完的很可能有下游要用，所以餵給kfk，讓下游去聆聽我output sink丟去的kfk</p>
<p>實時蒐集完，然後展示</p>
<p>在selective時作bias而不是造假ＸＤ．．．　就是個流式分析！</p>
<p>分布式計算：都已經是分布式機器，每個機器有自己的executor，每個executor有自己的core, 每個core會執行一個task，只是它不是個multi-thread, 而是一個multi-process的操作；自己已經是個多線程操作，我沒必要再自己去開一堆。</p>
<h1 id="Structured-Streaming-Cont"><a href="#Structured-Streaming-Cont" class="headerlink" title="Structured Streaming Cont"></a>Structured Streaming Cont</h1><ul>
<li>streaming</li>
<li>fault tolerance</li>
</ul>
<p>Window: 就是個GroupBy操作；2mins的window, 30secs step</p>
<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gmaw01fw7mj30l607yn16.jpg" alt="image-20210103224247930" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38aacy3a7j213c0fawi9.jpg" alt="image-20210103224231269" style="zoom:50%;" />





<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gmaw6jljiij31jo0tgk9z.jpg" alt="image-20210103224902437" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38aaf8h03j21as0pu0v6.jpg" alt="image-20210103224919270" style="zoom:50%;" />



<h6 id="Watermark-API很好用-TODO"><a href="#Watermark-API很好用-TODO" class="headerlink" title="Watermark API很好用, TODO!"></a>Watermark API很好用, TODO!</h6><h3 id="Tweeter-Consumer"><a href="#Tweeter-Consumer" class="headerlink" title="Tweeter Consumer"></a>Tweeter Consumer</h3><ul>
<li>Itempotency!</li>
<li>紀錄 process_id, 不要 auto-commit；得要是個blocking thread；不可一邊commit，這要手動搞</li>
</ul>
<p>自己維護靜態的內容，然後自己groupby</p>
<ul>
<li>Create_time 就是event_time, 它和我去拉下來的trigger的時間可能差很大的</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38aaiwwoxj20p00jodgt.jpg" alt="image-20210103231100449" style="zoom:50%;" />

<p>MCN公司建網紅，公司付錢用Ins, or Twitter, FB APIs把data service出去，第三方的software抓來做dashboard, 以達到data driven，知道話題，如健身的時間、quanrantine、workout … ==&gt; build homegym</p>
<p>網紅的流量，訂閱看我的流量，知道我的view；我知道我相對去年的view、like、如何</p>
<p>個人的影響就可以分析</p>
<p>我不需要生數據，也不用創新只要用現有數據去建model、精準metrics、推廣app!</p>
<p>Consumer: 從普通的java app變成了 structure-streaming app, 是比較強的framework</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/01/02/BigData_java/2021-1-2%20LAB4:%20/">BigData_java/2021-1-2 LAB4: </a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-01-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/BigData-java/">BigData_java</a></span><div class="content"><h1 id="LAB-4"><a href="#LAB-4" class="headerlink" title="LAB 4"></a>LAB 4</h1><p>EMR就是，就是說所有的hadoop相關的服務，Amazon都是透過EMR cluster來提供的，而Spark也是hadoop的一個組成元素，所以Spark-streaming job也是由EMR cluster所提供的; </p>
<p>Streaming-job會實時從kafka讀取data，然後實時處理data，例子還是一樣，sent analyais對tweet作情感分析，只不過現在是實時處理，而不是靜態處理文件，</p>
<p>最後是兩種展現方式: 1)Console printout, 2)data實時input放去Kafka，好處是第一、我也可以把KFK的內容一直打印在屏幕上，用戶也是可以實時看到 kfk的topics有什麼data；但如果直接打在console上，最終downstream還是用不了當前的data，需要有kfk去蒐集，讓downstream最終可以直接 consume analysis results 的 topic。這是個非常 re-usable的選項。</p>
<p>我只需重複使用EMR的clusters，去跟kfk溝通的一定是master機器，所以需要要讓它跟kfk可以通訊。所以所有從EMR master發出跟requests，都可以去向kfk。</p>
<pre><code>1. INBOUND: allow EMR
2. OUTBOUND: allow EMR</code></pre><p>OUTBOUND 就得選去向EMR master的。</p>
<p>EMR：也需要對我的電腦開放: – INBOUND: add MyIP</p>
<h2 id="Privileges-inbound-outbound-of-clusters"><a href="#Privileges-inbound-outbound-of-clusters" class="headerlink" title="Privileges (inbound/outbound) of clusters"></a>Privileges (inbound/outbound) of clusters</h2><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9nmazassj31ma0s2k3i.jpg" alt="image-20210102210705550" style="zoom:50%;" />



<p>好煩呀，為什 EC2要上去就是</p>
<p>ssh -i xxx.pem <a href="mailto:ec2-user@ec2xxxx.com">ec2-user@ec2xxxx.com</a>pute.amazonaws.com 就連上去了</p>
<p>要上去EMR的cluster還得自己手動去設個inboound然後點個 My IP？</p>
<p>連上EMR的方式是？</p>
<h2 id="Coniguration-on-EMR-Cluster"><a href="#Coniguration-on-EMR-Cluster" class="headerlink" title="Coniguration on EMR Cluster"></a>Coniguration on EMR Cluster</h2><p>在EMR機器上配置，然後主要run streaming job; 其次相當是個kfk的client機器，要去發data 跟pull data</p>
<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9nxfqmm1j31420nk4pd.jpg" alt="image-20210102211755069" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9nz0rqekj319e04m44j.jpg" alt="image-20210102211927901" style="zoom:50%;" />





<h4 id="Upload-structure-streaming-from-local"><a href="#Upload-structure-streaming-from-local" class="headerlink" title="Upload structure-streaming, from local"></a>Upload structure-streaming, from local</h4><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9o0sibggj319u088gvk.jpg" alt="image-20210102212109957" style="zoom:50%;" />







<h2 id="Run-Streaming-job-它可乾等-並打印在-Console"><a href="#Run-Streaming-job-它可乾等-並打印在-Console" class="headerlink" title="Run Streaming job (它可乾等) 並打印在 Console"></a>Run Streaming job (它可乾等) 並打印在 Console</h2><p>現在的kfk是剛建的，裡面沒有任何東西，我現在想先把我的structured-streaming job 跑起來，讓它一直查去pull Kafka、waiting for the data feeding into kafka, 以前要是靜態data, 要是沒有data生成，就會說沒生成，但現在是一直在等data 來trigger 我的computation。</p>
<h4 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h4><ol>
<li>要有jar包，裡面有要main class給run；當前的job有很多的dependency，去看pom文件！本地</li>
</ol>
<h4 id="第一條指令-準備要運行streaming-job"><a href="#第一條指令-準備要運行streaming-job" class="headerlink" title="第一條指令 準備要運行streaming job"></a>第一條指令 準備要運行streaming job</h4><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9o3832axj319m0hcx3a.jpg" alt="image-20210102212329913" style="zoom:50%;" />

<h6 id="結果"><a href="#結果" class="headerlink" title="結果"></a>結果</h6><h4 id="第二條指令-讓-structure-streaming，讓-streaming-job-保持在running"><a href="#第二條指令-讓-structure-streaming，讓-streaming-job-保持在running" class="headerlink" title="第二條指令　讓 structure-streaming，讓 streaming job 保持在running"></a>第二條指令　讓 structure-streaming，讓 streaming job 保持在running</h4><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a989lddj21a60goajm.jpg" alt="image-20210102212532018" style="zoom:50%;" />

<h6 id="結果-1"><a href="#結果-1" class="headerlink" title="結果"></a>結果</h6><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a9bi2swj219w0i0tfu.jpg" alt="image-20210102212612811" style="zoom:50%;" />

<p>一直在找</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a9dg6jjj219u0h012a.jpg" alt="image-20210102212627019" style="zoom:50%;" />



<h4 id="第三條指令-–-讓producer出場了"><a href="#第三條指令-–-讓producer出場了" class="headerlink" title="第三條指令 – 讓producer出場了"></a>第三條指令 – 讓producer出場了</h4><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a9e5ufsj219w0gawk1.jpg" alt="image-20210102212720955" style="zoom:50%;" />

<ul>
<li>去遠程的twitter api pull tweets</li>
</ul>
<h6 id="結果-2"><a href="#結果-2" class="headerlink" title="結果"></a>結果</h6><p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a9j0d5nj21a20qiqm4.jpg" alt="image-20210102212826013" style="zoom:50%;" /><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9o8to8w6j318w0osqv5.jpg" alt="image-20210102212852435"></p>
<ul>
<li><p>Structrued-streaming job 的角度: 實時處理！</p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38aavvdirj21480pcgye.jpg" alt="image-20210102212914766" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9o9g68qkj319k0q27wh.jpg" alt="image-20210102212928672" style="zoom:50%;" />



</li>
</ul>
<h2 id="改變-streaming-job-發送data去kafka-–-存出"><a href="#改變-streaming-job-發送data去kafka-–-存出" class="headerlink" title="改變 streaming job 發送data去kafka – 存出!"></a>改變 streaming job 發送data去kafka – 存出!</h2><p><strong>讓結果以比較永久的方式存起來，而不只是打印出到 Console</strong></p>
<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9oe88u1kj30ya0ba0vo.jpg" alt="image-20210102213403650" style="zoom:50%;" />

<p>↓    ↓    ↓</p>
<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9oehohsqj311q0d0tf5.jpg" alt="image-20210102213419496" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a9oy4l2j218803cq3e.jpg" alt="image-20210102215646934" style="zoom:50%;" />



<h5 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h5><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9p48c8w1j31b20o2hdt.jpg" alt="image-20210102215903890" style="zoom:50%;" />



<h4 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h4><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9p4pgxsmj31a008cdoc.jpg" alt="image-20210102215931799" style="zoom:50%;" />

<h5 id="監聽中"><a href="#監聽中" class="headerlink" title="監聽中"></a>監聽中</h5><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9p5aw00sj31ae06s7bm.jpg" alt="image-20210102220005633" style="zoom:50%;" />



<h5 id="Structured-Streaming-job寫出到Kafka"><a href="#Structured-Streaming-job寫出到Kafka" class="headerlink" title="Structured-Streaming job寫出到Kafka"></a>Structured-Streaming job寫出到Kafka</h5><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a9xcqx4j219k04sgn7.jpg" alt="image-20210102220057867" style="zoom:50%;" />

<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9p76pjjlj31ae04y0z1.jpg" alt="image-20210102220153132"></p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gm9p81rjggj31a40aiguw.jpg" alt="image-20210102220243440"></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/12/30/BigData_java/Spark/Spark%20APIs%20Note/">BigData_java/Spark/Spark APIs Note</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Spark/">Spark</a></span><div class="content"><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><h4 id="GetRDD"><a href="#GetRDD" class="headerlink" title="GetRDD"></a>GetRDD</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Row&gt; rdd = spark</span><br><span class="line">        .read()</span><br><span class="line">        .format(<span class="string">"csv"</span>)</span><br><span class="line">        .option(<span class="string">"inferSchema"</span>, <span class="keyword">true</span>)</span><br><span class="line">        .option(<span class="string">"header"</span>, <span class="keyword">true</span>)</span><br><span class="line">        .csv(path)</span><br><span class="line">        .toJavaRDD();</span><br></pre></td></tr></table></figure>



<h4 id="Foreach"><a href="#Foreach" class="headerlink" title="Foreach"></a>Foreach</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.foreach(row -&gt; &#123;</span><br><span class="line">  	System.out.println(<span class="string">"elem: "</span> + row.get(<span class="number">1</span>));</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<h3 id="kv-Operation"><a href="#kv-Operation" class="headerlink" title="kv Operation"></a>kv Operation</h3><p>d[key] += 1 &lt;==      dgetOrDefault()</p>
<p>print(d[‘CA’])</p>
<h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Row&gt; calPark = rdd.filter(row -&gt;  row.getString(<span class="number">1</span>).equals(<span class="string">"California"</span>));</span><br><span class="line"></span><br><span class="line"><span class="keyword">long</span> count = calPark.count()</span><br></pre></td></tr></table></figure>



<h4 id="Wide"><a href="#Wide" class="headerlink" title="Wide"></a>Wide</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">JavaPairRDD&lt;String, Integer&gt; parksPerState = rdd.mapToPair(row -&gt; &#123;</span><br><span class="line">  	String state = row.getString(<span class="number">1</span>);</span><br><span class="line">  	<span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(state, <span class="number">1</span>);		<span class="comment">// (Utah, 1), (Oregon, 1), (Utah, 1)..</span></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">/***↑map 　↓reduce***/</span></span><br><span class="line">JavaPairRDD&lt;String, Integer&gt; stateParks = rdd.reduceByKey((x, y) -&gt; x + y);</span><br><span class="line"></span><br><span class="line">        stateParks.collect().forEach(tuple -&gt; &#123;</span><br><span class="line">            System.out.println(String.format(<span class="string">"%s: %s national parks"</span>, tuple._1, tuple._2));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        stateParks.foreach((VoidFunction&lt;Tuple2&lt;String, Integer&gt;&gt;) tuple -&gt;</span><br><span class="line">                System.out.println(String.format(<span class="string">"%s: %s national parks"</span>, tuple._1, tuple._2)));</span><br></pre></td></tr></table></figure>



<h2 id="DF-amp-Dataset"><a href="#DF-amp-Dataset" class="headerlink" title="DF &amp; Dataset"></a>DF &amp; Dataset</h2><h4 id="GetDF-RDD-VS-DF"><a href="#GetDF-RDD-VS-DF" class="headerlink" title="GetDF,  RDD VS DF"></a>GetDF,  RDD VS DF</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">        Dataset&lt;Row&gt; df = spark</span><br><span class="line">                .read()</span><br><span class="line">                .format(<span class="string">"csv"</span>)</span><br><span class="line">                .option(<span class="string">"inferSchema"</span>, <span class="keyword">true</span>)</span><br><span class="line">                .option(<span class="string">"header"</span>, <span class="keyword">true</span>)</span><br><span class="line">                .csv(path)</span><br><span class="line">                .toDF();</span><br><span class="line">df.printSchema();</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">root</span></span><br><span class="line"><span class="comment"> |-- Number: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Name: string (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Type1: string (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Type2: string (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Total: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- HP: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Attack: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Defense: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- SpecialAtk: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- SpecialDef: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Speed: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Generation: integer (nullable = true)</span></span><br><span class="line"><span class="comment"> |-- Legendary: boolean (nullable = true)</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* VS rdd */</span></span><br><span class="line">rdd.foreach(row -&gt; &#123;</span><br><span class="line">  	System.out.println(<span class="string">"name: "</span> + row.get(<span class="number">1</span>));</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// In Contrast, for DF</span></span><br><span class="line">        <span class="comment">//由于dataframe的column信息已经有了，所以可以直接去访问column的值</span></span><br><span class="line">        dataFrame.select(<span class="string">"name"</span>).foreach(name_value -&gt; &#123;</span><br><span class="line">            System.out.println(<span class="string">"pokemon name: "</span> + name_value.toString());</span><br><span class="line">        &#125;);</span><br><span class="line">        dataFrame.foreach(row -&gt; &#123;</span><br><span class="line">            System.out.println(<span class="string">"df name 2: "</span> + row.getString(<span class="number">1</span>));</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wut's more, </span></span><br><span class="line">        <span class="comment">//dataframe可以通过spark sql访问，但rdd不可以</span></span><br><span class="line">        dataFrame.createOrReplaceTempView(<span class="string">"pokemon"</span>);</span><br><span class="line">        Dataset&lt;Row&gt; poisonPokemon = spark.sql(<span class="string">"select * from pokemon where Type1='Poison' limit 10"</span>);</span><br><span class="line">        poisonPokemon.show();</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">+------+---------+------+------+-----+---+------+-------+----------+----------+-----+----------+---------+</span></span><br><span class="line"><span class="comment">|Number|     Name| Type1| Type2|Total| HP|Attack|Defense|SpecialAtk|SpecialDef|Speed|Generation|Legendary|</span></span><br><span class="line"><span class="comment">+------+---------+------+------+-----+---+------+-------+----------+----------+-----+----------+---------+</span></span><br><span class="line"><span class="comment">|    23|    Ekans|Poison|    NA|  288| 35|    60|     44|        40|        54|   55|         1|    false|</span></span><br><span class="line"><span class="comment">|    24|    Arbok|Poison|    NA|  438| 60|    85|     69|        65|        79|   80|         1|    false|</span></span><br><span class="line"><span class="comment">|    29|Nidoran ♀|Poison|    NA|  275| 55|    47|     52|        40|        40|   41|         1|    false|</span></span><br><span class="line"><span class="comment">|    30| Nidorina|Poison|    NA|  365| 70|    62|     67|        55|        55|   56|         1|    false|</span></span><br><span class="line"><span class="comment">|    31|Nidoqueen|Poison|Ground|  505| 90|    92|     87|        75|        85|   76|         1|    false|</span></span><br><span class="line"><span class="comment">|    32|Nidoran ♂|Poison|    NA|  273| 46|    57|     40|        40|        40|   50|         1|    false|</span></span><br><span class="line"><span class="comment">|    33| Nidorino|Poison|    NA|  365| 61|    72|     57|        55|        55|   65|         1|    false|</span></span><br><span class="line"><span class="comment">|    34| Nidoking|Poison|Ground|  505| 81|   102|     77|        85|        75|   85|         1|    false|</span></span><br><span class="line"><span class="comment">|    41|    Zubat|Poison|Flying|  245| 40|    45|     35|        30|        40|   55|         1|    false|</span></span><br><span class="line"><span class="comment">|    42|   Golbat|Poison|Flying|  455| 75|    80|     70|        65|        75|   90|         1|    false|</span></span><br><span class="line"><span class="comment">+------+---------+------+------+-----+---+------+-------+----------+----------+-----+----------+---------+</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; df = spark</span><br><span class="line">        .read()</span><br><span class="line">        .format(<span class="string">"csv"</span>)</span><br><span class="line">        .option(<span class="string">"inferSchema"</span>, <span class="keyword">true</span>)</span><br><span class="line">        .option(<span class="string">"header"</span>, <span class="keyword">true</span>)</span><br><span class="line">        .csv(path)</span><br><span class="line">        .toDF()</span><br><span class="line">        .select(<span class="string">"userId"</span>, <span class="string">"movieId"</span>, <span class="string">"rating"</span>); <span class="comment">// Say, Discard "Timestamp"</span></span><br></pre></td></tr></table></figure>





<h4 id="GetDataset-DF-VS-Dataset"><a href="#GetDataset-DF-VS-Dataset" class="headerlink" title="GetDataset, DF VS Dataset"></a>GetDataset, DF VS Dataset</h4><h6 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h6><h6 id="select"><a href="#select" class="headerlink" title="select"></a>select</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">        Encoder&lt;Pokemon&gt; pokemon_encoder = Encoders.bean(Pokemon<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        Dataset&lt;Pokemon&gt; dataset = spark</span><br><span class="line">                .read()</span><br><span class="line">                .format(<span class="string">"csv"</span>)</span><br><span class="line">                .option(<span class="string">"inferSchema"</span>, <span class="keyword">true</span>)</span><br><span class="line">                .option(<span class="string">"header"</span>, <span class="keyword">true</span>)</span><br><span class="line">                .csv(path)</span><br><span class="line">                .as(pokemon_encoder);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//dataframe 每一行是row，而dataset每一行是一个指定的class</span></span><br><span class="line">        df.foreach(row -&gt; &#123;</span><br><span class="line">            String name_tmp = row.getString(<span class="number">1</span>);</span><br><span class="line">            System.out.println(<span class="string">"name from DF getString: "</span> + name_tmp);</span><br><span class="line">        &#125;);</span><br><span class="line">        df.select(<span class="string">"name"</span>).foreach(name_val -&gt; &#123;</span><br><span class="line">            System.out.println(<span class="string">"name from DF select: "</span> + name_val.toString());</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">/*** ↑ ↑ ↑</span></span><br><span class="line"><span class="comment">        name from DF getString: Zenaora</span></span><br><span class="line"><span class="comment">        name from DF select: [Bulbasaur]</span></span><br><span class="line"><span class="comment">        ***/</span></span><br><span class="line">        </span><br><span class="line">        dataset.foreach(row -&gt; &#123;</span><br><span class="line">            String name = row.getName();</span><br><span class="line"><span class="comment">//            System.out.println("name: " + name);</span></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//dataframe的解析错误只能在runtime时候报错，而dataset解析错误compile时候就会报错</span></span><br><span class="line">        df.foreach(row -&gt; &#123;</span><br><span class="line"><span class="comment">//            System.out.println("getAs: " + row.getAs("type").toString()); // 壓根沒有這個字段，只有type1 &amp; type2</span></span><br><span class="line">        &#125;);</span><br><span class="line">        dataset.foreach((row -&gt; &#123;</span><br><span class="line">            row.getType1(); <span class="comment">// HOW to jump to there?</span></span><br><span class="line"><span class="comment">//            row.getType2();</span></span><br><span class="line">        &#125;));</span><br></pre></td></tr></table></figure>





<h2 id="rdd-df-dataset-共同點"><a href="#rdd-df-dataset-共同點" class="headerlink" title="rdd, df, dataset 共同點"></a>rdd, df, dataset 共同點</h2><ul>
<li>lazy init</li>
<li>都會根據ram自動cache運算</li>
<li>都有partition</li>
</ul>
<h2 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h2><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38abmpmi6j20qh094abb.jpg" alt="image-20210128230633294" style="zoom:50%;" />



<h3 id="DF-APIs"><a href="#DF-APIs" class="headerlink" title="DF APIs"></a>DF APIs</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">//找到water 和 fire pokemon</span></span><br><span class="line">    Dataset&lt;Row&gt; waterPokemon = pokemonType.filter(</span><br><span class="line">            pokemonType.col(<span class="string">"Type1"</span>).equalTo(<span class="string">"Water"</span>)</span><br><span class="line">                    .or(pokemonType.col(<span class="string">"Type2"</span>).equalTo(<span class="string">"Water"</span>))</span><br><span class="line">    );</span><br><span class="line">    pokemonType.createOrReplaceTempView(<span class="string">"pokemon_type"</span>);</span><br><span class="line">    Dataset&lt;Row&gt; firePokemon = spark</span><br><span class="line">            .sql(<span class="string">"select * from pokemon_type where Type1='Fire' or Type2='Fire'"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//找到water and fire Pokemon</span></span><br><span class="line">    Dataset&lt;Row&gt; versertilePokemon = waterPokemon.select(waterPokemon.col(<span class="string">"Name"</span>))</span><br><span class="line">            .intersect(firePokemon.select(firePokemon.col(<span class="string">"Name"</span>)));</span><br><span class="line">    System.out.println(<span class="string">"VERSERTILE: "</span>);</span><br><span class="line">    versertilePokemon.show();</span><br><span class="line"><span class="comment">/***</span></span><br><span class="line"><span class="comment">    VERSERTILE: </span></span><br><span class="line"><span class="comment">    +---------+</span></span><br><span class="line"><span class="comment">    |     Name|</span></span><br><span class="line"><span class="comment">    +---------+</span></span><br><span class="line"><span class="comment">    |Volcanion|</span></span><br><span class="line"><span class="comment">    +---------+</span></span><br><span class="line"><span class="comment">    ***/</span></span><br></pre></td></tr></table></figure>



<h4 id="Join-–-Left"><a href="#Join-–-Left" class="headerlink" title="Join – Left"></a>Join – Left</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">//attack max in water pokemon</span></span><br><span class="line">    Dataset&lt;Row&gt; waterPokemonWithForce = waterPokemon.join(</span><br><span class="line">            pokemonForce,</span><br><span class="line">            waterPokemon.col(<span class="string">"Number"</span>).equalTo(pokemonForce.col(<span class="string">"Number"</span>)),</span><br><span class="line">            <span class="string">"left"</span></span><br><span class="line">    );</span><br><span class="line">    waterPokemonWithForce.sort(waterPokemonWithForce.col(<span class="string">"Attack"</span>).desc());</span><br><span class="line">    Row[] waterPokemonWithMaxForce = (Row[])waterPokemonWithForce.take(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//type1 有哪些类型的pokemon type， -&gt; count</span></span><br><span class="line">    Dataset&lt;Row&gt; pokemonCounts = spark.sql(<span class="string">"select Type1, count(*) as total_count from pokemon_type group by Type1"</span>);</span><br><span class="line">    pokemonCounts.show();</span><br><span class="line">    pokemonCounts.count();</span><br><span class="line"><span class="comment">/***</span></span><br><span class="line"><span class="comment">    +--------+-----------+</span></span><br><span class="line"><span class="comment">    |   Type1|total_count|</span></span><br><span class="line"><span class="comment">    +--------+-----------+</span></span><br><span class="line"><span class="comment">    |   Water|        122|</span></span><br><span class="line"><span class="comment">    |  Poison|         34|</span></span><br><span class="line"><span class="comment">    |   Steel|         29|</span></span><br><span class="line"><span class="comment">    |    Rock|         51|</span></span><br><span class="line"><span class="comment">    |     Ice|         24|</span></span><br><span class="line"><span class="comment">    |   Ghost|         36|</span></span><br><span class="line"><span class="comment">    |   Fairy|         18|</span></span><br><span class="line"><span class="comment">    | Psychic|         67|</span></span><br><span class="line"><span class="comment">    |  Dragon|         35|</span></span><br><span class="line"><span class="comment">    |  Flying|          4|</span></span><br><span class="line"><span class="comment">    |     Bug|         78|</span></span><br><span class="line"><span class="comment">    |Electric|         49|</span></span><br><span class="line"><span class="comment">    |    Fire|         58|</span></span><br><span class="line"><span class="comment">    |  Ground|         34|</span></span><br><span class="line"><span class="comment">    |    Dark|         32|</span></span><br><span class="line"><span class="comment">    |Fighting|         31|</span></span><br><span class="line"><span class="comment">    |   Grass|         82|</span></span><br><span class="line"><span class="comment">    |  Normal|        110|</span></span><br><span class="line"><span class="comment">    +--------+-----------+</span></span><br><span class="line"><span class="comment">    ***/</span></span><br></pre></td></tr></table></figure>



<h4 id="Join-–-self-join-–-gt-filter-–-gt-map-–-gt-groupie-–-gt-agg"><a href="#Join-–-self-join-–-gt-filter-–-gt-map-–-gt-groupie-–-gt-agg" class="headerlink" title="Join – self-join –&gt; filter –&gt; map –&gt; groupie –&gt; agg"></a>Join – self-join –&gt; filter –&gt; map –&gt; groupie –&gt; agg</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">    Dataset&lt;Row&gt; joinedRatings = userRating.as(<span class="string">"a"</span>).join(</span><br><span class="line">            userRating.as(<span class="string">"b"</span>),</span><br><span class="line">            <span class="string">"userId"</span>)</span><br><span class="line">            .selectExpr(<span class="string">"a.userId as userId"</span>, <span class="string">"a.movieId as movie1"</span>,</span><br><span class="line">                    <span class="string">"a.rating as rating1"</span>, <span class="string">"b.movieId as movie2"</span>, <span class="string">"b.rating as rating2"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 在 self-join 後，過濾掉重覆的組合 */</span></span><br><span class="line">    Dataset&lt;Row&gt; uniqueJoinedRatings = joinedRatings.filter(joinedRatings.col(<span class="string">"movie1"</span>)</span><br><span class="line">            .$less(joinedRatings.col(<span class="string">"movie2"</span>)));</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 做成 &lt;movie1:movie2&gt; : &lt;rating1，rating2&gt;的pair */</span></span><br><span class="line">    Dataset&lt;MoviePair&gt; pairs = uniqueRatings</span><br><span class="line">            .map((MapFunction&lt;Row, MoviePair&gt;) row -&gt; &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> MoviePair(</span><br><span class="line">                                row.getAs(<span class="string">"movie1"</span>),</span><br><span class="line">                                row.getAs(<span class="string">"movie2"</span>),</span><br><span class="line">                                row.getAs(<span class="string">"rating1"</span>),</span><br><span class="line">                                row.getAs(<span class="string">"rating2"</span>));</span><br><span class="line">            &#125;, Encoders.bean(MoviePair<span class="class">.<span class="keyword">class</span>))</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* group by (movie1,movie2),每一行都是所有的（rating1， rating2）的组合 */</span></span><br><span class="line">    Dataset&lt;Row&gt; movieRatingPairs = uniqueRatings</span><br><span class="line">            .groupBy(<span class="string">"movie_pair"</span>)</span><br><span class="line">            .agg(functions.collect_list(<span class="string">"rating_pair"</span>).as(<span class="string">"rating_pairs"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">/* dump to file */</span></span><br><span class="line">    movieSimilarities.coalesce(<span class="number">10</span>).write().csv(<span class="string">"movie_relationship"</span>);</span><br></pre></td></tr></table></figure>



<h4 id="Map-for-similarity"><a href="#Map-for-similarity" class="headerlink" title="Map : for similarity"></a>Map : for similarity</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Dataset&lt;MovieSimilarity&gt; <span class="title">computeSimilarity</span><span class="params">(Dataset&lt;Row&gt; movieRatingPairs)</span> </span>&#123;</span><br><span class="line">    Dataset&lt;MovieSimilarity&gt; movieSimilarities = movieRatingPairs</span><br><span class="line">            .map((MapFunction&lt;Row, MovieSimilarity&gt;) row-&gt; &#123;</span><br><span class="line">                WrappedArray&lt;Integer&gt; movie_pair = row.getAs(<span class="string">"movie_pair"</span>);</span><br><span class="line">                WrappedArray&lt;WrappedArray&lt;Double&gt;&gt; rating_pairs = row.getAs(<span class="string">"rating_pairs"</span>);</span><br><span class="line">                <span class="keyword">int</span> num_pairs = rating_pairs.size();</span><br><span class="line">                <span class="keyword">double</span> sum_xx = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">double</span> sum_yy =<span class="number">0</span>;</span><br><span class="line">                <span class="keyword">double</span> sum_xy=<span class="number">0</span>;</span><br><span class="line">                Iterator&lt;WrappedArray&lt;Double&gt;&gt; iterator = rating_pairs.iterator();</span><br><span class="line">                <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                    WrappedArray rating = iterator.next();</span><br><span class="line">                    <span class="keyword">double</span> rating1 = (<span class="keyword">double</span>) rating.head();</span><br><span class="line">                    <span class="keyword">double</span> rating2 = (<span class="keyword">double</span>) rating.last();</span><br><span class="line">                    sum_xx += rating1*rating1;</span><br><span class="line">                    sum_yy += rating2*rating2;</span><br><span class="line">                    sum_xy += rating1*rating2;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">double</span> denominator = Math.sqrt(sum_xx) * Math.sqrt(sum_yy);</span><br><span class="line">                <span class="keyword">double</span> score = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span> (denominator != <span class="number">0</span>) &#123;</span><br><span class="line">                    score = sum_xy/denominator;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> MovieSimilarity(movie_pair.head(), movie_pair.last(), score, num_pairs);</span><br><span class="line">    &#125;, Encoders.bean(MovieSimilarity<span class="class">.<span class="keyword">class</span>))</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> movieSimilarities;</span><br></pre></td></tr></table></figure>

<h4 id="Using-Similarity"><a href="#Using-Similarity" class="headerlink" title="Using Similarity"></a>Using Similarity</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">        RecommendMovie main = <span class="keyword">new</span> RecommendMovie();</span><br><span class="line">        Dataset&lt;Row&gt; movieSimilarity = main.getMovieSimilarity(spark);</span><br><span class="line"><span class="comment">//        int targetMovie = Integer.parseInt(args[0]);</span></span><br><span class="line">        <span class="keyword">int</span> targetMovie = <span class="number">6</span>;</span><br><span class="line">        <span class="keyword">double</span> scoreThreshold = Double.parseDouble(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">int</span> pairThreshold = Integer.parseInt(args[<span class="number">2</span>]);</span><br><span class="line">        System.out.println(<span class="string">"my args: "</span> + targetMovie);</span><br><span class="line">        main.findMostSimilarMovies(spark, movieSimilarity, targetMovie, scoreThreshold, pairThreshold);</span><br><span class="line"><span class="comment">/***</span></span><br><span class="line"><span class="comment">my args: 6</span></span><br><span class="line"><span class="comment">+--------------------+</span></span><br><span class="line"><span class="comment">|      original_title|</span></span><br><span class="line"><span class="comment">+--------------------+</span></span><br><span class="line"><span class="comment">|Sissi - Die junge...|</span></span><br><span class="line"><span class="comment">|         The Patriot|</span></span><br><span class="line"><span class="comment">|                null|</span></span><br><span class="line"><span class="comment">|         Poltergeist|</span></span><br><span class="line"><span class="comment">|Till det som är v...|</span></span><br><span class="line"><span class="comment">+--------------------+</span></span><br><span class="line"><span class="comment">***/</span></span><br></pre></td></tr></table></figure>







<h3 id="Action"><a href="#Action" class="headerlink" title="Action"></a>Action</h3><ul>
<li>Collect(), take(), count(), first(), foreach()</li>
</ul>
<h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><ul>
<li>groupByKey</li>
</ul>
<p>Work –&gt; survey, algorithm, </p>
<p>​                Table</p>
<p>big_data</p>
<p>大腿们能否请教个Query~</p>
<p>表A<br>| notification_id    | type                                                 |<br>| uuid                      | AorBorC 反正是categorial的       |</p>
<p>表B<br>|id.            |       notification_id      | visitor_token         |<br>|uuid         |         就是外键指向A    |   uuid                     |</p>
<p>我只是想知道<br>表A里不同的 type各自会造成多少的 subscribe_rate  (字段名字已换掉，但关系大致是这样)</p>
<p>表B里的就是已经订阅了的意思，</p>
<p>用SELECT id AS _id, type FROM tbl_A GROUP BY tbl_A.type<br>是会有各个type 的东西，如<br>A  300<br>B. 300<br>C. 400</p>
<p>但为什么我<br>SELECT tbl_a.type, count(<em>)<br>FROM  tbl_a<br>INNER JOIN tbl_b<br>ON tbl_a.notification_id = tbl_b.notification_id<br>GROUP BY tbl_a.type<br>ORDER BY count(</em>) DESC;<br>出来的结果，A, B, C各个数字都还比较大。。　我觉得应该要比较小呀。。这样我才可以把出来的除以对A表单做的GroupBy的Query. .<br>我试着拉个1000 row到本地做实验，但是<br>最后出来的都是空的。。。　＞＜　真的不行了，所以想请教下。。</p>
<p>abcdefghijklmnopqrstuvwxyz</p>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/78/">78</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2023 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>