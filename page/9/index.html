<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-180692466-1', 'auto');
ga('send', 'pageview');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">386</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">66</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Awaken Desparado</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/12/19/BigData_java/2020-12-19-LAB3:%20Hadoop%20based%20DFS%20&amp;%20Hive%20on%20EMR%20/">BigData_java/2020-12-19-LAB3: Hadoop based DFS &amp; Hive on EMR </a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-19</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/BigData-java/">BigData_java</a></span><div class="content"><h1 id="HDFS-Hadoop-based-DFS-amp-Hive-on-EMR"><a href="#HDFS-Hadoop-based-DFS-amp-Hive-on-EMR" class="headerlink" title="HDFS(Hadoop based DFS) &amp; Hive on EMR"></a>HDFS(Hadoop based DFS) &amp; Hive on EMR</h1><h2 id="Access-Remote-EMR-clusters"><a href="#Access-Remote-EMR-clusters" class="headerlink" title="Access Remote EMR clusters"></a>Access Remote EMR clusters</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># joe @ MacBook-Pro-4 in ~/Desktop/mi_BigData/Codes_BD [21:38:47]</span></span><br><span class="line">$ ssh -i onionAWS.pem hadoop@ec2-18-191-169-131.us-east-2.compute.amazonaws.com</span><br><span class="line">The authenticity of host <span class="string">'ec2-18-191-169-131.us-east-2.compute.amazonaws.com (18.191.169.131)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:UdI9VoiJMwSRoJ/SHjNpgger1vgUIxa4MgWtKU9XQaE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>ec2-18-191-169-131.us-east-2.compute.amazonaws.com,18.191.169.131<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Last login: Sat Dec 19 14:59:38 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       __|  __|_  )</span></span><br><span class="line"><span class="string">       _|  (     /   Amazon Linux 2 AMI</span></span><br><span class="line"><span class="string">      ___|\___|___|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://aws.amazon.com/amazon-linux-2/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR</span></span><br><span class="line"><span class="string">E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R</span></span><br><span class="line"><span class="string">EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R</span></span><br><span class="line"><span class="string">  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R</span></span><br><span class="line"><span class="string">  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R</span></span><br><span class="line"><span class="string">  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR</span></span><br><span class="line"><span class="string">  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R</span></span><br><span class="line"><span class="string">  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R</span></span><br><span class="line"><span class="string">EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ ls</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs --help</span></span><br><span class="line"><span class="string">--help: Unknown command</span></span><br><span class="line"><span class="string">Usage: hadoop fs [generic options]</span></span><br><span class="line"><span class="string">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-cat [-ignoreCrc] &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-checksum &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-chgrp [-R] GROUP PATH...]</span></span><br><span class="line"><span class="string">	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span></span><br><span class="line"><span class="string">	[-chown [-R] [OWNER][:[GROUP]] PATH...]</span></span><br><span class="line"><span class="string">	[-copyFromLocal [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span></span><br><span class="line"><span class="string">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span></span><br><span class="line"><span class="string">	[-df [-h] [&lt;path&gt; ...]]</span></span><br><span class="line"><span class="string">	[-du [-s] [-h] [-x] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-expunge]</span></span><br><span class="line"><span class="string">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span></span><br><span class="line"><span class="string">	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-getfacl [-R] &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-help [cmd ...]]</span></span><br><span class="line"><span class="string">	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span></span><br><span class="line"><span class="string">	[-mkdir [-p] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-mv &lt;src&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span></span><br><span class="line"><span class="string">	[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span></span><br><span class="line"><span class="string">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span></span><br><span class="line"><span class="string">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-stat [format] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-tail [-f] &lt;file&gt;]</span></span><br><span class="line"><span class="string">	[-test -[defsz] &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-text [-ignoreCrc] &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-touchz &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-usage [cmd ...]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generic options supported are:</span></span><br><span class="line"><span class="string">-conf &lt;configuration file&gt;        specify an application configuration file</span></span><br><span class="line"><span class="string">-D &lt;property=value&gt;               define a value for a given property</span></span><br><span class="line"><span class="string">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides '</span>fs.defaultFS<span class="string">' property from configurations.</span></span><br><span class="line"><span class="string">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span></span><br><span class="line"><span class="string">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span></span><br><span class="line"><span class="string">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span></span><br><span class="line"><span class="string">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The general command line syntax is:</span></span><br><span class="line"><span class="string">command [genericOptions] [commandOptions]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true</span></span><br><span class="line"><span class="string">hive&gt; show databases;</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">default</span></span><br><span class="line"><span class="string">Time taken: 0.722 seconds, Fetched: 1 row(s)</span></span><br><span class="line"><span class="string">hive&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><ul>
<li><p>比如5台機器在hadoop 集群裡，hdfs 或我們的分布式文件存儲系統就存在於這5台機器上，不需要去關心裡面，就是黑箱，所有合、拆、刪在裡面；我只要在hdfs平台去交互，我只要給他data，他要怎麼做是他內部自己操作</p>
</li>
<li><p>本質就是文件系統</p>
</li>
<li><p>把data存在N台機器上，但client 處理了內部的所有黑箱，所以我只有一個對手! 就如果我在本地</p>
</li>
</ul>
<h2 id="HDFS-commands"><a href="#HDFS-commands" class="headerlink" title="HDFS commands"></a>HDFS commands</h2><h5 id="S3-–-基於AWS的-dfs，HDFS-–-基於Hadoop的-dfs，本質是非常相似的，只不過S3是UI介面"><a href="#S3-–-基於AWS的-dfs，HDFS-–-基於Hadoop的-dfs，本質是非常相似的，只不過S3是UI介面" class="headerlink" title="S3 – 基於AWS的 dfs，HDFS – 基於Hadoop的 dfs，本質是非常相似的，只不過S3是UI介面"></a>S3 – 基於AWS的 dfs，HDFS – 基於Hadoop的 dfs，本質是非常相似的，只不過S3是UI介面</h5><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:10 .hiveJars</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -mkdir bigdata</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:10 .hiveJars</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:23 bigdata</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ vim file.csv</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ ls</span><br><span class="line">file.csv</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -put file.csv bigdata/</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:10 .hiveJars</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:28 bigdata</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls data/</span><br><span class="line">ls: `data/<span class="string">': No such file or directory</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls bigdata/</span></span><br><span class="line"><span class="string">Found 1 items</span></span><br><span class="line"><span class="string">-rw-r--r--   1 hadoop hadoop         19 2020-12-19 15:28 bigdata/file.csv</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -cat bigdata/file.csv</span></span><br><span class="line"><span class="string">i. love. big. data</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -cat bigdata/file1.csv</span></span><br><span class="line"><span class="string">cat: `bigdata/file1.csv'</span>: No such file or directory</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -rm bigdata/file.csv</span><br><span class="line">Deleted bigdata/file.csv</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -rm bigdata</span><br><span class="line">rm: `bigdata<span class="string">': Is a directory</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -rmr bigdata</span></span><br><span class="line"><span class="string">rmr: DEPRECATED: Please use '</span>-rm -r<span class="string">' instead.</span></span><br><span class="line"><span class="string">Deleted bigdata</span></span><br></pre></td></tr></table></figure>



<h2 id="Create-Hive-DB-amp-Table"><a href="#Create-Hive-DB-amp-Table" class="headerlink" title="Create Hive DB &amp; Table"></a>Create Hive DB &amp; Table</h2><h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h4><ul>
<li>只是把data映射出來讓大家很好地作query</li>
<li>仍是存儲在HDFS上，以文件的型式存儲的</li>
<li>所以該指定的除了 Table Name、Schema, 還該指定最底層的data source的format, 可以是 ORC, Parquet, Avro，由我指定，然後data會被自動存成這個型式，hive會幫我們做這些操作! 不需要操心<ul>
<li>我需要操心的是寫進去hdfs時，想要以怎樣的一個文件型式寫入!</li>
</ul>
</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8q6u4gj20o607omxk.jpg" alt="image-20201219234856829" style="zoom: 33%;" />

<ul>
<li>所有的指令都和SQL一樣，底層就是antler的解析器</li>
<li>hive還是要存去底層給hdfs上去的，然後要指定format，這邊指定成了 ORC</li>
</ul>
<figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@ip-172-31-47-22 ~]$ hive</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> file:/etc/hive/conf.dist/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">hive&gt; show database;</span><br><span class="line">NoViableAltException(78@[846:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | createMaterializedViewStatement | dropViewStatement | dropMaterializedViewStatement | createFunctionStatement | createMacroStatement | createIndexStatement | dropIndexStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=&gt; grantPrivileges | ( revokePrivileges )=&gt; revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole | abortTransactionStatement );])</span><br><span class="line">	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)</span><br><span class="line">	at org.antlr.runtime.DFA.predict(DFA.java:116)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:3757)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2382)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:244)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:158)</span><br><span class="line">FAILED: ParseException line 1:5 cannot recognize input near <span class="string">'show'</span> <span class="string">'database'</span> <span class="string">'&lt;EOF&gt;'</span> <span class="keyword">in</span> ddl statement</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: 0.263 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; create bigdata;</span><br><span class="line">NoViableAltException(24@[846:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | createMaterializedViewStatement | dropViewStatement | dropMaterializedViewStatement | createFunctionStatement | createMacroStatement | createIndexStatement | dropIndexStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=&gt; grantPrivileges | ( revokePrivileges )=&gt; revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole | abortTransactionStatement );])</span><br><span class="line">	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)</span><br><span class="line">	at org.antlr.runtime.DFA.predict(DFA.java:144)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:3757)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2382)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:244)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:158)</span><br><span class="line">FAILED: ParseException line 1:7 cannot recognize input near <span class="string">'create'</span> <span class="string">'bigdata'</span> <span class="string">'&lt;EOF&gt;'</span> <span class="keyword">in</span> ddl statement</span><br><span class="line">hive&gt; create database bigdata</span><br><span class="line">    &gt; ;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.301 seconds</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">bigdata</span><br><span class="line">default</span><br><span class="line">Time taken: 0.028 seconds, Fetched: 2 row(s)</span><br><span class="line">hive&gt; use bigdata;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.105 seconds</span><br><span class="line">hive&gt; CREATE TABLE IF NOT EXISTS bigdata.movie_similarity</span><br><span class="line">    &gt; (</span><br><span class="line">    &gt; movie1          INT,</span><br><span class="line">    &gt; movie2 INT,</span><br><span class="line">    &gt; num_pairs INT,</span><br><span class="line">    &gt; similarity DOUBLE</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; STORED AS ORC;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.56 seconds</span><br><span class="line">hive&gt; describe movie_similarity;</span><br><span class="line">OK</span><br><span class="line">movie1              	int</span><br><span class="line">movie2              	int</span><br><span class="line">num_pairs           	int</span><br><span class="line">similarity          	double</span><br><span class="line">Time taken: 0.093 seconds, Fetched: 4 row(s)</span><br><span class="line">hive&gt; describe formatted movie_similarity;</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># col_name            	data_type           	comment</span></span><br><span class="line"></span><br><span class="line">movie1              	int</span><br><span class="line">movie2              	int</span><br><span class="line">num_pairs           	int</span><br><span class="line">similarity          	double</span><br><span class="line"></span><br><span class="line"><span class="comment"># Detailed Table Information</span></span><br><span class="line">Database:           	bigdata</span><br><span class="line">Owner:              	hadoop</span><br><span class="line">CreateTime:         	Sat Dec 19 15:53:27 UTC 2020</span><br><span class="line">LastAccessTime:     	UNKNOWN</span><br><span class="line">Retention:          	0</span><br><span class="line">Location:           	hdfs://ip-172-31-47-22.us-east-2.compute.internal:8020/user/hive/warehouse/bigdata.db/movie_similarity</span><br><span class="line">Table Type:         	MANAGED_TABLE</span><br><span class="line">Table Parameters:</span><br><span class="line">	COLUMN_STATS_ACCURATE	&#123;\"BASIC_STATS\":\"<span class="literal">true</span>\"&#125;</span><br><span class="line">	numFiles            	0</span><br><span class="line">	numRows             	0</span><br><span class="line">	rawDataSize         	0</span><br><span class="line">	totalSize           	0</span><br><span class="line">	transient_lastDdlTime	1608393207</span><br><span class="line"></span><br><span class="line"><span class="comment"># Storage Information</span></span><br><span class="line">SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde</span><br><span class="line">InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat</span><br><span class="line">OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat</span><br><span class="line">Compressed:         	No</span><br><span class="line">Num Buckets:        	-1</span><br><span class="line">Bucket Columns:     	[]</span><br><span class="line">Sort Columns:       	[]</span><br><span class="line">Storage Desc Params:</span><br><span class="line">	serialization.format	1</span><br><span class="line">Time taken: 0.125 seconds, Fetched: 33 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>



<h2 id="Write-data-into-Hive-Table-fr-Spark"><a href="#Write-data-into-Hive-Table-fr-Spark" class="headerlink" title="Write data into Hive Table, fr Spark"></a>Write data into Hive Table, fr Spark</h2><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8slwhcj21e406mq3w.jpg" alt="image-20201220001941568" style="zoom:50%;" />

<ul>
<li><p>得確認我所generate 出來的DataFrame的schema跟我所需要寫進去的Table裡的schema是完全一致的，不然就會報錯</p>
</li>
<li><p>run Step 的差别在：現在在EMR上run它的JAR的話，data不是存到S3，而是會存到hive連下去的DFS</p>
</li>
</ul>
<h5 id="當需要的是「非static-data」，而是「動態被傳入的-data」，Arguments-上就要被進行一個傳入"><a href="#當需要的是「非static-data」，而是「動態被傳入的-data」，Arguments-上就要被進行一個傳入" class="headerlink" title="當需要的是「非static data」，而是「動態被傳入的 data」，Arguments 上就要被進行一個傳入"></a>當需要的是「非static data」，而是「動態被傳入的 data」，Arguments 上就要被進行一個傳入</h5><ul>
<li><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltmtsovq8j317i0owq9d.jpg" alt="image-20201220003046498" style="zoom:50%;" />



</li>
</ul>
<ul>
<li><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a87yanmj20sm09u3zs.jpg" alt="image-20201220003235826" style="zoom:50%;" />





</li>
</ul>
<h2 id="Interact-with-data-in-Hive-Table"><a href="#Interact-with-data-in-Hive-Table" class="headerlink" title="Interact with data in Hive Table"></a>Interact with data in Hive Table</h2><p>Complete後<br><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltmxlpvqtj319o0l87ok.jpg" alt="image-20201220003427565" style="zoom:50%;" /></p>
<ul>
<li>Limit 1 後才有Trigger，就是aws上的hive還是怪怪的</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8abfduj21620nen25.jpg" alt="image-20201220003534838" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltmz8c6y9j30n00fstnt.jpg" alt="image-20201220003601410" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8giz5qj216a0fcwka.jpg" alt="image-20201220003929425" style="zoom:50%;" />



<ul>
<li>snappy就是個常見的壓縮方式</li>
</ul>
<p>想要找到某一個target 的movie所對應的相關聯的電影中的top 3；我們的方法呢？</p>
<p>那時還沒有hive，也沒有query engine，就只能建新的java的file叫 recommend_movie.java 去query output 的 ORC的data，然後把這些data load到DataFrame去，然後DataFrame再進行一系列的 JOIN的操作，然後再把結果給print出來</p>
<p>現在有了Hive，也許就不需要再Spark這種heavy的操作</p>
<ul>
<li><p>Spark – 需要 </p>
<ul>
<li>寫一個spark application</li>
<li>要把spark applicateion 上傳去AWS</li>
<li>要去add step、去執行它<ul>
<li>再等AWS裡面的Spark job還要去 schedule, 之後還要run，run後生成的結果才能去std.out查到!</li>
</ul>
</li>
</ul>
</li>
<li><p>現在有hive，又可以view了，不如就是用hive直接交互數據</p>
</li>
</ul>
<h4 id="Case-movie-target-id-1-相關的前-Top3-movie"><a href="#Case-movie-target-id-1-相關的前-Top3-movie" class="headerlink" title="Case: movie target id=1 相關的前 Top3 movie"></a>Case: movie target id=1 相關的前 Top3 movie</h4><ul>
<li>在 Hive裡 SQL 語句 – query的界面</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltnmjustxj315s0gyasu.jpg" alt="image-20201220005825999"></p>
<ul>
<li>這個時候未啟動 reducer</li>
</ul>
<ul>
<li>↓這時候在reducer裡進行了一系列的排序(?!)，是因為有了排序，所以比較慢了點</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltnn83zk0j316e0h44h3.jpg" alt="image-20201220005905548"></p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8kox1ij20om0lctcp.jpg" alt="image-20201220005953451" style="zoom:50%;" />

<ul>
<li>偷看一眼用</li>
<li>這樣工作效率就高很多，就是因為有 Hive! 可以取代了 Spark code in java</li>
<li>可以簡化了基礎的流程</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/12/19/BigData_java/2020-12-20-Structued%20Streaminng/">BigData_java/2020-12-20-Structued Streaminng</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-19</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/BigData-java/">BigData_java</a></span><div class="content"><h1 id="Structured-Streaming"><a href="#Structured-Streaming" class="headerlink" title="Structured Streaming"></a>Structured Streaming</h1></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/12/06/System-Design/2020-12-06-Our-IG-Service/">System-Design/2020-12-06-Our-IG-Service</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/SystemDesign/">SystemDesign</a></span><div class="content"><p>###</p>
<p><img src="https://p.ipic.vip/4i2nfa.png" alt="image-20230601111522503"></p>
<p>Together with : Grace, Guoguo, Wu, Congcong, Sisi</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/12/04/BigData_java/2020-12-04%20Distributed%20File%20System/">BigData_java/2020-12-04 Distributed File System</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/BigData-java/">BigData_java</a></span><div class="content"><h1 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h1><h3 id="文件存儲形式"><a href="#文件存儲形式" class="headerlink" title="文件存儲形式"></a>文件存儲形式</h3><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a7wdhq8j217s038gmb.jpg" alt="image-20201212101801165" style="zoom:50%;" />



<ul>
<li><p>ps chrome w/ enter</p>
</li>
<li><p>Database, table, content</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">database</span>;</span><br><span class="line"><span class="keyword">use</span> bigdata;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> bigdata.movie_sililarity</span><br></pre></td></tr></table></figure>





</li>
</ul>
<h2 id="My-prev-Ref"><a href="#My-prev-Ref" class="headerlink" title="My prev. Ref"></a>My prev. Ref</h2><h1 id="General"><a href="#General" class="headerlink" title="General"></a>General</h1><ul>
<li>GFS Client</li>
<li>Heart Beat</li>
</ul>
<blockquote>
<p>GFS 采用的是 Master-slave 的架构。他的一些基本设计原理，如 chunk 的设计（对应文件系统中的 block）和普通文件系统是相通的。整套系统的设计是基于一些合理假设的（这些假设都是Google从大量工程实践中总结出来的），其中一条假设就是：“We expect a few million files, each typically 100MB or larger in size”。如果实在需要存很多小文件的话， 其实可以将这些小文件打包成一个大文件然后再存储，只要记录下每个小文件在文件内部的起始字节和大小就行了。</p>
</blockquote>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="要解決的問題"><a href="#要解決的問題" class="headerlink" title="要解決的問題"></a>要解決的問題</h2><ul>
<li><p>Storage不够、QPS太大的問題</p>
</li>
<li><p>Google 很多小計算機建DFS</p>
<p>ＶＳ</p>
<p>SUN大型、貴的（大家買不起），09年被Oracle收了</p>
</li>
<li><p>GFS、BigTable、MapReduce</p>
</li>
</ul>
<h1 id="GFS"><a href="#GFS" class="headerlink" title="GFS"></a>GFS</h1><h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><ul>
<li>作為一個文件系統，一定要提供兩種常用的操作：寫跟讀<ul>
<li>寫：要文件名、內容</li>
<li>讀：文件名，返回文件內容</li>
</ul>
</li>
</ul>
<h3 id="需求1-總存储量有多大"><a href="#需求1-總存储量有多大" class="headerlink" title="需求1: 總存储量有多大?"></a>需求1: 總存储量有多大?</h3><ul>
<li>比如 &gt; 1000T，才會上分布式；機器數也是越大越好</li>
</ul>
<h3 id="需求2-多台機器，越多越好"><a href="#需求2-多台機器，越多越好" class="headerlink" title="需求2: 多台機器，越多越好"></a>需求2: 多台機器，越多越好</h3><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><ul>
<li><p>如圖書館要有Server、Client。</p>
<p>GFS提供的一個就是讀取，另一個是寫入的服務</p>
</li>
</ul>
<h3 id="Q-多台Server間怎溝通？"><a href="#Q-多台Server間怎溝通？" class="headerlink" title="Q: 多台Server間怎溝通？"></a>Q: 多台Server間怎溝通？</h3><h4 id="P2P"><a href="#P2P" class="headerlink" title="P2P"></a>P2P</h4><p>平級溝通</p>
<ul>
<li>優：一台掛了還可以工作，沒有單點故障問題</li>
<li>缺：但，大家平級，要常通信保持一致性。</li>
<li>p2p的通信一般是比較難寫的，直覺就是。</li>
<li>Distributed I/O –&gt; data精確度比較弱</li>
</ul>
<h4 id="V-Master-vs-Slaves，因為抄GFS"><a href="#V-Master-vs-Slaves，因為抄GFS" class="headerlink" title="(V) Master vs Slaves，因為抄GFS"></a>(V) Master vs Slaves，因為抄GFS</h4><p>老大一致對外，對內分配幹活</p>
<ul>
<li><p>優: 數據易<strong>保持一致性</strong>。就老大分配</p>
<ul>
<li>設計簡單，負責人固定</li>
</ul>
</li>
<li><p>缺：有單點robust問題。大哥master掛了，整個系統不work的問題</p>
</li>
<li><p>這是GFS最終的選擇，他是後端的服務，不是前端的，所以掛個一分鐘不會怎樣，就如果master掛了，把master重啟就好了。</p>
</li>
</ul>
<blockquote>
<h5 id="单选题-大家猜猜GFS会用哪种设计模式？"><a href="#单选题-大家猜猜GFS会用哪种设计模式？" class="headerlink" title="单选题]大家猜猜GFS会用哪种设计模式？"></a>单选题]大家猜猜GFS会用哪种设计模式？</h5><p>A.社会主义 P2P23.06% 选择</p>
<p>B.资本主义 Master Slave76.94% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是A</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>Master Slave的优势是：<br>设计简单<br>数据很容易保持一致</p>
</blockquote>
<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><h3 id="Q-大文件存在哪？"><a href="#Q-大文件存在哪？" class="headerlink" title="Q: 大文件存在哪？"></a>Q: 大文件存在哪？</h3><ul>
<li><p>當然是disk，10PB只有disk可以，內存不可能啦</p>
</li>
<li><p>如何存到這個文件系統裡？</p>
<ul>
<li>怎麼設計GFS?</li>
<li>怎麼存？如果總量 &lt; 100G？</li>
</ul>
</li>
</ul>
<blockquote>
<h5 id="单选题-在-GFS-中，大文件存在哪儿？"><a href="#单选题-在-GFS-中，大文件存在哪儿？" class="headerlink" title="[单选题]在 GFS 中，大文件存在哪儿？"></a>[单选题]在 GFS 中，大文件存在哪儿？</h5><p>A.内存中，读写快！5.03% 选择</p>
<p>B.硬盘中，超大空间！94.97% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
</blockquote>
<h3 id="Q-Metadata存哪？"><a href="#Q-Metadata存哪？" class="headerlink" title="Q: Metadata存哪？"></a>Q: Metadata存哪？</h3><ul>
<li>metadata元數據常常被無意識訪問到, 怎麼存好？一般打開文件夾就被動看到了</li>
</ul>
<blockquote>
<h5 id="单选题-Meta-Data应该怎样储存？"><a href="#单选题-Meta-Data应该怎样储存？" class="headerlink" title="[单选题]Meta Data应该怎样储存？"></a>[单选题]Meta Data应该怎样储存？</h5><p><strong>A.所有文件的Meta Data 全放磁盘开头83.51% 选择</strong></p>
<p>B.每个文件的Meta Data都和它的内容放在一起16.49% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
<p><strong>正确答案:</strong>A</p>
<p><strong>解析:</strong></p>
<p>全放磁盘开头可以减少磁盘的寻轨时间</p>
</blockquote>
<p>理由跟硬盤的結構有關，機械硬盤的磁頭到軌道有尋軌時間的，一般磁頭跳到要找的位置要10ms</p>
<p>如果是選B，磁頭就要一直跳，100個文件就要10ms*100就一秒了。</p>
<p>A可以一次全讀出去。</p>
<ul>
<li>文件內容怎麼放呢？在磁盤中也有兩種方式：就是要不要把每個文件拆成各小塊交錯放呢？</li>
</ul>
<blockquote>
<h5 id="单选题-文件内容应怎样储存？"><a href="#单选题-文件内容应怎样储存？" class="headerlink" title="[单选题]文件内容应怎样储存？"></a>[单选题]文件内容应怎样储存？</h5><p>A.文件整体存储，所有文件连在一起21.44% 选择</p>
<p>B.将每个文件等分成很多小块后再存78.56% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>等分成很多小块后存储可以方便文件的修改操作</p>
</blockquote>
<p>如果選A的話，當2號文件是10KB這麼大，寫入後變100KB了，怎麼寫？還得把2號先砍了，再往後找一個夠100KB大小的空間寫進去。</p>
<p>選B的話，文件本來就是一小塊一小塊了，<strong>後面要增加90KB的文件就順勢往後寫就OK了</strong>。</p>
<p>一個硬盤就是一個非常大的array；</p>
<p>一般NTFS(win)、XFS(linux)的文件系統裡的默認Block是　<strong><em>4KB</em></strong></p>
<ul>
<li><p>本來的100G如果變成了100TB會遇到什麼問題？</p>
<ul>
<li><p>Block的數量會超多。要拿來儲存block的空間就已經非常大了！</p>
<ul>
<li>100x1024G = 100x 1024x1024M = 100x1024x1024x1024KB = 25x1024x1024x1024 <strong>blocks</strong></li>
</ul>
</li>
<li><p>怎麼改進？ ==&gt; 增加Block的大小。可能就是變成64MB，然後把這個改名叫<strong>Chunk</strong></p>
<ul>
<li><p>優：Reduce size of Metadata，就是筆數變少了</p>
</li>
<li><p>劣：浪費了些小空間，如果我要存的就是個小文章４MB，那也要拿64MB去存，60MB就變成了磁盤碎片浪費了。但就這樣吧。。</p>
<p>結論：Z &gt; B . </p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>对于这个trade off，我感觉十分牵强啊。增加chunk size是为了减少meta data 的数量，但是试想，如果存的文件全是非常非常小，都是几百kb或者几mb的，那这样岂不是更浪费空间，造成大量碎片？<ul>
<li>这个在GFS的paper里面有解释，<strong><em>GFS的设计是基于一些假设的（这些假设都是Google从大量工程实践中总结出来的</em></strong>），其中一条假设就是：“We expect a few million files, each typically 100MB or larger in size”。这是其一。另外，如果实在需要存很多小文件的话， 其实可以将这些小文件打包成一个大文件然后再存储，只要记录下每个小文件在文件内部的起始字节和大小就行了。</li>
</ul>
</li>
<li>如果在写新文件时发现之前预先留给metadata的空间被用完了怎么办呢？<ul>
<li>这时候应该就写不进去了 生产环境应该及时关注磁盘容量告警</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Q-100G-on-普通的OS怎存"><a href="#Q-100G-on-普通的OS怎存" class="headerlink" title="Q: 100G on 普通的OS怎存?"></a>Q: 100G on 普通的OS怎存?</h3><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcv05xsj30vc0g63zp.jpg" alt="image-20200822205723260"></p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzvxy7io1j30c607edgd.jpg" alt="image-20200822205814035" style="zoom: 33%;" />



<ol>
<li>因为比较小，預設就是4KB，一般就是照block傳就好了</li>
<li>如果再大一點怎辦？如果再按block算，就會有很多很多個block，去尋執很麻煩，不然我們就by chunk</li>
<li>如果100P就要上多台機子了，P = 1000T</li>
</ol>
<p>當然指的不是一個，而是很多個文件啦</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcwolz8j31160hwgnc.jpg" alt="image-20200822205929382"></p>
<h3 id="Q-100P-真的很大怎存？"><a href="#Q-100P-真的很大怎存？" class="headerlink" title="Q: 100P, 真的很大怎存？"></a>Q: 100P, 真的很大怎存？</h3><p>上分布式</p>
<h2 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrd0xlcfj317o0l40uu.jpg" alt="image-20200822210535805"></p>
<h3 id="Q-10PB存得下嗎？需要多台電腦-Master-Slave工作模式。"><a href="#Q-10PB存得下嗎？需要多台電腦-Master-Slave工作模式。" class="headerlink" title="Q: 10PB存得下嗎？需要多台電腦 Master-Slave工作模式。"></a>Q: 10PB存得下嗎？需要多台電腦 Master-Slave工作模式。</h3><ul>
<li>現在一台服務器現在最多插10個硬盤，一個硬盤撐死就100TB，這樣也就才0.1P，這樣如此的電腦至少要來100台共同工作才能做得了10PB的事</li>
</ul>
<blockquote>
<h5 id="单选题-在-GFS-中-Master-和-Slave-分别存什么数据？"><a href="#单选题-在-GFS-中-Master-和-Slave-分别存什么数据？" class="headerlink" title="[单选题]在 GFS 中 Master 和 Slave 分别存什么数据？"></a>[单选题]在 GFS 中 Master 和 Slave 分别存什么数据？</h5><p>A.Master 存 Metadata，Slave 存实际的文件内容96.81% 选择</p>
<p>B.Master 存实际的文件内容，Slave 存 Metadata3.19% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
</blockquote>
<p>Master存了所有的chunk該在哪台小弟的info</p>
<p><strong><em>所以就是slave就是chunk server</em></strong></p>
<blockquote>
<p>[单选题]GFS Master 是否有必要存储每个 chunk 在 Slave Server 上的 Offset?</p>
<p>你的选择:B</p>
<p>A:有必要</p>
<p>B:没有必要</p>
<p>答对了，您选择的答案是B</p>
</blockquote>
<p>這樣可減輕master壓力；而且slave裡面可以自己調整位置不用通跟master通信。</p>
<p><em>１chunk = <strong>64MB need 64B的metadata(經驗值)</strong>, 10PB needs 10G metadata, 存內存都可以！</em></p>
<blockquote>
<ul>
<li>如果master 挂了 只能恢复master ？ 不能promote slave server 变成master？<ul>
<li>GFS 里的 Master 的概念和 MySQL 里的 MasterSlave 的概念不同。GFS 里的 Master 只负责管理，不负责数据存储。MySQL 里的 Master Slave 都只负责数据存储不负责管理。要注意区分。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Q-每台-chunk-的Offset偏移量可否不在Master上？"><a href="#Q-每台-chunk-的Offset偏移量可否不在Master上？" class="headerlink" title="Q: 每台 chunk 的Offset偏移量可否不在Master上？"></a>Q: 每台 chunk 的Offset偏移量可否不在Master上？</h3><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcvagnsj31660k00w6.jpg" alt="image-20200822210804854"></p>
<h3 id="Q-Master-存10P文件的metadata要多少容量？"><a href="#Q-Master-存10P文件的metadata要多少容量？" class="headerlink" title="Q: Master 存10P文件的metadata要多少容量？"></a>Q: Master 存10P文件的metadata要多少容量？</h3><p>1 chunk = 64MB needs 64B. (经验值) 10P needs 10G</p>
<h1 id="One-Workable-Sol"><a href="#One-Workable-Sol" class="headerlink" title="One Workable Sol!"></a>One Workable Sol!</h1><h2 id="寫"><a href="#寫" class="headerlink" title="寫"></a>寫</h2><h3 id="架構-amp-Q-一個chunk怎麼寫入server的"><a href="#架構-amp-Q-一個chunk怎麼寫入server的" class="headerlink" title="架構 &amp; Q: 一個chunk怎麼寫入server的?"></a>架構 &amp; Q: 一個chunk怎麼寫入server的?</h3><p>這個還沒有做scale</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcw8r33j312y0isacp.jpg" alt="image-20200822211653883"></p>
<h4 id="Q-寫入怎麼寫好？一次vs多次"><a href="#Q-寫入怎麼寫好？一次vs多次" class="headerlink" title="Q: 寫入怎麼寫好？一次vs多次"></a>Q: 寫入怎麼寫好？一次vs多次</h4><blockquote>
<h5 id="单选题-怎么将文件写入GFS"><a href="#单选题-怎么将文件写入GFS" class="headerlink" title="[单选题]怎么将文件写入GFS?"></a>[单选题]怎么将文件写入GFS?</h5><p>A.将文件整体一次性写入3.39% 选择</p>
<p>B.将文件拆分成块多次写入96.61% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>不同的块有可能放到不同的Chunk Server，并且分块后方便重传</p>
</blockquote>
<h4 id="Q-從哪續傳好-當多次寫時每份大小？就Chunk，也是傳輸size"><a href="#Q-從哪續傳好-當多次寫時每份大小？就Chunk，也是傳輸size" class="headerlink" title="Q: 從哪續傳好? 當多次寫時每份大小？就Chunk，也是傳輸size"></a>Q: 從哪續傳好? 當多次寫時每份大小？就Chunk，也是傳輸size</h4><p>如果斷開了，可以從哪斷開的從哪續傳</p>
<blockquote>
<h5 id="单选题-GFS-Client-将文件拆分为多大进行传输比较合适？"><a href="#单选题-GFS-Client-将文件拆分为多大进行传输比较合适？" class="headerlink" title="[单选题]GFS Client 将文件拆分为多大进行传输比较合适？"></a>[单选题]GFS Client 将文件拆分为多大进行传输比较合适？</h5><p>A.64k6.92% 选择</p>
<p>B.1M5.73% 选择</p>
<p><strong>C.64M83.89% 选择</strong></p>
<p>D.1G3.46% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是C</p>
<p><strong>正确答案:</strong>C</p>
<p><strong>解析:</strong></p>
<p>64M 是一个 chunk 的大小。GFS 是按照 chunk 为单位进行存储的，所以 64M 为一组比较合适。</p>
</blockquote>
<p>傳輸單位也就是chunk。</p>
<blockquote>
<h5 id="单选题-GFS中每一个Chunk怎么写入Server？"><a href="#单选题-GFS中每一个Chunk怎么写入Server？" class="headerlink" title="[单选题]GFS中每一个Chunk怎么写入Server？"></a>[单选题]GFS中每一个Chunk怎么写入Server？</h5><p>A.把文件传给master，让Master处理所有的事情6.81% 选择</p>
<p>B.仅让Master分配Chunk Server，然后直接把Chunk传给相应的Chunk Server93.19% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p><strong><em>避免Master成为瓶颈</em></strong></p>
</blockquote>
<p>master的硬盤有限，網路也是有限的，自己會卡死，分下去給小弟們，讓client間也不會排隊</p>
<h4 id="Q-要修改一個-mp4-怎辦？"><a href="#Q-要修改一個-mp4-怎辦？" class="headerlink" title="Q: 要修改一個 .mp4 怎辦？"></a>Q: 要修改一個 .mp4 怎辦？</h4><ol>
<li>先刪掉 /gfs/home/xxx.mp4</li>
<li>重把整個文件寫一份</li>
</ol>
<h5 id="結論就-別update"><a href="#結論就-別update" class="headerlink" title="結論就: 別update!"></a>結論就: 別update!</h5><h3 id="總結"><a href="#總結" class="headerlink" title="總結:"></a>總結:</h3><p><strong><em>Master要存所有的metadata,  chunkserver存真正的大data 讀要找到對應的chunkserver，寫時要找到空閒的chunkserver；</em></strong></p>
<ul>
<li><strong>寫入是每次找老大問，老大分配空間，自己去找小弟寫</strong></li>
<li><strong>讀出是問老大拿到chunklist，然後去問小弟拿到chunk就ok</strong></li>
</ul>
<blockquote>
<ul>
<li>請問gfs讀取file的話, client會從master得到一個chunk list, 那我並行讀取每個chunk,還是一個一個循序讀取chunk? 有什麼比較快的讀取方法嗎?<ul>
<li>GFS中，Chunk分散储存在若干个Chunk Server中，读取的时候Client可以从不同的Chunk Server同时读取Chunk，相当于有一个并行的效果</li>
</ul>
</li>
<li>The client is the end user side, or another proxy? I don’t think the end user client should worry about which chunk server to write in.<ul>
<li>The “client” mentioned here is a library that is linked to the end user’s application, serving as an <strong><em>abstraction layer between the application and the underlying GFS</em></strong>.</li>
</ul>
</li>
<li>client读取档案时，master server是给他一个chunk list，那client写入档案时，master是给他一个chunk list让他依次写入，还是一个写完master再告知下一个chunk该分配到那个chunk server?<ul>
<li>给一个 chunk list 让他依次写入。否则 master 和 client 之间通信太多很负累，也没啥意义。</li>
</ul>
</li>
<li>client 定义是什么 ？是end user吗？ 还是只是gfs 跟外界的一个interface？<ul>
<li>gfs 和外界的一个 interface。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="讀"><a href="#讀" class="headerlink" title="讀"></a>讀</h2><h4 id="Q-一次-or-多次？"><a href="#Q-一次-or-多次？" class="headerlink" title="Q: 一次 or 多次？"></a>Q: 一次 or 多次？</h4><h4 id="Q-Client-怎知-xxx-mp4-被切成了多少塊？"><a href="#Q-Client-怎知-xxx-mp4-被切成了多少塊？" class="headerlink" title="Q: Client 怎知 xxx.mp4 被切成了多少塊？"></a>Q: Client 怎知 xxx.mp4 被切成了多少塊？</h4><p>每個chunk知道在哪個server，所以一個file知道分別位在哪些server</p>
<h3 id="架構"><a href="#架構" class="headerlink" title="架構"></a>架構</h3><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrd0jwinj314w0k6wgl.jpg" alt="image-20200822212139023"></p>
<h2 id="SCALE-–-in-One-Workable-Sol"><a href="#SCALE-–-in-One-Workable-Sol" class="headerlink" title="SCALE –  in One Workable Sol"></a>SCALE –  in One Workable Sol</h2><h3 id="Master-做的事"><a href="#Master-做的事" class="headerlink" title="Master 做的事"></a>Master 做的事</h3><h4 id="Q-Master-Task"><a href="#Q-Master-Task" class="headerlink" title="Q: Master Task:"></a>Q: Master Task:</h4><ul>
<li>存儲各個文件數據的 metadata</li>
<li>存儲 Map<ul>
<li>讀取時找到對應的 Chunk Server, </li>
<li>寫入時分配空閒三 Chunk Server</li>
</ul>
</li>
</ul>
<h4 id="Q-單master夠嗎？"><a href="#Q-單master夠嗎？" class="headerlink" title="Q: 單master夠嗎？"></a>Q: 單master夠嗎？</h4><ul>
<li>90%很好了一般都是這樣；大不了就是 Paxos Alg. 的多Master，再多也受不了會有延遲</li>
</ul>
<h5 id="Double-Master"><a href="#Double-Master" class="headerlink" title="Double Master"></a>Double Master</h5><ul>
<li>Paper: Apache Hadoop Goes Realtime at Facebook</li>
</ul>
<h5 id="Multi-Master"><a href="#Multi-Master" class="headerlink" title="Multi Master"></a>Multi Master</h5><ul>
<li>Paper: Paxos Algorithm</li>
</ul>
<h4 id="Q-怎麼看資料有掛了"><a href="#Q-怎麼看資料有掛了" class="headerlink" title="Q: 怎麼看資料有掛了"></a>Q: 怎麼看資料有掛了</h4><ul>
<li><p>CHECKSUM, md5  哈希，原串發生變化，哈希值就巨大變化，一旦不同，就是原數據必毀</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdtqklpedij31260tun45.jpg" alt="image-20200414235740148" style="zoom: 25%;" />



</li>
</ul>
<ul>
<li><p>也可以用 XOR作checksum, </p>
</li>
<li><p>也可以用SHA1, SHA256, SHA512</p>
</li>
<li><p>Read More: <a href="https://en.wikipedia.org/wiki/Checksum" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Checksum</a></p>
</li>
</ul>
<h5 id="Checksum"><a href="#Checksum" class="headerlink" title="Checksum"></a>Checksum</h5><ul>
<li>size: 也就 4Bytes</li>
<li>時機</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a7p7jqqj20am05o3ye.jpg" alt="image-20200822213411583" style="zoom: 50%;" />

<blockquote>
<h5 id="单选题-什么时候写入-checksum-如上圖"><a href="#单选题-什么时候写入-checksum-如上圖" class="headerlink" title="[单选题]什么时候写入 checksum? 如上圖"></a>[单选题]什么时候写入 checksum? 如上圖</h5><p>A.每个一段时间遍历所有数据计算 checksum 并写入3.32% 选择</p>
<p>B.每个 chunk 在写入的时候计算 checksum 并记录在 chunk 的末尾39.40% 选择</p>
<p>C.每个 chunk 在写入的时候计算 checksum 并集中记录在当前的 Slave 上36.58% 选择</p>
<p>D.每个 chunk 在写入的时候计算 checksum 并集中记录在 Master 上20.70% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
</blockquote>
<blockquote>
<h5 id="单选题-一般来说什么时候检查-checksum？"><a href="#单选题-一般来说什么时候检查-checksum？" class="headerlink" title="[单选题]一般来说什么时候检查 checksum？"></a>[单选题]一般来说什么时候检查 checksum？</h5><p>A.每次读取 chunk 的时候重新计算并对比以前的 checksum90.56% 选择</p>
<p>B.周期性的遍历所有数据检查 checksum9.44% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
<p><strong>正确答案:</strong>A</p>
</blockquote>
<p>而且還可以再週期性地檢查；是根據業務調整。</p>
<h4 id="問題集"><a href="#問題集" class="headerlink" title="問題集"></a>問題集</h4><blockquote>
<ul>
<li>master down了重启， 是通过读log来恢复吗<ul>
<li>是的，但是一般会定期制作一个checkpoint，挂掉之后只需要从上一个checkpoint开始重放log就行了，不需要每次都从头开始。</li>
</ul>
</li>
<li>master down了之后怎么恢复？<ul>
<li>master down了之后重启就好</li>
</ul>
</li>
<li>老师请问还有其他的加密算法么？还是只能用这个MD5<ul>
<li>有啊，网上搜一大堆。比如 sha1 sha2, sha256.</li>
</ul>
</li>
<li>Check Sum 检查一位错误的例子，如果刚好有两个数据发生改变，是有可能导致xor结果不变的吧。这种情况我们就不知道有文件损坏了？<ul>
<li>是的。check sum 本来就是 false positive 的。你说的这种情况出现概率是很低的。系统设计的领域里面有很多允许 false postive 或者 false negative 的情况，如 BloomFilter 就是一个例子。这些场景下，我们都不能保证 100% work，但是高概率是有效的。这个系统设计区别于算法设计的很大的不同，要注意体会这个地方。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="更多-Q-amp-A"><a href="#更多-Q-amp-A" class="headerlink" title="更多 Q&amp;A"></a>更多 Q&amp;A</h2><h4 id="How-to-avoid-data-loss-when-a-Chunk-Server-is-down-fail"><a href="#How-to-avoid-data-loss-when-a-Chunk-Server-is-down-fail" class="headerlink" title="How to avoid data loss when a Chunk Server is down/fail?"></a>How to avoid data loss when a Chunk Server is down/fail?</h4><p>​    Ans: replica 作備份</p>
<blockquote>
<p>​    [单选题]GFS的Replica怎么存放？</p>
<p>1.三个备份都放在一个地方（加州）0.97% 选择</p>
<p>2.三个备份放在三个相隔较远的地方（加州，滨州，纽约州）9.99% 选择</p>
<p>3.两个备份相对比较近，另一个放在较远的地方（2个加州，1个滨州）89.04% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是3</p>
<p><strong>正确答案:</strong>3</p>
<p><strong>解析:</strong></p>
<p>两个备份较近，保证出错时快速恢复，一个较远，保证安全性</p>
</blockquote>
<h4 id="How-to-recover-when-a-chunk-is-broken"><a href="#How-to-recover-when-a-chunk-is-broken" class="headerlink" title="How to recover when a chunk is broken?"></a>How to recover when a chunk is broken?</h4><p>讓Master 幫, Master知道他所有的小弟在哪，</p>
<ol>
<li><h5 id="架構-1"><a href="#架構-1" class="headerlink" title="架構"></a>架構</h5></li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcxr7eej31120nwdhw.jpg" alt="image-20200822213644383"></p>
<h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><h4 id="如何知道有個小弟完全掛了？-gt-心跳，小弟自報"><a href="#如何知道有個小弟完全掛了？-gt-心跳，小弟自報" class="headerlink" title="如何知道有個小弟完全掛了？==&gt; 心跳，小弟自報"></a>如何知道有個小弟完全掛了？==&gt; 心跳，小弟自報</h4><blockquote>
<h5 id="单选题-心跳（Heartbeat）机制怎么设计？"><a href="#单选题-心跳（Heartbeat）机制怎么设计？" class="headerlink" title="[单选题]心跳（Heartbeat）机制怎么设计？"></a>[单选题]心跳（Heartbeat）机制怎么设计？</h5><p>A.Master 轮询 Chunk Server13.62% 选择</p>
<p>B.Chunk Server 主动向 Master 汇报86.38% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>主动汇报可以减少通信次数，就一次，不然要兩次。</p>
</blockquote>
<h2 id="Scale-再更多-Q-amp-A"><a href="#Scale-再更多-Q-amp-A" class="headerlink" title="Scale 再更多 Q&amp;A"></a>Scale 再更多 Q&amp;A</h2><h4 id="Q-寫到一台server安全嗎？"><a href="#Q-寫到一台server安全嗎？" class="headerlink" title="Q: 寫到一台server安全嗎？"></a>Q: 寫到一台server安全嗎？</h4><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcx7copj31180i876i.jpg" alt="image-20200822214053264"></p>
<h4 id="Q-解決客戶端瓶頸，不然client分別去寫他太累了"><a href="#Q-解決客戶端瓶頸，不然client分別去寫他太累了" class="headerlink" title="Q: 解決客戶端瓶頸，不然client分別去寫他太累了"></a>Q: 解決客戶端瓶頸，不然client分別去寫他太累了</h4><h5 id="讓隊長去-寫怎麼寫？client-把chunk一次傳給三個小弟作replica後，client會變成瓶頸…"><a href="#讓隊長去-寫怎麼寫？client-把chunk一次傳給三個小弟作replica後，client會變成瓶頸…" class="headerlink" title="讓隊長去! 寫怎麼寫？client 把chunk一次傳給三個小弟作replica後，client會變成瓶頸…"></a>讓隊長去! 寫怎麼寫？client 把chunk一次傳給三個小弟作replica後，client會變成瓶頸…</h5><p>隊長再去寫給另兩個人</p>
<blockquote>
<h5 id="单选题-如何解决-client-传输-replica-chunk-的问题？"><a href="#单选题-如何解决-client-传输-replica-chunk-的问题？" class="headerlink" title="[单选题]如何解决 client 传输 replica chunk 的问题？"></a>[单选题]如何解决 client 传输 replica chunk 的问题？</h5><p>A.client 将 chunk 传给 master，由 master 去纷发到 3 台 chunk server6.24% 选择</p>
<p>B.client 将 chunk 传给其中一台 chunk server，然后由这台 chunk server 再传给另外的 2 台63.29% 选择</p>
<p>C.client 将 chunk 传给其中一台 chunk server，另外的两个 chunk server 空了再慢慢传30.47% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
</blockquote>
<p>內網自己傳肯定快得多</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrd04m7pj311a0isdhr.jpg" alt="image-20200822214408981"></p>
<h4 id="Q-怎麼選隊長？"><a href="#Q-怎麼選隊長？" class="headerlink" title="Q: 怎麼選隊長？"></a>Q: 怎麼選隊長？</h4><ol>
<li>可找最近的 (快)</li>
<li>找現在沒在幹活的 (平衡)</li>
</ol>
<blockquote>
<h5 id="多选题-下列哪些因素是我们挑选-chunk-server-队长时所需要考虑的"><a href="#多选题-下列哪些因素是我们挑选-chunk-server-队长时所需要考虑的" class="headerlink" title="[多选题]下列哪些因素是我们挑选 chunk server 队长时所需要考虑的"></a>[多选题]下列哪些因素是我们挑选 chunk server 队长时所需要考虑的</h5><p>A.机器的繁忙程度38.48% 选择</p>
<p>B.距离的远近36.78% 选择</p>
<p>C.剩余存储空间的大小13.27% 选择</p>
<p>D.CPU 的个数11.47% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是B</p>
<p><strong>正确答案:</strong>AB</p>
</blockquote>
<blockquote>
<h5 id="单选题-每次找的-chunk-server-队长是一样的么？"><a href="#单选题-每次找的-chunk-server-队长是一样的么？" class="headerlink" title="[单选题]每次找的 chunk server 队长是一样的么？"></a>[单选题]每次找的 chunk server 队长是一样的么？</h5><p>A.是1.55% 选择</p>
<p>B.不是20.48% 选择</p>
<p>C.不一定77.97% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是C</p>
<p><strong>正确答案:</strong>C</p>
</blockquote>
<h4 id="Q-解決-Chunk-Server-Failure"><a href="#Q-解決-Chunk-Server-Failure" class="headerlink" title="Q: 解決 Chunk Server Failure"></a>Q: 解決 Chunk Server Failure</h4><p>如果有人掛了，隊長知道了，它會跟master說不要再讓哪台能「被」分配東西了</p>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcybaduj313q0kmtal.jpg" alt="image-20200822214605591"></p>
<ul>
<li>如果沒寫上就一直試，如果太多次，就算了吧大家都完了，都掛了也不大可能</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrczjn6jj31320jy75c.jpg" alt="image-20200822214620405"></p>
<h2 id="總結-1"><a href="#總結-1" class="headerlink" title="總結"></a>總結</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glcrcz5brsj30yy0u0416.jpg" alt="image-20200822214743151"></p>
<h2 id="GFS-Problem"><a href="#GFS-Problem" class="headerlink" title="GFS Problem"></a>GFS Problem</h2><p><a href="https://www.jiuzhang.com/qa/627/" target="_blank" rel="noopener">https://www.jiuzhang.com/qa/627/</a></p>
<blockquote>
<ul>
<li><p><strong>设计一个只读的lookup service. 后台的数据是10 billion个key-value pair, 服务形式是接受用户输入的key，返回对应的value。</strong><br><strong>已知每个key的size是0.1kB，每个value的size是1kB。要求系统qps &gt;= 5000，latency &lt; 200ms.</strong></p>
<ul>
<li>key: 100; value: 1000個ascii字</li>
<li>server性能参数需要自己问，我当时只问了这些，可能有需要的但是没有问到的……<br>commodity server<br>8X CPU cores on each server<br>32G memory<br>6T disk</li>
</ul>
<p>使用任意数量的server，设计这个service。</p>
<p>就不发我的解法了，真的很渣……=。=|||</p>
<ul>
<li><p>我总结了SG_SWE_GM以及其他同学的解答，在此基础上我的想法如下，有问题的地方还请老师同学指正，</p>
<p>=&gt; total key size ~ 10 billion * 0.1kB = 1T        ==&gt; 40台*32G 查起來就是很快<br>=&gt; total value size ~ 10 billion * 1kB = 10T<br>所以每台服务器用两块硬盘，共12T。数据结构用SSTable就好了。</p>
<p>充分利用内存，本来我想用binary search tree做index，但是仔细想想这个服务是只读的，而且硬盘存储键值对用的是SSTable是有序的，key和value长度又是固定的，所以直接把key以有序的方式存在内存就好了，查询的时候对key进行binary search，然后用key在内存中的offset来计算键值对在硬盘中的offset。1T/32G = 31.25. 所以一共需要32台服务器的内存分担key index。前面加一个master负责管理consistent hasing。lg(32G) = 35, 平均查询一个key就算18次内存访问，大约才1800ns，在ms这个量级上可以忽略。</p>
<p>每一次request，在硬盘上读取1kB value的时间：<strong><em>10ms(disk seek)</em></strong> + 4ms(rotation delay for 7200rpm) + 1kB/1MB * <strong><em>30ms(reading 1kB sequentially from disk)</em></strong> = 14ms. 目前一台server能处理的的QPS: 1000ms/14ms = 71, 总的QPS: 71 * 32 = 2272。距离要求还有两倍多的差距。所以我们可以给每台server装上6个6T硬盘，组成3套数据硬盘，3套硬盘可以并行处理3个请求，这样也算是稍微利用了一下8X的多核CPU。这时QPS即为2272 * 3=6816.</p>
<p>延迟：</p>
<ol>
<li>master内存查找consistent hashing map的时间：忽略</li>
<li>master与slave的round trip delay：1 round trip in the same data center is 1ms.</li>
</ol>
</li>
</ul>
<ol start="3">
<li><p>slave内存index查询时间：忽略</p>
<ol start="4">
<li>slave硬盘读取时间：14ms</li>
</ol>
<p>so total latency is 15ms。</p>
</li>
</ol>
</li>
</ul>
</blockquote>
<ul>
<li>企業現在一般怎麼配server?<ul>
<li>16G、32G、64G、256G、1T、3T，一般不會直接用</li>
<li>Disk: </li>
</ul>
</li>
<li>300ms是在尋道</li>
<li>0.5 m 是round trip在同個data center間，比在硬盤上找還是快多了</li>
<li><strong><em>30ms(reading 1kB sequentially from disk)</em></strong><ul>
<li>是因為硬盤估計每秒可讀30MB的數據，所以1MB就是30ms；這個讀了１kB的話就0.03 ms跟disk seek比起來可以不計。</li>
</ul>
</li>
<li>QPS on 1 server : 1s/10ms 次（一秒一台可以100次。） * 2disk = 200次</li>
<li>5000個QPS/200就需要25台服務器</li>
</ul>
<h4 id="Latency"><a href="#Latency" class="headerlink" title="Latency:"></a>Latency:</h4><ol>
<li>找到key : 硬盤中作二分查找，每次的主要的是找的時間。</li>
<li>讀到value很小，每次的可以忽略</li>
</ol>
<ul>
<li>Sol1.</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a7sj9bjj20te0gajsq.jpg" alt="image-20200412220508731" style="zoom:50%;" />

<ul>
<li><input disabled="" type="checkbox"> 分析：log該以2為底，所以就變30 * 10ms(每動一次就要10ms) 就是 300ms, 超過題目規定的200ms，找到key的工作只能在硬盤上做，而且單個硬盤不能並行執行，所以一次query 至少要300ms了，一個硬盤１秒內只能做三次，兩個只能做六次，所以要5000QPS要至少約1000台服務器。這跟他給出的25台差很多！</li>
<li><input disabled="" type="checkbox"> 我們最希望減少的就是300ms的<strong><em>查詢時間</em></strong>；<strong><em>而我們未用上他的內存</em></strong>，一台有32G，<strong><em>40</em></strong>台就有超1TB，就跟所有的數據key量一樣了，所以提示了可以在內存作操作，把所有key都存過去。如果內存中有個內存到硬盤位置的映射的話…！</li>
<li><input disabled="" type="checkbox"> 一個key 0.1kB , 一個position 8Byte，所以一筆仍是0.1kB, 10個billion也是用1TB的內存。所以40台的內存並一起變一個大內存裡直接二分查找就快，每次的時間比硬盤的10ms少到幾乎可不計。</li>
<li><input disabled="" type="checkbox"> 內存去對應硬盤的position。</li>
<li><input disabled="" type="checkbox"> 硬盤二分查找每次要花10ms (disk seek)，但內存不用時間。雖一樣是花30次。所以300ms就省了。所以整體10ms + 0.5ms 就10.5ms。</li>
<li><input disabled="" type="checkbox"> 現在已有40台機器，每台有兩個硬盤，而且每台機器的硬盤可以存下全量的數據 ( 就key啦，就 1T而已)，就是說整體的數據一共有40份拷貝，每個硬盤要花約10ms查找一次，一台可以200次操作(一台有兩顆disk)，40台可以並行作，所以就是<strong>8000QPS &gt; 5000QPS!</strong></li>
<li><input disabled="" type="checkbox"> 總結：一共４０台機器，內存就是４０台合併起來當一大塊用，內存大小是１TB；它存放的是key到硬盤中position的數據。每台兩個硬盤，硬盤中存的是全量的數據，一次查找的過程就是，首先通過整體的內存找到在硬盤上的某個位置，均衡負載到40台的某個機器上，讀它的key, value，因為在內存查的時間可以不計，所以最後的延遲就在disk seek時間10ms還有整體在網路上傳輸的0.5ms。</li>
</ul>
<blockquote>
<ul>
<li>key 1T value 10T 机器6T 为啥一台机器的硬盘可以存下全部的信息？<ul>
<li>key 1T + value 10T = 11T<br>一块硬盘6T，一台机器两块硬盘即可存下</li>
</ul>
</li>
<li>为什么一个硬盘能存全量数据？一个硬盘是6TB，key加value需要11TB。如果是两块硬盘存下所有数据，那应该就是100次，为什么是100*2？<ul>
<li>是2块硬盘存下所有数据，每块存一半。100 * 2 的意思是，你有 2 块一瓶，一块硬盘能够提供 100 IOPS，两块就是 200 IOPS，因为你可以并发嘛。</li>
</ul>
</li>
<li>请问，内存是在不同的40台机器上，如何垮机器二分查找呢？机器间通信延迟怎么可以忽略呢？<ul>
<li>40台机器的内存合并起来看成一个大内存。打个比方，一次查找时可以先去编号20的机器上看，如果大了，再去10号机器上看，如果大了，再去5号机器上看。。。。依次类推。40台机器接在一个交换机（switch）上，通信延迟可以做到很低，如果再配合内核旁路，两台机器内存到内存的延迟最低可以做到接近纳秒级别</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="GFS-QA"><a href="#GFS-QA" class="headerlink" title="GFS QA"></a>GFS QA</h2><ul>
<li>硬盤的一個block默認就4KB, as an array in disk</li>
<li>Checksum ，也紀錄下和, 可用md5算hash<ul>
<li>可能會有False Positive, 比如 1,4 两个数据，check sum = 5，但是 2 + 3 也是 5。所以 check sum 相同不能证明数据一定没有发生变化。但是 check sum 不同就表示原始数据肯定发生了变化。因此 check sum 是存在 False Positive 但不存在 False Negative 的。</li>
</ul>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/11/20/BigData_java/2020-11-20%20Movie%20Recommendation%20System/">BigData_java/2020-11-20 Movie Recommendation System</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-11-20</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/BigData-java/">BigData_java</a></span><div class="content"><h1 id="Movie-Recommendation-System"><a href="#Movie-Recommendation-System" class="headerlink" title="Movie Recommendation System"></a>Movie Recommendation System</h1><ol>
<li>用戶沒有明確蒐索時的推薦<ul>
<li>點進Netflix瀏覽時</li>
<li>抖音 – 看你停留的時間、按讚</li>
</ul>
</li>
<li>用戶有明確蒐索目標時<ul>
<li>在 Google 索東西時<ul>
<li>當BigData時，相關放前面<ul>
<li>但不同人想的 BigData 可能不一樣</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="幾種設計方法"><a href="#幾種設計方法" class="headerlink" title="幾種設計方法"></a>幾種設計方法</h2><h3 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h3><p>底層原理之一，以更好地結合 Spark</p>
<h4 id="User-based"><a href="#User-based" class="headerlink" title="User-based"></a>User-based</h4><p>基於用戶的相似性來推薦物品</p>
<ul>
<li><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkwj4okczpj313609oabt.jpg" alt="image-20201121091913608" style="zoom:33%;" />



</li>
</ul>
<h4 id="Item-based"><a href="#Item-based" class="headerlink" title="Item-based"></a>Item-based</h4><ul>
<li><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a7dzn3zj20rs08emxi.jpg" alt="image-20201121092248091" style="zoom: 33%;" />
- User C 喜歡 Movie1, 所以想找像 Movie1 這樣子的好電影



</li>
</ul>
<h4 id="哪個好-gt-沒一定"><a href="#哪個好-gt-沒一定" class="headerlink" title="哪個好? ==&gt; 沒一定"></a>哪個好? ==&gt; 沒一定</h4><ul>
<li>不同的 use-case 沒一定</li>
</ul>
<h2 id="In-Proj-Item-Based-CF"><a href="#In-Proj-Item-Based-CF" class="headerlink" title="In Proj - Item-Based CF"></a>In Proj - Item-Based CF</h2><ol>
<li>Item 數量比 user少，計算量也會減少</li>
<li>Item 的改變更加低頻，計算量比夜少</li>
<li>更加具有說服力 (?!)<ul>
<li>Vs 新聞、理財產品</li>
</ul>
</li>
</ol>
<h3 id="Co-Occurence-Matrix"><a href="#Co-Occurence-Matrix" class="headerlink" title="Co Occurence Matrix"></a>Co Occurence Matrix</h3><ul>
<li>如果有愈多人同時rate過這兩部電影，我們認為這兩部的相關性愈大<ul>
<li>Why this Assumption?<ol>
<li>Rating 的前提是感興趣的</li>
<li>但如果IronMan + 貞子呢？<ul>
<li>當 pool變足夠大時，一個average的行為可信；相關的電影的相關性會愈來愈大</li>
<li>只會變 Noise</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h5 id="Self-join-on-userId"><a href="#Self-join-on-userId" class="headerlink" title="Self-~join on userId"></a>Self-~join on userId</h5><h3 id="Source"><a href="#Source" class="headerlink" title="Source:"></a>Source:</h3><p>on <a href="http://www.kaggle.com" target="_blank" rel="noopener">www.kaggle.com</a></p>
<ul>
<li>features: user_id, movie_id, rating</li>
<li>找到每個user所看的所有 movie的組合 (movie1, movie2, rating1, rating2)</li>
</ul>
<h2 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h2><h3 id="Co-Occurence-Matrix-1"><a href="#Co-Occurence-Matrix-1" class="headerlink" title="Co Occurence Matrix"></a>Co Occurence Matrix</h3><h4 id="New-DF"><a href="#New-DF" class="headerlink" title="New DF"></a>New DF</h4><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl4n71au7ej30hs070gnr.jpg" alt="image-20201128094423479" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkwkyxqkckj30u006mtby.jpg" alt="image-20201121102253957" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkwl1yvezoj30f608q0un.jpg" alt="image-20201121102549289" style="zoom: 50%;" />



<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl4mp7r3djj310s0jon7n.jpg" alt="image-20201128092712951" style="zoom:50%;" />



<h5 id="DeDup-by-setting-Rule"><a href="#DeDup-by-setting-Rule" class="headerlink" title="DeDup by setting Rule"></a>DeDup by setting Rule</h5><ul>
<li>Rule : movie1 &lt; movie2</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gkwm12852bj312k0g4wjt.jpg" alt="image-20201121105932505" style="zoom:50%;" />





<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl4nne9fx1j30y80dydka.jpg" alt="image-20201128100007159" style="zoom:50%;" />



<h5 id="Coalesce"><a href="#Coalesce" class="headerlink" title="Coalesce"></a>Coalesce</h5><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl4peg4qz5j31fk0gsncz.jpg" alt="image-20201128110042938" style="zoom:50%;" />



<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl4phg8zzfj31ei0o6wth.jpg" alt="image-20201128110336591"></p>
<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gl4pj34pedj30qm0hgtgz.jpg" alt="image-20201128110511202" style="zoom:50%;" />

</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/78/">78</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2023 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>