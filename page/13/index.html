<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-180692466-1', 'auto');
ga('send', 'pageview');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">386</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">66</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Awaken Desparado</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2020/04/09/jupyter-demo/2020-04-09-SNPS%20stock%20prediction/">jupyter-demo/2020-04-09-SNPS stock prediction</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-09</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Jupy-Demo/">Jupy-Demo</a></span><div class="content"><iframe
src="https://nbviewer.jupyter.org/github/Joecth/Joecth.github.io/blob/hexo-melody/source/_posts/jupyter-demo/stock_pred_w_RNN/rnn_stock.ipynb" width="100%" height="1000">
</iframe>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/28/NLP/2020-03-28-NLP_nltk_0/">NLP/2020-03-28-NLP_nltk_0</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-28</time><div class="content"><h3 id="Text-Preprocessing"><a href="#Text-Preprocessing" class="headerlink" title="Text Preprocessing"></a>Text Preprocessing</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># regex for removing punctuation!</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># nltk preprocessing magic</span></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="comment"># grabbing a part of speech function:</span></span><br><span class="line"><span class="keyword">from</span> part_of_speech <span class="keyword">import</span> get_part_of_speech</span><br><span class="line"></span><br><span class="line">text = <span class="string">"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed."</span></span><br><span class="line"></span><br><span class="line">cleaned = re.sub(<span class="string">'\W+'</span>, <span class="string">' '</span>, text)</span><br><span class="line">tokenized = word_tokenize(cleaned)</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">stemmed = [stemmer.stem(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokenized]</span><br><span class="line"></span><br><span class="line"><span class="comment">## -- CHANGE these -- ##</span></span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line">lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) <span class="keyword">for</span> token <span class="keyword">in</span> tokenized]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Stemmed text:"</span>)</span><br><span class="line">print(stemmed)</span><br><span class="line">print(<span class="string">"\nLemmatized text:"</span>)</span><br><span class="line">print(lemmatized)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Stemmed text:</span><br><span class="line">[<span class="string">'So'</span>, <span class="string">'mani'</span>, <span class="string">'squid'</span>, <span class="string">'are'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'suitcas'</span>, <span class="string">'these'</span>, <span class="string">'day'</span>, <span class="string">'that'</span>, <span class="string">'you'</span>, <span class="string">'can'</span>, <span class="string">'bare'</span>, <span class="string">'go'</span>, <span class="string">'anywher'</span>, <span class="string">'without'</span>, <span class="string">'see'</span>, <span class="string">'one'</span>, <span class="string">'burst'</span>, <span class="string">'forth'</span>, <span class="string">'from'</span>, <span class="string">'a'</span>, <span class="string">'tightli'</span>, <span class="string">'pack'</span>, <span class="string">'valis'</span>, <span class="string">'I'</span>, <span class="string">'went'</span>, <span class="string">'to'</span>, <span class="string">'the'</span>, <span class="string">'dentist'</span>, <span class="string">'the'</span>, <span class="string">'other'</span>, <span class="string">'day'</span>, <span class="string">'and'</span>, <span class="string">'sure'</span>, <span class="string">'enough'</span>, <span class="string">'I'</span>, <span class="string">'saw'</span>, <span class="string">'an'</span>, <span class="string">'angri'</span>, <span class="string">'one'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'my'</span>, <span class="string">'dentist'</span>, <span class="string">'s'</span>, <span class="string">'bag'</span>, <span class="string">'within'</span>, <span class="string">'minut'</span>, <span class="string">'of'</span>, <span class="string">'arriv'</span>, <span class="string">'she'</span>, <span class="string">'hardli'</span>, <span class="string">'even'</span>, <span class="string">'notic'</span>]</span><br><span class="line"></span><br><span class="line">Lemmatized text:</span><br><span class="line">[<span class="string">'So'</span>, <span class="string">'many'</span>, <span class="string">'squid'</span>, <span class="string">'be'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'suitcase'</span>, <span class="string">'these'</span>, <span class="string">'day'</span>, <span class="string">'that'</span>, <span class="string">'you'</span>, <span class="string">'can'</span>, <span class="string">'barely'</span>, <span class="string">'go'</span>, <span class="string">'anywhere'</span>, <span class="string">'without'</span>, <span class="string">'see'</span>, <span class="string">'one'</span>, <span class="string">'burst'</span>, <span class="string">'forth'</span>, <span class="string">'from'</span>, <span class="string">'a'</span>, <span class="string">'tightly'</span>, <span class="string">'pack'</span>, <span class="string">'valise'</span>, <span class="string">'I'</span>, <span class="string">'go'</span>, <span class="string">'to'</span>, <span class="string">'the'</span>, <span class="string">'dentist'</span>, <span class="string">'the'</span>, <span class="string">'other'</span>, <span class="string">'day'</span>, <span class="string">'and'</span>, <span class="string">'sure'</span>, <span class="string">'enough'</span>, <span class="string">'I'</span>, <span class="string">'saw'</span>, <span class="string">'an'</span>, <span class="string">'angry'</span>, <span class="string">'one'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'my'</span>, <span class="string">'dentist'</span>, <span class="string">'s'</span>, <span class="string">'bag'</span>, <span class="string">'within'</span>, <span class="string">'minute'</span>, <span class="string">'of'</span>, <span class="string">'arrive'</span>, <span class="string">'She'</span>, <span class="string">'hardly'</span>, <span class="string">'even'</span>, <span class="string">'notice'</span>]</span><br></pre></td></tr></table></figure>



<h3 id="Parsing-Text"><a href="#Parsing-Text" class="headerlink" title="Parsing Text"></a>Parsing Text</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> Tree</span><br><span class="line"><span class="keyword">from</span> squids <span class="keyword">import</span> squids_text</span><br><span class="line"></span><br><span class="line">dependency_parser = spacy.load(<span class="string">'en'</span>)</span><br><span class="line"></span><br><span class="line">parsed_squids = dependency_parser(squids_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assign my_sentence a new value:</span></span><br><span class="line">my_sentence = <span class="string">"Your sentence goes here!"</span></span><br><span class="line">my_parsed_sentence = dependency_parser(my_sentence)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_nltk_tree</span><span class="params">(node)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> node.n_lefts + node.n_rights &gt; <span class="number">0</span>:</span><br><span class="line">    parsed_child_nodes = [to_nltk_tree(child) <span class="keyword">for</span> child <span class="keyword">in</span> node.children]</span><br><span class="line">    <span class="keyword">return</span> Tree(node.orth_, parsed_child_nodes)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> node.orth_</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> parsed_squids.sents:</span><br><span class="line">  to_nltk_tree(sent.root).pretty_print()</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> my_parsed_sentence.sents:</span><br><span class="line"> to_nltk_tree(sent.root).pretty_print()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">        jumping                       </span><br><span class="line">  _________|_______________________    </span><br><span class="line"> |   |     |      |       out      |  </span><br><span class="line"> |   |     |      |        |       |   </span><br><span class="line"> |   |     |    squids     of     days</span><br><span class="line"> |   |     |      |        |       |   </span><br><span class="line"> So are    .     many  suitcases these</span><br><span class="line"></span><br><span class="line">          go                       </span><br><span class="line">  ________|____________________     </span><br><span class="line"> |   |    |       |      |  without</span><br><span class="line"> |   |    |       |      |     |    </span><br><span class="line"> |   |    |       |      |   seeing</span><br><span class="line"> |   |    |       |      |     |    </span><br><span class="line">You can barely anywhere  .    one  </span><br><span class="line"></span><br><span class="line">          went               </span><br><span class="line">  _________|_________         </span><br><span class="line"> |   |     to        |       </span><br><span class="line"> |   |     |         |        </span><br><span class="line"> |   |  dentist     day      </span><br><span class="line"> |   |     |      ___|____    </span><br><span class="line"> I   .    the   the     other</span><br><span class="line"></span><br><span class="line">             saw                                     </span><br><span class="line">  ____________|___________________                    </span><br><span class="line"> |   |   |    |                  jump                </span><br><span class="line"> |   |   |    |          _________|__________         </span><br><span class="line"> |   |   |    |         |                   out      </span><br><span class="line"> |   |   |    |         |                    |        </span><br><span class="line"> |   |   |    |         |                    of      </span><br><span class="line"> |   |   |    |         |                    |        </span><br><span class="line"> |   |   |    |         |                   bag      </span><br><span class="line"> |   |   |    |         |                    |        </span><br><span class="line"> |   |   |  enough     one                dentist    </span><br><span class="line"> |   |   |    |      ___|____           _____|_____   </span><br><span class="line"> ,   I   .   Sure   an     angry       my          <span class="string">'s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    noticed         </span></span><br><span class="line"><span class="string">  _____|__________   </span></span><br><span class="line"><span class="string">She  hardly even  . </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     goes         </span></span><br><span class="line"><span class="string">  ____|______      </span></span><br><span class="line"><span class="string"> |    |   sentence</span></span><br><span class="line"><span class="string"> |    |      |     </span></span><br><span class="line"><span class="string">here  !     Your</span></span><br></pre></td></tr></table></figure>



<h3 id="Language-Models-Bag-of-Words-Approach"><a href="#Language-Models-Bag-of-Words-Approach" class="headerlink" title="Language Models - Bag-of-Words Approach"></a>Language Models - Bag-of-Words Approach</h3><p>When grammar and word order are irrelevant, this is probably a good model to use.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># importing regex and nltk</span></span><br><span class="line"><span class="keyword">import</span> re, nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="comment"># importing Counter to get word counts for bag of words</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="comment"># importing a passage from Through the Looking Glass</span></span><br><span class="line"><span class="keyword">from</span> looking_glass <span class="keyword">import</span> looking_glass_text</span><br><span class="line"><span class="comment"># importing part-of-speech function for lemmatization</span></span><br><span class="line"><span class="keyword">from</span> part_of_speech <span class="keyword">import</span> get_part_of_speech</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change text to another string:</span></span><br><span class="line"><span class="comment"># text = looking_glass_text</span></span><br><span class="line">text = <span class="string">"hello world i miss you"</span></span><br><span class="line"></span><br><span class="line">cleaned = re.sub(<span class="string">'\W+'</span>, <span class="string">' '</span>, text).lower()</span><br><span class="line">tokenized = word_tokenize(cleaned)</span><br><span class="line"></span><br><span class="line">stop_words = stopwords.words(<span class="string">'english'</span>)</span><br><span class="line">filtered = [word <span class="keyword">for</span> word <span class="keyword">in</span> tokenized <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"></span><br><span class="line">normalizer = WordNetLemmatizer()</span><br><span class="line">normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) <span class="keyword">for</span> token <span class="keyword">in</span> filtered]</span><br><span class="line"><span class="comment"># Comment out the print statement below</span></span><br><span class="line">print(normalized)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define bag_of_looking_glass_words &amp; print:</span></span><br><span class="line">bag_of_looking_glass_words = Counter(normalized)</span><br><span class="line">print(bag_of_looking_glass_words)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'hello'</span>, <span class="string">'world'</span>, <span class="string">'miss'</span>]</span><br><span class="line">Counter(&#123;<span class="string">'hello'</span>: 1, <span class="string">'world'</span>: 1, <span class="string">'miss'</span>: 1&#125;)</span><br></pre></td></tr></table></figure>



<h3 id="Language-Models-N-Grams-and-NLM"><a href="#Language-Models-N-Grams-and-NLM" class="headerlink" title="Language Models - N-Grams and NLM"></a>Language Models - N-Grams and NLM</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk, re</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="comment"># importing ngrams module from nltk</span></span><br><span class="line"><span class="keyword">from</span> nltk.util <span class="keyword">import</span> ngrams</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> looking_glass <span class="keyword">import</span> looking_glass_full_text</span><br><span class="line"></span><br><span class="line">cleaned = re.sub(<span class="string">'\W+'</span>, <span class="string">' '</span>, looking_glass_full_text).lower()</span><br><span class="line">tokenized = word_tokenize(cleaned)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change the n value to 2:</span></span><br><span class="line">looking_glass_bigrams = ngrams(tokenized, <span class="number">2</span>)</span><br><span class="line">looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change the n value to 3:</span></span><br><span class="line">looking_glass_trigrams = ngrams(tokenized, <span class="number">3</span>)</span><br><span class="line">looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change the n value to a number greater than 3:</span></span><br><span class="line">looking_glass_ngrams = ngrams(tokenized, <span class="number">4</span>)</span><br><span class="line">looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Looking Glass Bigrams:"</span>)</span><br><span class="line">print(looking_glass_bigrams_frequency.most_common(<span class="number">10</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nLooking Glass Trigrams:"</span>)</span><br><span class="line">print(looking_glass_trigrams_frequency.most_common(<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">print(<span class="string">"\nLooking Glass n-grams:"</span>)</span><br><span class="line">print(looking_glass_ngrams_frequency.most_common(<span class="number">10</span>), <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Looking Glass Bigrams:</span><br><span class="line">[((<span class="string">'of'</span>, <span class="string">'the'</span>), 101), ((<span class="string">'said'</span>, <span class="string">'the'</span>), 98), ((<span class="string">'in'</span>, <span class="string">'a'</span>), 97), ((<span class="string">'in'</span>, <span class="string">'the'</span>), 90), ((<span class="string">'as'</span>, <span class="string">'she'</span>), 82), ((<span class="string">'you'</span>, <span class="string">'know'</span>), 72), ((<span class="string">'a'</span>, <span class="string">'little'</span>), 68), ((<span class="string">'the'</span>, <span class="string">'queen'</span>), 67), ((<span class="string">'said'</span>, <span class="string">'alice'</span>), 67), ((<span class="string">'to'</span>, <span class="string">'the'</span>), 66)] 2</span><br><span class="line"></span><br><span class="line">Looking Glass Trigrams:</span><br><span class="line">[((<span class="string">'the'</span>, <span class="string">'red'</span>, <span class="string">'queen'</span>), 54), ((<span class="string">'the'</span>, <span class="string">'white'</span>, <span class="string">'queen'</span>), 31), ((<span class="string">'said'</span>, <span class="string">'in'</span>, <span class="string">'a'</span>), 21), ((<span class="string">'she'</span>, <span class="string">'went'</span>, <span class="string">'on'</span>), 18), ((<span class="string">'said'</span>, <span class="string">'the'</span>, <span class="string">'red'</span>), 17), ((<span class="string">'thought'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 16), ((<span class="string">'the'</span>, <span class="string">'queen'</span>, <span class="string">'said'</span>), 16), ((<span class="string">'said'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 14), ((<span class="string">'said'</span>, <span class="string">'humpty'</span>, <span class="string">'dumpty'</span>), 14), ((<span class="string">'the'</span>, <span class="string">'knight'</span>, <span class="string">'said'</span>), 14)] 3</span><br><span class="line"></span><br><span class="line">Looking Glass n-grams:</span><br><span class="line">[((<span class="string">'said'</span>, <span class="string">'the'</span>, <span class="string">'red'</span>, <span class="string">'queen'</span>), 15), ((<span class="string">'she'</span>, <span class="string">'said'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 11), ((<span class="string">'alice'</span>, <span class="string">'thought'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 9), ((<span class="string">'to'</span>, <span class="string">'herself'</span>, <span class="string">'as'</span>, <span class="string">'she'</span>), 9), ((<span class="string">'one'</span>, <span class="string">'and'</span>, <span class="string">'one'</span>, <span class="string">'and'</span>), 8), ((<span class="string">'and'</span>, <span class="string">'one'</span>, <span class="string">'and'</span>, <span class="string">'one'</span>), 8), ((<span class="string">'alice'</span>, <span class="string">'said'</span>, <span class="string">'in'</span>, <span class="string">'a'</span>), 6), ((<span class="string">'for'</span>, <span class="string">'a'</span>, <span class="string">'minute'</span>, <span class="string">'or'</span>), 6), ((<span class="string">'a'</span>, <span class="string">'minute'</span>, <span class="string">'or'</span>, <span class="string">'two'</span>), 6), ((<span class="string">'in'</span>, <span class="string">'a'</span>, <span class="string">'tone'</span>, <span class="string">'of'</span>), 6)] 5</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk, re</span><br><span class="line"><span class="keyword">from</span> sherlock_holmes <span class="keyword">import</span> bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3</span><br><span class="line"><span class="keyword">from</span> preprocessing <span class="keyword">import</span> preprocess_text</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> LatentDirichletAllocation</span><br><span class="line"></span><br><span class="line"><span class="comment"># preparing the text</span></span><br><span class="line">corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]</span><br><span class="line">preprocessed_corpus = [preprocess_text(chapter) <span class="keyword">for</span> chapter <span class="keyword">in</span> corpus]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update stop_list:</span></span><br><span class="line">stop_list = []</span><br><span class="line"><span class="comment"># filtering topics for stop words</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_out_stop_words</span><span class="params">(corpus)</span>:</span></span><br><span class="line">  no_stops_corpus = []</span><br><span class="line">  <span class="keyword">for</span> chapter <span class="keyword">in</span> corpus:</span><br><span class="line">    no_stops_chapter = <span class="string">" "</span>.join([word <span class="keyword">for</span> word <span class="keyword">in</span> chapter.split(<span class="string">" "</span>) <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_list])</span><br><span class="line">    no_stops_corpus.append(no_stops_chapter)</span><br><span class="line">  <span class="keyword">return</span> no_stops_corpus</span><br><span class="line">filtered_for_stops = filter_out_stop_words(preprocessed_corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the bag of words model</span></span><br><span class="line">bag_of_words_creator = CountVectorizer()</span><br><span class="line">bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the tf-idf model</span></span><br><span class="line">tfidf_creator = TfidfVectorizer(min_df = <span class="number">0.2</span>)</span><br><span class="line">tfidf = tfidf_creator.fit_transform(preprocessed_corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the bag of words LDA model</span></span><br><span class="line">lda_bag_of_words_creator = LatentDirichletAllocation(learning_method=<span class="string">'online'</span>, n_components=<span class="number">10</span>)</span><br><span class="line">lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the tf-idf LDA model</span></span><br><span class="line">lda_tfidf_creator = LatentDirichletAllocation(learning_method=<span class="string">'online'</span>, n_components=<span class="number">10</span>)</span><br><span class="line">lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"~~~ Topics found by bag of words LDA ~~~"</span>)</span><br><span class="line"><span class="keyword">for</span> topic_id, topic <span class="keyword">in</span> enumerate(lda_bag_of_words_creator.components_):</span><br><span class="line">  message = <span class="string">"Topic #&#123;&#125;: "</span>.format(topic_id + <span class="number">1</span>)</span><br><span class="line">  message += <span class="string">" "</span>.join([bag_of_words_creator.get_feature_names()[i] <span class="keyword">for</span> i <span class="keyword">in</span> topic.argsort()[:<span class="number">-5</span> :<span class="number">-1</span>]])</span><br><span class="line">  print(message)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\n\n~~~ Topics found by tf-idf LDA ~~~"</span>)</span><br><span class="line"><span class="keyword">for</span> topic_id, topic <span class="keyword">in</span> enumerate(lda_tfidf_creator.components_):</span><br><span class="line">  message = <span class="string">"Topic #&#123;&#125;: "</span>.format(topic_id + <span class="number">1</span>)</span><br><span class="line">  message += <span class="string">" "</span>.join([tfidf_creator.get_feature_names()[i] <span class="keyword">for</span> i <span class="keyword">in</span> topic.argsort()[:<span class="number">-5</span> :<span class="number">-1</span>]])</span><br><span class="line">  print(message)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">~~~ Topics found by bag of words LDA ~~~</span><br><span class="line">Topic <span class="comment">#1: holmes say little upon</span></span><br><span class="line">Topic <span class="comment">#2: house come could man</span></span><br><span class="line">Topic <span class="comment">#3: holmes say know come</span></span><br><span class="line">Topic <span class="comment">#4: holmes would say know</span></span><br><span class="line">Topic <span class="comment">#5: say holmes know see</span></span><br><span class="line">Topic <span class="comment">#6: say holmes man could</span></span><br><span class="line">Topic <span class="comment">#7: say upon mccarthy man</span></span><br><span class="line">Topic <span class="comment">#8: make holmes cry majesty</span></span><br><span class="line">Topic <span class="comment">#9: holmes say man upon</span></span><br><span class="line">Topic <span class="comment">#10: upon holmes see say</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~ Topics found by tf-idf LDA ~~~</span><br><span class="line">Topic <span class="comment">#1: merely upon boot catch</span></span><br><span class="line">Topic <span class="comment">#2: boot save holmes mccarthy</span></span><br><span class="line">Topic <span class="comment">#3: norton resolve help refer</span></span><br><span class="line">Topic <span class="comment">#4: leave remove three lodge</span></span><br><span class="line">Topic <span class="comment">#5: say neck resolute stone</span></span><br><span class="line">Topic <span class="comment">#6: holmes king majesty photograph</span></span><br><span class="line">Topic <span class="comment">#7: fear together heavy upon</span></span><br><span class="line">Topic <span class="comment">#8: holmes say know man</span></span><br><span class="line">Topic <span class="comment">#9: figure surround definite heel</span></span><br><span class="line">Topic <span class="comment">#10: know many swiftly scotland</span></span><br></pre></td></tr></table></figure>



<h3 id="Text-Similarity"><a href="#Text-Similarity" class="headerlink" title="Text Similarity"></a>Text Similarity</h3><p>Most of us have a good autocorrect story. Our phone’s messenger quietly swaps one letter for another as we type and suddenly the meaning of our message has changed (to our horror or pleasure). However, addressing <strong><em>text similarity\</em></strong> — including spelling correction — is a major challenge within natural language processing.</p>
<p>Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering the <strong><em>Levenshtein distance\</em></strong> or minimal edit distance between two words. The distance is calculated through the minimum number of insertions, deletions, and substitutions that would need to occur for one word to become another. For example, turning “bees” into “beans” would require one substitution (“a” for “e”) and one insertion (“n”), so the Levenshtein distance would be two.</p>
<p>Phonetic similarity is also a major challenge within speech recognition. English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia,” but it’s a far more challenging task for a machine! More advanced autocorrect and spelling correction technology additionally considers key distance on a keyboard and <strong><em>phonetic similarity\</em></strong> (how much two words or phrases sound the same).</p>
<p>It’s also helpful to find out if texts are the same to guard against plagiarism, which we can identify through <strong><em>lexical similarity\</em></strong> (the degree to which texts use the same vocabulary and phrases). Meanwhile, <strong><em>semantic similarity\</em></strong> (the degree to which documents contain similar meaning or topics) is useful when you want to find (or recommend) an article or book similar to one you recently finished.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="comment"># NLTK has a built-in function</span></span><br><span class="line"><span class="comment"># to check Levenshtein distance:</span></span><br><span class="line"><span class="keyword">from</span> nltk.metrics <span class="keyword">import</span> edit_distance</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_levenshtein</span><span class="params">(string1, string2)</span>:</span></span><br><span class="line">  print(<span class="string">"The Levenshtein distance from '&#123;0&#125;' to '&#123;1&#125;' is &#123;2&#125;!"</span>.format(string1, string2, edit_distance(string1, string2)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the distance between</span></span><br><span class="line"><span class="comment"># any two words here!</span></span><br><span class="line">print_levenshtein(<span class="string">"fart"</span>, <span class="string">"target"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assign passing strings here:</span></span><br><span class="line">three_away_from_code = <span class="string">"cat"</span></span><br><span class="line"></span><br><span class="line">two_away_from_chunk = <span class="string">"cheek"</span></span><br><span class="line"></span><br><span class="line">print_levenshtein(<span class="string">"code"</span>, three_away_from_code)</span><br><span class="line">print_levenshtein(<span class="string">"chunk"</span>, two_away_from_chunk)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The Levenshtein distance from <span class="string">'fart'</span> to <span class="string">'target'</span> is 3!</span><br><span class="line">The Levenshtein distance from <span class="string">'code'</span> to <span class="string">'cat'</span> is 3!</span><br><span class="line">The Levenshtein distance from <span class="string">'chunk'</span> to <span class="string">'cheek'</span> is 2!</span><br></pre></td></tr></table></figure>



<h3 id="Language-Prediction-amp-Text-Generation"><a href="#Language-Prediction-amp-Text-Generation" class="headerlink" title="Language Prediction &amp; Text Generation"></a>Language Prediction &amp; Text Generation</h3><p>How does your favorite search engine complete your search queries? How does your phone’s keyboard know what you want to type next? <strong><em>Language prediction\</em></strong> is an application of NLP concerned with predicting text given preceding text. Autosuggest, autocomplete, and suggested replies are common forms of language prediction.</p>
<p>Your first step to language prediction is picking a language model. Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.</p>
<p>If you go the <em>n</em>-gram route, you will most likely rely on <strong><em>Markov chains\</em></strong> to predict the statistical likelihood of each following word (or character) based on the training corpus. Markov chains are memory-less and make statistical predictions based entirely on the current <em>n</em>-gram on hand.</p>
<p>For example, let’s take a sentence beginning, “I ate so many grilled cheese”. Using a trigram model (where <em>n</em> is 3), a Markov chain would predict the following word as “sandwiches” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data.</p>
<p>A more advanced approach, using a neural language model, is the <strong><em>Long Short Term Memory (LSTM)\</em></strong> model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them better suited for text prediction than traditional neural networks.</p>
<p><strong>1.</strong></p>
<p>Add three short stories by your favorite author or the lyrics to three songs by your favorite artist to <strong>document1.py</strong>, <strong>document2.py</strong>, and <strong>document3.py</strong>. Then run <strong>script.py</strong> to see a short example of text prediction.</p>
<p>Does it look like something by your favorite author or artist?</p>
<p>If you accidentally close one of the files, just click the file folder in the top left corner of the code editor to find the file and re-open it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk, re, random</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, deque</span><br><span class="line"><span class="keyword">from</span> document1 <span class="keyword">import</span> training_doc1</span><br><span class="line"><span class="keyword">from</span> document2 <span class="keyword">import</span> training_doc2</span><br><span class="line"><span class="keyword">from</span> document3 <span class="keyword">import</span> training_doc3</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarkovChain</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.lookup_dict = defaultdict(list)</span><br><span class="line">    self._seeded = <span class="literal">False</span></span><br><span class="line">    self.__seed_me()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__seed_me</span><span class="params">(self, rand_seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self._seeded <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">True</span>:</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> rand_seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">          random.seed(rand_seed)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          random.seed()</span><br><span class="line">        self._seeded = <span class="literal">True</span></span><br><span class="line">      <span class="keyword">except</span> NotImplementedError:</span><br><span class="line">        self._seeded = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add_document</span><span class="params">(self, str)</span>:</span></span><br><span class="line">    preprocessed_list = self._preprocess(str)</span><br><span class="line">    pairs = self.__generate_tuple_keys(preprocessed_list)</span><br><span class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</span><br><span class="line">      self.lookup_dict[pair[<span class="number">0</span>]].append(pair[<span class="number">1</span>])</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_preprocess</span><span class="params">(self, str)</span>:</span></span><br><span class="line">    cleaned = re.sub(<span class="string">r'\W+'</span>, <span class="string">' '</span>, str).lower()</span><br><span class="line">    tokenized = word_tokenize(cleaned)</span><br><span class="line">    <span class="keyword">return</span> tokenized</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__generate_tuple_keys</span><span class="params">(self, data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(data) &lt; <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data) - <span class="number">1</span>):</span><br><span class="line">      <span class="keyword">yield</span> [ data[i], data[i + <span class="number">1</span>] ]</span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">generate_text</span><span class="params">(self, max_length=<span class="number">50</span>)</span>:</span></span><br><span class="line">    context = deque()</span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">if</span> len(self.lookup_dict) &gt; <span class="number">0</span>:</span><br><span class="line">      self.__seed_me(rand_seed=len(self.lookup_dict))</span><br><span class="line">      chain_head = [list(self.lookup_dict)[<span class="number">0</span>]]</span><br><span class="line">      context.extend(chain_head)</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">while</span> len(output) &lt; (max_length - <span class="number">1</span>):</span><br><span class="line">        next_choices = self.lookup_dict[context[<span class="number">-1</span>]]</span><br><span class="line">        <span class="keyword">if</span> len(next_choices) &gt; <span class="number">0</span>:</span><br><span class="line">          next_word = random.choice(next_choices)</span><br><span class="line">          context.append(next_word)</span><br><span class="line">          output.append(context.popleft())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">      output.extend(list(context))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">" "</span>.join(output)</span><br><span class="line"></span><br><span class="line">my_markov = MarkovChain()</span><br><span class="line">my_markov.add_document(training_doc1)</span><br><span class="line">my_markov.add_document(training_doc2)</span><br><span class="line">my_markov.add_document(training_doc3)</span><br><span class="line">generated_text = my_markov.generate_text()</span><br><span class="line">print(generated_text)</span><br></pre></td></tr></table></figure>

</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/28/Web/2020-03-28-Full%20Stack%20review/">Web/2020-03-28-Full Stack review</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-28</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Web/">Web</a></span><div class="content"><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd8e8h8nvzj317g0g6n9s.jpg" alt="image-20200327125348804" style="zoom:67%;" /></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/18/Books/2020-03-23-Lifes%20in%20English/">Books/2020-03-23-Lifes in English</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-18</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/English-Life/">English &amp; Life</a></span><div class="content"><blockquote>
<p>Ross: Probably. But you know, I’ll tell you something. Passion is way overrated.</p>
<p>Rachel: Yeah right.</p>
<p>Ross: It is… eventually… kind of… <strong>burns out</strong>. But hopefully, what you’re left with is trust, and security, and… well, in the case of my ex-wife, lesbianism. So, you know, for all of those people who <strong>miss out</strong> on that passion… thing, there’s all that other good stuff.</p>
</blockquote>
<p>Ref: <a href="http://www.tingroom.com/print_35336.html" target="_blank" rel="noopener">http://www.tingroom.com/print_35336.html</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2020/03/15/System-Design/9p/2020-03-15-System%20Design%20-%20Seckill%20&amp;%20Booling%20System/">System-Design/9p/2020-03-15-System Design - Seckill &amp; Booling System</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/System/">System</a></span><div class="content"><h1 id="秒殺"><a href="#秒殺" class="headerlink" title="秒殺"></a>秒殺</h1><ul>
<li>0點開始</li>
<li>限量100台</li>
<li>一人限購一台</li>
<li>如：搶紅包、搶火車票、搶手機</li>
</ul>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200824202003669.png" alt="image-20200824202003669" style="zoom: 50%;" />



<h3 id="QPS分析"><a href="#QPS分析" class="headerlink" title="QPS分析"></a>QPS分析</h3><ul>
<li>平日每秒1000人訪問這頁面。秒殺時每秒數十萬人訪問該頁面，QPS增加100倍以上。</li>
</ul>
<h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><h3 id="商品購買和下單流程"><a href="#商品購買和下單流程" class="headerlink" title="商品購買和下單流程"></a>商品購買和下單流程</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi26516fedj30xi0g6gp6.jpg" alt="image-20200824202207414"></p>
<ul>
<li>付完錢才是完成，如果沒有按時付錢，庫存是會被放開的</li>
</ul>
<h3 id="需要解決的問題們"><a href="#需要解決的問題們" class="headerlink" title="需要解決的問題們"></a>需要解決的問題們</h3><h4 id="瞬間大流量高併發"><a href="#瞬間大流量高併發" class="headerlink" title="瞬間大流量高併發"></a>瞬間大流量高併發</h4><p>server、db能承載的QPS有限，如db一般是單機 1000 QPS，需要根據業務預估併發量</p>
<h4 id="有限庫存，不能超賣"><a href="#有限庫存，不能超賣" class="headerlink" title="有限庫存，不能超賣"></a>有限庫存，不能超賣</h4><p>庫存是有限的，需要精準地保證，就是賣掉了N個商品。不能超賣，當然也不能少賣</p>
<h4 id="黃牛惡意請求"><a href="#黃牛惡意請求" class="headerlink" title="黃牛惡意請求"></a>黃牛惡意請求</h4><p>使用腳本模擬用戶購買，模擬出10幾萬個請求去搶購</p>
<h4 id="固定時間開啟"><a href="#固定時間開啟" class="headerlink" title="固定時間開啟"></a>固定時間開啟</h4><p>時間到了才能購買，提前一秒都不可以 (以商家的時間為準)</p>
<h4 id="嚴格限購"><a href="#嚴格限購" class="headerlink" title="嚴格限購"></a>嚴格限購</h4><p>一個商家只能買固定數量個</p>
<h3 id="需求拆解"><a href="#需求拆解" class="headerlink" title="需求拆解"></a>需求拆解</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi26c7mxc1j30sq0ck40q.jpg" alt="image-20200824202903828"></p>
<h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><h3 id="服務結構設計"><a href="#服務結構設計" class="headerlink" title="服務結構設計"></a>服務結構設計</h3><h4 id="單體"><a href="#單體" class="headerlink" title="單體"></a>單體</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi26fynt3uj30zo0gk11x.jpg" alt="image-20200824203239695" style="zoom:67%;" />



<h4 id="微服務"><a href="#微服務" class="headerlink" title="微服務"></a>微服務</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi26i5p40mj30zs0e211l.jpg" alt="image-20200824203446425" style="zoom:67%;" />



<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><h3 id="寫"><a href="#寫" class="headerlink" title="寫"></a>寫</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi26m08ptzj30y00e8wil.jpg" alt="image-20200824203827977"></p>
<h4 id="Q-如何添加索引"><a href="#Q-如何添加索引" class="headerlink" title="Q: 如何添加索引?"></a>Q: 如何添加索引?</h4><ul>
<li><p>商品表: 沒有關聯其他表，所以就主鍵就ok了</p>
</li>
<li><p>秒殺表: 綁定了商品，所以 commodity_id 可以作索引查起來快</p>
</li>
<li><p>庫存表: 綁了商品、以及活動 id，要保證同一個商品跟活動 id 要綁定作唯一鍵</p>
</li>
<li><p>訂單表: 商品、活動以及用戶可以綁一起作唯一鍵</p>
</li>
</ul>
<h4 id="數據流"><a href="#數據流" class="headerlink" title="數據流"></a>數據流</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi26tz811sj31460fa77r.jpg" alt="image-20200824204607333"></p>
<ul>
<li>紅框那個很重要，因為最重要的就是去搶庫存，所以那個要更新</li>
<li>一般不作外鍵，慢</li>
<li>建立索引有原則的</li>
</ul>
<h4 id="Q-解決超賣-秒殺操作-扣減庫存"><a href="#Q-解決超賣-秒殺操作-扣減庫存" class="headerlink" title="Q: 解決超賣: 秒殺操作 - 扣減庫存"></a>Q: 解決超賣: 秒殺操作 - 扣減庫存</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi26z4b2plj30ja0eaq7l.jpg" alt="image-20200824205104211" style="zoom: 33%;" />



<h5 id="解法一-X-事務-FOR-UPDATE-但效率低"><a href="#解法一-X-事務-FOR-UPDATE-但效率低" class="headerlink" title="解法一(X) : 事務, FOR UPDATE, 但效率低"></a>解法一(X) : 事務, FOR UPDATE, 但效率低</h5><p>就是說我要去更新它，所以告訴SQL鎖住</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi270rmakwj30p00fgn23.jpg" alt="image-20200824205239608" style="zoom: 33%;" />



<h5 id="解法二-V-用-UPDATE-語句自帶的行鎖"><a href="#解法二-V-用-UPDATE-語句自帶的行鎖" class="headerlink" title="解法二(V): 用 UPDATE 語句自帶的行鎖"></a>解法二(V): 用 UPDATE 語句自帶的行鎖</h5><p>這個是透過MySQL本身的機制來保證的</p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200824205433454.png" alt="image-20200824205433454" style="zoom:33%;" />

<h4 id="Q-大量請求都訪問-MySQL-導致-MySQL-崩潰。"><a href="#Q-大量請求都訪問-MySQL-導致-MySQL-崩潰。" class="headerlink" title="Q: 大量請求都訪問 MySQL, 導致 MySQL 崩潰。"></a>Q: 大量請求都訪問 MySQL, 導致 MySQL 崩潰。</h4><ul>
<li>對於搶購活動來說，可能幾十萬人搶100台iphone, 實際大部分請求都是無效的，不需要下沉到MySQL</li>
</ul>
<h5 id="解法一-庫存預熱-by-Redis"><a href="#解法一-庫存預熱-by-Redis" class="headerlink" title="解法一: 庫存預熱 by Redis"></a>解法一: 庫存預熱 by Redis</h5><ul>
<li><p>秒殺的本質，就是對庫存的搶奪。</p>
</li>
<li><p>每個秒殺的用戶來都去DFB校驗庫存，然後扣減庫存，會導致DB崩潰</p>
</li>
<li><p>MySQL DB單點能支撐 1000 QPS, 但 Redis 單點能支撐 10萬 QPS，可以考慮將庫存信息加載到 Redis中。</p>
</li>
<li><p><strong>直接通過 Redis 來判斷並扣減庫存。</strong></p>
<h5 id="Redis"><a href="#Redis" class="headerlink" title="Redis:"></a>Redis:</h5><ul>
<li>kv pair, 也可持久化</li>
<li>支持<ul>
<li>string</li>
<li>hash</li>
<li>list</li>
<li>set</li>
<li>zset</li>
</ul>
</li>
<li>單線程的數據庫，沒有併發的問題。通過IO多路復用實現併發，大家進來就是一個個排隊</li>
<li>主持主備容災 (Disaster Tolerance) 存儲，主掛了，小弟會趕緊頂上去</li>
<li>所有單個指令操作都是原子的，要麼完全ok或失敗。</li>
<li>多個指令可以通過 Lua 腳本實現原子性</li>
<li>因為所有操作都在 內存中，所以性能極高，單機一般可以到10萬數量級</li>
<li>可作為Cache、DB、MQ</li>
</ul>
</li>
<li><p>提前放到Redis預熱，它來判斷庫存</p>
</li>
</ul>
<h4 id="Q-何時預熱"><a href="#Q-何時預熱" class="headerlink" title="Q: 何時預熱?"></a>Q: 何時預熱?</h4><ul>
<li>活動前呀!</li>
</ul>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200824210846277.png" alt="image-20200824210846277" style="zoom:50%;" />

<ul>
<li><strong>開始通過redis 減庫存</strong></li>
</ul>
<p>現在直接去讀redis庫存，如果庫存小於0就結束；一個redis是單線程的，不會有併發的問題。創了訂單才會去DB扣庫存；</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi27khniyyj312k0feaia.jpg" alt="image-20200824211136832" style="zoom:50%;" />

<h4 id="Q-使用Redis-的問題-–-仍有超賣問題，超賣在redis也要解決"><a href="#Q-使用Redis-的問題-–-仍有超賣問題，超賣在redis也要解決" class="headerlink" title="Q: 使用Redis 的問題 – 仍有超賣問題，超賣在redis也要解決"></a>Q: 使用Redis 的問題 – 仍有超賣問題，超賣在redis也要解決</h4><h5 id="方案一-–-仍有放行的問題"><a href="#方案一-–-仍有放行的問題" class="headerlink" title="方案一 – 仍有放行的問題:"></a>方案一 – 仍有放行的問題:</h5><img src="/Users/joe/Library/Application Support/typora-user-images/image-20200824211410268.png" alt="image-20200824211410268" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi27o0eo75j311c0ek42p.jpg" alt="image-20200824211500506" style="zoom:50%;" />



<h5 id="方案二-通過-Lua-腳本執行原子操作，redis-2-6版後"><a href="#方案二-通過-Lua-腳本執行原子操作，redis-2-6版後" class="headerlink" title="方案二: 通過 Lua 腳本執行原子操作，redis 2.6版後"></a>方案二: 通過 Lua 腳本執行原子操作，redis 2.6版後</h5><p>就是把剛剛redis的 GET 跟 DECR 合而為一了，在一個事務裡去操作</p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200824211654764.png" alt="image-20200824211654764" style="zoom:50%;" />

<p>現在redis對於DB就只放給它100而已，性能也夠了。但現在會有的問題是：</p>
<h4 id="Q-如果秒殺量是1萬台、10萬台呢？-gt-MySQL仍撐不了"><a href="#Q-如果秒殺量是1萬台、10萬台呢？-gt-MySQL仍撐不了" class="headerlink" title="Q: 如果秒殺量是1萬台、10萬台呢？==&gt; MySQL仍撐不了"></a>Q: 如果秒殺量是1萬台、10萬台呢？==&gt; MySQL仍撐不了</h4><p>可通過MQ去削峰，讓量慢慢地放過去</p>
<ul>
<li><p>點了商品，他會讓我等一、兩秒，讓redis(一秒十萬個)慢慢去找MySQL(集群，所以也是可以一秒一萬個)</p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200824212243509.png" alt="image-20200824212243509" style="zoom:50%;" />



</li>
</ul>
<h5 id="MQ"><a href="#MQ" class="headerlink" title="MQ"></a>MQ</h5><ul>
<li>如去星巴克取餐這類，步調不一致的問題解決</li>
<li>解耦和async的操作</li>
<li>mq一般是帶有重試能力</li>
</ul>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200824212727568.png" alt="image-20200824212727568" style="zoom: 33%;" />

<h5 id="q-如果極端之下MQ出現部份投遞失敗怎辦？"><a href="#q-如果極端之下MQ出現部份投遞失敗怎辦？" class="headerlink" title="q: 如果極端之下MQ出現部份投遞失敗怎辦？"></a>q: 如果極端之下MQ出現部份投遞失敗怎辦？</h5><p>RabbitMQ, Kafka, RocketMQ</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gi283k35tvj314o0g8tdo.jpg" alt="image-20200824212957025" style="zoom: 50%;" />

<ul>
<li>沒長度限制的</li>
<li>redis是集群的；剛剛講的10萬是單機的</li>
<li>失敗可能是網路原因</li>
</ul>
<h3 id="讀"><a href="#讀" class="headerlink" title="讀"></a>讀</h3></div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/12/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><span class="space">&hellip;</span><a class="page-number" href="/page/78/">78</a><a class="extend next" rel="next" href="/page/14/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2023 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>