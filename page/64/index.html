<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-180692466-1', 'auto');
ga('send', 'pageview');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">384</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">66</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Awaken Desparado</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/04/15/System-Design/9p/2019-04-15-BigTable/">System-Design/9p/2019-04-15-BigTable</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/SystemDesign/">SystemDesign</a></span><div class="content"><h1 id="General"><a href="#General" class="headerlink" title="General"></a>General</h1><ul>
<li>NoSQL DB</li>
<li>SS table 的讀跟寫</li>
<li>Bloom Filter</li>
<li>Look up service</li>
<li>Merge K Sorted Arrays</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 486. Merge K Sorted Arrays</span></span><br><span class="line"><span class="comment"># 中文English</span></span><br><span class="line"><span class="comment"># Given k sorted integer arrays, merge them into one sorted array.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Example</span></span><br><span class="line"><span class="comment"># Example 1:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input: </span></span><br><span class="line"><span class="comment">#   [</span></span><br><span class="line"><span class="comment">#     [1, 3, 5, 7],</span></span><br><span class="line"><span class="comment">#     [2, 4, 6],</span></span><br><span class="line"><span class="comment">#     [0, 8, 9, 10, 11]</span></span><br><span class="line"><span class="comment">#   ]</span></span><br><span class="line"><span class="comment"># Output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</span></span><br><span class="line"><span class="comment"># Example 2:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input:</span></span><br><span class="line"><span class="comment">#   [</span></span><br><span class="line"><span class="comment">#     [1,2,3],</span></span><br><span class="line"><span class="comment">#     [1,2]</span></span><br><span class="line"><span class="comment">#   ]</span></span><br><span class="line"><span class="comment"># Output: [1,1,2,2,3]</span></span><br><span class="line"><span class="comment"># Challenge</span></span><br><span class="line"><span class="comment"># Do it in O(N log k).</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is the total number of integers.</span></span><br><span class="line"><span class="comment"># k is the number of arrays.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param arrays: k sorted integer arrays</span></span><br><span class="line"><span class="string">    @return: a sorted array</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mergekSortedArrays</span><span class="params">(self, arrays)</span>:</span></span><br><span class="line">        <span class="comment"># write your code here</span></span><br><span class="line">        <span class="keyword">return</span> self.helper_heapq(arrays)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper_heapq</span><span class="params">(self, arrays)</span>:</span></span><br><span class="line">        hp = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(len(arrays)):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> arrays[k]: <span class="keyword">continue</span></span><br><span class="line">            heapq.heappush(hp, (arrays[k][<span class="number">0</span>], k, <span class="number">0</span>))</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># print(hp)</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">while</span> hp: </span><br><span class="line">            _, kth, idx = heapq.heappop(hp)</span><br><span class="line">            <span class="comment"># print(val, arrays[kth][idx], kth, idx)</span></span><br><span class="line">            res.append(arrays[kth][idx])</span><br><span class="line">            <span class="keyword">if</span> idx+<span class="number">1</span> &lt;= len(arrays[kth])<span class="number">-1</span>:</span><br><span class="line">                heapq.heappush(hp, (arrays[kth][idx+<span class="number">1</span>], kth, idx+<span class="number">1</span>))</span><br><span class="line">            <span class="comment"># else:</span></span><br><span class="line">            <span class="comment"># 大可以等下一輪拉出來再push那條的下一個元素，不急</span></span><br><span class="line">            <span class="comment">#     # while idx+1 &gt; len(array[kth])-1:</span></span><br><span class="line">            <span class="comment">#     val, kth, idx = heapq.heappop(hp)</span></span><br><span class="line">            <span class="comment">#     res.append(val)</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<ul>
<li>Binary Tree Serialization</li>
</ul>
<h1 id="Big-Table"><a href="#Big-Table" class="headerlink" title="Big Table"></a>Big Table</h1><p>Big Table vs GFS</p>
<table>
<thead>
<tr>
<th></th>
<th>GFS</th>
<th>BiGTable</th>
</tr>
</thead>
<tbody><tr>
<td>本質</td>
<td>分布式File System</td>
<td>分布式 NoSQL DB</td>
</tr>
<tr>
<td>開發公司</td>
<td>Google</td>
<td>Google</td>
</tr>
<tr>
<td>是否開源</td>
<td>No</td>
<td>No</td>
</tr>
<tr>
<td>類似開源產品</td>
<td>HDFS</td>
<td>HBase</td>
</tr>
<tr>
<td>操作</td>
<td>Read &amp; Write</td>
<td>CRUD</td>
</tr>
<tr>
<td>類似什麼</td>
<td>家用電腦的文件系統</td>
<td>大數據版的Excel</td>
</tr>
</tbody></table>
<p>結構化文件可上BigTable終究是存上GFS。</p>
<blockquote>
<ul>
<li>BigTable 和 Dremel 主要有什么区别？<ul>
<li>bigtable是nosql storage system，dremel是query system。</li>
</ul>
</li>
<li></li>
</ul>
</blockquote>
<h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><ul>
<li>修改 CUD</li>
<li>查找 R</li>
</ul>
<p>後端通常給web server 使用，Scenario比較單一</p>
<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><p>數據庫在邏輯上就是kv型式。硬盤不能存表結構，最終都轉成string去存在disk上。</p>
<p>就是要從文件系統的基礎上去思考搭數據庫系統。</p>
<p><strong>Q: 在文件裡怎麼更好支持查詢操作？</strong></p>
<ul>
<li><input disabled="" type="checkbox"> 把文件硬盤全讀到內存，然後在內存作流程管理</li>
<li><input checked="" disabled="" type="checkbox"> 直接在硬盤中對數據作排序 + 硬盤中二分。<ul>
<li>How? 外排序</li>
</ul>
</li>
</ul>
<p><strong>Q: 如果有相應的feature有修改操作了，怎辦？</strong></p>
<ul>
<li><input disabled="" type="checkbox"> 直接在文件裡修改？<strong>==&gt; 大家都要移動</strong></li>
<li><input disabled="" type="checkbox"> 讀整個文件，修改好後，把原文件刪了，重新寫入新文件。<strong>==&gt; 耗時，每次要讀出寫入其他多餘不變的內容</strong></li>
<li><input checked="" disabled="" type="checkbox"> 不修改，直接append操作追加一條紀錄。<strong>V! ==&gt; 特快！</strong><ul>
<li>但…怎麼識別最新的紀錄？<ul>
<li>在<strong>所有數據後面加個時間戳</strong>，時間戳最大的就是真正的我們要找的紀錄</li>
</ul>
</li>
<li>沒順序怎麼二分？<ul>
<li>分塊有序<ul>
<li>每個塊內部有序</li>
<li>寫時候只有最後一塊無序，並且隔一段時間整理成有序。</li>
<li>例：每次寫時就是在最後塊append一條紀錄，for看最後塊有沒有那個值，沒的話就往前一塊作二分查找；每隔時間到就把最後塊給整理成有序下，然後啟新的塊。一個塊是256MB，滿了後就不再寫了。</li>
</ul>
</li>
<li>分塊太多時會有很多重複紀錄啊？如某個feature一直被改改改<ul>
<li>Solution: 把有序塊中作去重，作k路去重merge，如果是根據時間去重，當時間一樣時，</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Big Table 為了寫優化，選了３作。</p>
<blockquote>
<ul>
<li><strong>老师，请问，数据库系统不存储数据，所有数据最终都存储在文件系统当中， 数据库只是负责查询和组织诗句是吗</strong><ul>
<li><strong>是的，大多DB都使用现有的文件系统作为底层存储机制</strong></li>
</ul>
</li>
<li>数据库是不是可以建立在任何的文件系统， 还是要为数据库写一个特殊的文件系统以提高效率？<ul>
<li><strong>一般没有必要</strong>，使用普通的文件系统在性能上已经足够了。</li>
</ul>
</li>
<li>利用数据库系统查询文件系统的时候 是在写入文件时就将文件里的所有表内容 按key-value的形式 再写入数据库么？<ul>
<li>按特定的数据结构存储，正如你说的key-value的形式，然后写入数据库中</li>
</ul>
</li>
<li>这里除去duplicate的k路归并也是全部读到内存中进行么？每一块中可以有duplicate存在么？<ul>
<li>K路归并的方法在后面的拓展视频中有讲到，每一个块中可以有duplicate</li>
</ul>
</li>
<li>我如果想查找k的值，我怎么知道k在哪个文件里？<ul>
<li>每个文件存储的k都是一个范围内的k值，直接二分搜索找到k在哪个范围内就找到了那个文件</li>
</ul>
</li>
<li>append到文件末尾也会增加文件大小啊，怎么避免和其他文件储存的位置冲突？<ul>
<li>如果是基于文件系统上搭建数据库，不必考虑此问题，操作系统会帮你完成；如果是自己直接基于disk搭建数据库，每个数据块初始化的时候就预先开辟适量的disk空间，当写入增加的数据量超过原有分配的空间的时候就需要申请一块更大的disk空间，将它拷贝过去，然后将原有的空间释放即可。（可以看下操作系统书中有关机械磁盘基本原理的介绍）</li>
</ul>
</li>
<li>直接把修改内容append到最后，这个方法，如果修改得太多了，会不会感觉很乱，每次读的时候cost增加，因为一个file可能有三分之一的size是修修改改？需不需要在修改的量超过一个比例的时候处理一下？<ul>
<li>首先要明确一点的是，没有任何一个数据库是生来都适合所有操作和场景的。也就是说，BigTable 本来就不适合修改特别多的场景。另外 BigTable 这类数据库会定期 Merge sstable，从而消除那些中间态的修改记录。</li>
</ul>
</li>
<li>分块是什么意思？<ul>
<li>是指将指定的“filename” 这个文件分成很多块，然后每次存储的时候找该文件编号的最后一块进行写入。如果写满了 是需要在文件系统里追加写入一个新的对应该文件名的块</li>
</ul>
</li>
<li>在讲到“分块有序”时，老师说到每个块之间都是有序的。但是块与块之间是不是有序的呢？也就是说是不是第1个块的 key 值都小于第2块的 key 值，第2块的 key 值都小于第3块的key值？（最后一块除外）<ul>
<li>块与块之间是有序的。这里可以是key-value存储记录块的索引地址；也可以是B+树索引，方便查找修改。</li>
</ul>
</li>
<li>那每次查询都要查询每一个块吗？不能直接锁定就在哪一个块中（例如索引之类的）？<ul>
<li><strong><em>好问题，后面的视频中讲到了索引，带着问题继续学习，棒！</em></strong></li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="One-Work-Solution"><a href="#One-Work-Solution" class="headerlink" title="One Work Solution"></a>One Work Solution</h1><h3 id="寫入過程"><a href="#寫入過程" class="headerlink" title="寫入過程"></a>寫入過程</h3><p>​    一個無序快快寫滿時，就排序然後寫入到FS。</p>
<ul>
<li><input disabled="" type="checkbox"> 讀入到內存作快排 X</li>
<li><input disabled="" type="checkbox"> 外排序 X;  <ul>
<li>每個文件最多就也256M，一個內存完全可放下，完全不需要外排</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> <strong>可不可一開始就放在內存？V!</strong></li>
</ul>
<p>比方，內存會用跳表這個結構，</p>
<h3 id="機器掛了怎辦？"><a href="#機器掛了怎辦？" class="headerlink" title="機器掛了怎辦？"></a>機器掛了怎辦？</h3><p>Write Ahead Log (WAL)，就只是append而已</p>
<p>append寫disk非常快和一般寫不同，打開模式為寫</p>
<p>如果要把最後一個塊放在硬盤的話，它不只要可以append還要可以read的操作；</p>
<p>append和寫不同，打開如果是append模式時，ptr是在尾巴；如果是寫模式，文件原內容會被清楚，ptr會指在文件頭。</p>
<p>所以：</p>
<p>内存排序+1次硬盘统一写入+1次硬盘写Log</p>
<p>Link: <a href="http://www.larsgeorge.com/2010/01/hbase-architecture-101-write-" target="_blank" rel="noopener">http://www.larsgeorge.com/2010/01/hbase-architecture-101-write-</a> ahead-log.html</p>
<blockquote>
<ul>
<li>每个块最大256M，是Google的Big Table自己定义的吗？还是一种通用的标准呢？而且这个“块”的学名是什么呀？谢谢！<ul>
<li>是的，是官方定义。<br>256M 块的大小是google工程师长期实践的一个经验值，统计过这个值比较合理。<br>最后，块的专业名词为：chunk</li>
</ul>
</li>
<li>什么是硬盘外部排序？<ul>
<li>当参加排序的数的量太大，或内存不足以存放时，需要使用外排序。外部排序指的是大文件的排序，即待排序的记录存储在外存储器上，待排序的文件无法一次装入内存，需要在内存和外部存储器之间进行多次数据交换，以达到排序整个文件的目的。</li>
</ul>
</li>
<li>老师，最后一个块是存在内存里的，存满256MB之后再写入硬盘。请问每次append一条数据到块里，是每次利用二分查找来保证这条数据插入到正确位置，还是每次就直接在块的尾部直接添加数据，直到块满了256MB，再排序变为有序啊。<ul>
<li><strong>直接在尾部添加，直到块满就进行排序。</strong></li>
</ul>
</li>
<li>这里的WAL 不是应该 每次有新的数据到内存都要写一次WAL吗？ 为什么说是1次的硬盘写LOG ？<ul>
<li><strong>是的，Write Ahead Log，就是对内存中Sorted Table里，new append data在HDD做一次append的操作。请再认</strong>真听讲，要确认每一个细节，其实再认真听讲是可以找到答案的，老师有讲到此细节。确实有问题了，仔细思考后再提出问题，这样学习深刻。</li>
</ul>
</li>
<li>Write ahead log 为什么是一次， 不应该每次有update 都要写一次吗？<ul>
<li>这块的WAL是多次的数据操作才会触发一次log写操作。这是为了性能的考量，HDD写的速度相比磁头寻道速度还是很快的，不想把太多时间浪费在寻道上，也就是每次写操作就需要寻道一次。</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="讀取優化"><a href="#讀取優化" class="headerlink" title="讀取優化"></a>讀取優化</h1><p>本來是只能在FS上作二分，更好的是建index</p>
<ul>
<li>一個簡單的建index 的方法：<ul>
<li>Key<ul>
<li><strong>就不需要整個文件作二分</strong></li>
<li>把一些Key放入內存作為index</li>
<li>Index有效減少磁盤讀寫次數</li>
</ul>
</li>
</ul>
</li>
<li>B-tree Index</li>
</ul>
<blockquote>
<ul>
<li>所以内存中有无序的存储块，和sorted存储块（sorted list），先用for查找无序没有，再去有内存中sorted list上跳跃查找？<ul>
<li>大体思路是对的，详细流程稍微更正如下：<br>(1)query Master server by the key;<br>(2) tablet server（即SSTable slave server）;<br>(3) check skip list(即跳表，在memory中) in the tablet server（对象没有排序的Sorted List）;<br>(4) check bloom filter(memory) for a SSTable(Sorted String Table)（最好一块有序块）;<br>(5) check index(memory) for a SSTable(查找其key的大致范围);<br>(6) find the value in the SSTable(根据在内存index定位到的小范围，在磁盘中通过二分查找这个key的文件内容).</li>
</ul>
</li>
<li>“看到这里我对于建立index又有些迷茫了。本来一个块中就是有序的，直接二分不就行了。就算是先看index，也是要先对index进行二分，再拿着找到的位置去块本身里找东西。那么index的好处是不是因为index比数据本身的size小，所以在index里二分比在数据里二分块？ “<ul>
<li>index 是在内存里的，你在内存里二分 index 比直接去 disk 上二分要快好几十倍。<strong><em>文件操作远远慢于内存操作</em></strong>。</li>
</ul>
</li>
</ul>
</blockquote>
<blockquote>
<p>这里是课上提到的 B-Tree / B+ Tree 的一些资料：</p>
<p>Wiki:<br><a href="https://en.wikipedia.org/wiki/B-tree" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/B-tree</a></p>
<p>以B tree和B+ tree的区别来分析mysql索引实现:<br><a href="https://www.jianshu.com/p/0371c9569736" target="_blank" rel="noopener">https://www.jianshu.com/p/0371c9569736</a></p>
<p>这部分的内容只需要了解原理，不需要代码实现。</p>
</blockquote>
<h2 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h2><h3 id="繼續讀優化"><a href="#繼續讀優化" class="headerlink" title="繼續讀優化"></a>繼續讀優化</h3><p>因為寫的時候我們用的是append, 但就是讀的時候被搞亂了，所以讀就要一直優化。</p>
<p>數組長度自己定。</p>
<ul>
<li>多個 hash fn時，還是一樣同個數組在作mapping作查詢。</li>
<li>是可能會有False Positive的；但就是起了filter的作用</li>
<li>精度跟<ol>
<li>加入的字符串數目</li>
<li>hash個字</li>
<li>array長度。</li>
</ol>
</li>
<li>有誤判率計算公式</li>
</ul>
<p><strong><em>先用bloom filter看這個塊有沒有、有的話再用index找key可能在塊中的範圍，再用二分找這個塊裡的值</em></strong>　當然，在這之前是要先做完對最後那個在內存的塊的線性查找。</p>
<h2 id="SS-Table-amp-Skip-List"><a href="#SS-Table-amp-Skip-List" class="headerlink" title="SS Table &amp; Skip List"></a>SS Table &amp; Skip List</h2><ul>
<li>String is Store in the File. </li>
<li>SSTable = Sorted String Table</li>
<li>Sorted List 用 Skip List 实现</li>
</ul>
<ol>
<li>Skip List<br> Code: <a href="https://github.com/petegoodliffe/skip_list" target="_blank" rel="noopener">https://github.com/petegoodliffe/skip_list</a> Wiki: <a href="http://bit.ly/2g0C29a" target="_blank" rel="noopener">http://bit.ly/2g0C29a</a></li>
<li>SSTable<br> Google SSTable Page: <a href="http://bit.ly/1kqwrFe" target="_blank" rel="noopener">http://bit.ly/1kqwrFe</a></li>
</ol>
<blockquote>
<ul>
<li>“比如如果我用64个hash函数，每个文件按256M算(按每行数据256bytes算就是一百万行)。你这里难道把一百万个key都放在bloom filter里么 “<ul>
<li>是的，bloom filter的基本单位是bit，这里记录的是hash后的数字，不是文件，<strong><em>一个字节8bit，大约10w 字节，就是100kb，也就是0.1mb</em></strong>。（为了精度可能需要大概几mb的内存就够了）</li>
</ul>
</li>
<li>之前一小节讲的查一个key在不在这个块里需要先把这个块的index load到内存中再二分。你load index文件的时候不也是O(n)么？不在硬盘上二分是因为磁头移动的时间？所以宁愿先load一整个index file到内存中<ul>
<li>是的，因为磁头寻道时间开销相对内存来说很大。（可以读下《现代操作系统》 5.4 磁盘臂调度算法）为了减少磁头寻道的开销衍生出的一些算法</li>
</ul>
</li>
<li>“算两次hash值是为了防止collision么？可以扩大bloom filter的array大小，这样hash也不容易碰撞，这样可以么？ “<ul>
<li>多个hash function可以减少误判率，扩大array的大小也可以减少误判率</li>
</ul>
</li>
<li>index是B+ tree对吧，那为什么只能找到范围？最后一步，二分搜索在磁盘上进行，不是会非常慢么？<ul>
<li>对的，因为具体的内容要到块当中去找，块里的内容key是用字典序排列的，因此存在相同字母开头的key，所以只能锁定一个范围。锁定这个范围之后就可以二分了，logn的速度可以接受</li>
</ul>
</li>
<li>之前提到BigTable会定期执行Disk Merge Sort，这样不就可以直接对磁盘Blocks二分了吗? 为什么还要设计Bloom Filter？<ul>
<li>有很多请求是需要判断key是否在文件中，HDD二分查找也是log级别的操作，Bloom Filter来过滤掉很多不必要的HDD操作</li>
</ul>
</li>
<li>对于SSTable来说好像Index和Bloomfilter都是想要做一件事。视频的图中为什么两个都需要建立呢？<ul>
<li>Boom Filter是起到快速筛选的，如果Bloom Filter中key存在，那么就可能在此SSTable中存在，如果不存在那么肯定就不存在此有序列块中。当key经过Bloom Filter确定可能存在后，再通过index确定其所限的查找范围，确定后，再去磁盘中进行二分查找。</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="Scale-Sharding"><a href="#Scale-Sharding" class="headerlink" title="Scale - Sharding"></a>Scale - Sharding</h1><ul>
<li><p>垂直沒那麼好，因為可能也想關心這筆data的其他元素。會增加讀取其他feature的難度。</p>
</li>
<li><h3 id="所以水平-consistent-hashing"><a href="#所以水平-consistent-hashing" class="headerlink" title="所以水平 consistent hashing"></a>所以水平 consistent hashing</h3><ul>
<li>大表被拆成了很多個小表們</li>
</ul>
</li>
</ul>
<h1 id="BigTable的分布式鎖"><a href="#BigTable的分布式鎖" class="headerlink" title="BigTable的分布式鎖"></a>BigTable的分布式鎖</h1><ul>
<li><p>讀寫鎖</p>
<ul>
<li><p>一把鎖，兩種屬性：讀跟寫</p>
<ul>
<li><p>讀鎖：讀的時候上的</p>
</li>
<li><p>寫鎖：寫的時候上的</p>
</li>
<li><p>特性</p>
<ol>
<li><p>線程一加讀鎖成功了，又來了兩個讀線程，可以加鎖成功，可進行讀共享</p>
</li>
<li><p>線程一加寫鎖成功了，又來了兩個讀線程，不能加鎖成功，兩個讀的要block；</p>
<p>寫的時候只允許一個線程對內存作寫操作</p>
</li>
<li><p>線程一加讀鎖成功了，來了寫被block，又來了讀就要被block了；寫的優先級是比較高的，並且線程二來得早線程三就得要等了。</p>
<ol>
<li>讀是共享</li>
<li>寫是獨占</li>
<li>寫的優先級更高</li>
</ol>
</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l4p7wsnj30nw0nwte3.jpg" alt="image-20200420195125198" style="zoom:50%;" />

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0h80fthij30s00j0n0m.jpg" alt="image-20200420195309119"></p>
<ul>
<li><p>鎖服務器：</p>
<ul>
<li>Master的一致性哈希分配小弟也就這邊執行了</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>如果是多台锁服务器，怎么处理并发呢，或者说这么保证对同一个key的上锁操作在同一台锁服务器上进行呢？也是hash吗？map存在web server 上？<ul>
<li>如果锁是一个cluster，<strong><em>多个节点之间会通过一致性协议来保持同步，对外表现得就像是只有一个节点一样，所以不用担心不一致的问题</em></strong>。</li>
</ul>
</li>
<li>这里的图显示 读 和写的都要LOCK 这个KEY，那不是会神慢吗？ 如果KEY LOCKED了后续怎么样呢？ 写一个RETRY吗？ 如果 很多程序同时读这个 KEY，那都得等待了？<ul>
<li>从安全和数据一致性来说，会相对重要些。lock肯定会慢些，但是这不影响总体的运行效率和用户体验。lock本身就是一个信号量互斥的实现机制，也线程间通信的基础， 详阅<a href="https://en.wikipedia.org/wiki/Lock_(computer_science)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Lock_(computer_science)</a></li>
</ul>
</li>
<li>consistent hash map存在lock server上，那master还有什么作用？<ul>
<li>比如 heatbeat 。还有好多事儿要做呢，除了存储还有计算的工作呀。包括管理很多机器的启动，备份恢复等等。</li>
</ul>
</li>
<li>感觉这个lock server不是distributed lock, 实际上是一个centralized locking service?<ul>
<li>lock service内部是distributed实现的，但对外接口是中心化锁服务。具体使用的是chubby。</li>
</ul>
</li>
<li>如果写入的是一个新key，metadata里面还没有这个key对应的consistent hashing value，这时候的写入过程是什么呢？<ul>
<li>lock server查找有这个key的话锁上-》lock server返回server id-》去tablet server对应id写入-》写入GFS-》成功写入 返回True-》lock server释放锁</li>
</ul>
</li>
<li>请问 summary of write 这张图里的 master， 所以其实没有起到任何作用， master 的功能全部被 lock server代替了么？<ul>
<li>master还是提供分发控制的作用；lock server就是保证数据一致性（同一时间只有一个线程操作）</li>
</ul>
</li>
<li>另外你这个读请求的锁，应该只是block其他的对于这个key的write request吧。如果有很多对这个key的read request怎么办？这种情况下难道不能单机多线程读这个key吗。 虽然这种hot partition的case很多情况下是用户没设计好partition key..<ul>
<li>如果有很多对这个key的read request，系统只能等待，性能确实就会下降很多。如果想提这块的性能，可以使用双指针的方法，数据多份备份，这样这块的性能就会提升，但是数据脏读概率增大了。</li>
</ul>
</li>
<li>读取data这种操作 为何也需要锁 没有锁也没有任何问题吧。<ul>
<li>因为在read的同时可能还有针对同一个对象的updata/delete正在进行，为了防止读数据读到一半数据被删了这种不愉快的事情发生，加锁是必要的。</li>
</ul>
</li>
<li>所以timestamp这个信息是存在lock server上的内存中么？一旦完成了上一个，lock server会向下一个最新的request的server发送消息告诉它，现在unlock了，可以开始处理它的请求了？还是以别的方式做信息交互？<ul>
<li>你说的是对的，不过需要知道的是timestamp在sstable上也会存，便于查找的时候知道哪一个是现在最新的值</li>
</ul>
</li>
<li>如果client 拿到锁之后 还没来得及写就意外挂了， 锁怎么释放？有时间限制吗？<ul>
<li>“A client’s session<br>expires if it is unable to renew its session lease within the<br>lease expiration time. When a client’s session expires, it<br>loses any locks and open handles.” client超时后，会释放锁资源。</li>
</ul>
</li>
<li>Disk 中的 每个SSTable 都有自己的bloomfilter 和 index 吗？ 不同SSTable 的可以不一样吗<ul>
<li>对，这样理解是对的。当TableServer启动后开始服务初始化期间，会读取所有的有序块（SSTable）的Bloom Filer和Index到某个内存区域，以便服务查找。</li>
</ul>
</li>
<li>Lock Server存的metadata的数据来源是哪里呢？从BigTable Master里来的？他们之间怎么交流的呢？<ul>
<li>BigTable Master的key<br>这个系统对外会提供一个接口，接口不可直接操作bigtable master、lock server等，这些操作都是内部同步实现的逻辑<br>通信都是socket，可以考虑之间加上消息队列，直接append提交任务给目标server</li>
</ul>
</li>
<li>the entire process did not involve the master server… are we saying that if we have lock/zookeeper, we dont have to access master when reading and writing?<ul>
<li>only the first time from the a client for request, the master server should handle it, as master server need all the tablet server status for the first time of health-check, later then master server will synchronize/delegate all the tablet server status to locking server for the task hand-over. in that case, master server will move on to focus the health-check only.</li>
</ul>
</li>
<li>分布式锁这里面的“分布式”是什么意思？就是说这个锁适用于处理distributed system里面多台服务器需要对同一个资源进行读写的情形？那么不分布式的锁是怎么回事呢？<ul>
<li>这里的分布式指的是适用于多台服务器试图访问同一资源，相当于多台服务器上的多个进程共享资源。<br>非分布式的锁，就是单机上对资源的访问锁。通常就是线程间的资源共享或者单机上多个进程资源共享，通常使用操作系统提供的基础锁设施就可以完成。</li>
</ul>
</li>
<li>整个write 过程， 那一点是系统最慢的过程？ 同样，整个read 过程哪一点是系统最慢的过程？ 这个分布式锁应该会让系统变慢不少把？<ul>
<li>写的过程中，写满一个SSTable序列化到硬盘的时候相对比较慢，读的时候查引索时相对慢一点。实际在使用时，bigtable会使用一种叫做copy on write(cow)的机制，来保证读写操作的原子性，降低分布式锁对速度的影响</li>
</ul>
</li>
<li>感觉bigtable 这种read 的时候需要一个个SSTable 分别查找的方式不如Cassandra 这种直接一个 consistent hashing ring，一步就能知道data 在哪个node上面快啊？<ul>
<li>bigtable首先会在锁服务器上查一致性哈希表，得到在哪个server上。得知之后，会去那个server上查memtable、bloomfilter、sstable</li>
</ul>
</li>
<li>为什么一开始shard还是要通过master 来进行呢，为什么不能通过Locker 来进行呢<ul>
<li>通过最开始的文件sharding，取得与其负责管辖的tablet servers的建立联系，并且记录其各自的运行情况（alive/not alive）和健康信息。</li>
</ul>
</li>
<li>这里讲的master会shard file是什么意思？以及consistent hashing的map如果是存在 master/lock server的内存里，master/lock挂了怎么办<ul>
<li>shard file是共享文件的意思，就是master所管理的各个node server都可以访问和操作这个文件。<br>以前确出现在master挂掉导致整个集群挂掉的情况，现在master都会加有数据持久化的功能、master备份（主master挂掉，备份master就会启动，基本不影响集群运行）</li>
</ul>
</li>
<li>把metadata放在lock里跟放在master里比有什么优势吗？如果这样做会使系统性能更好，那是为什么呢？<ul>
<li>这里相当于将lock这个服务从master里面拆分了出来，这样肯定好的，master一般就是系统的性能瓶颈，减少master的负载，提高系统性能。</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="Appendixes"><a href="#Appendixes" class="headerlink" title="Appendixes"></a>Appendixes</h1><h2 id="K-sorted-merge"><a href="#K-sorted-merge" class="headerlink" title="K sorted merge"></a>K sorted merge</h2><p>每次找最小的都要O(K), N個下來就要 O(NxK), 用個heap吧 可O(Nxlg(K))</p>
<h2 id="外排序"><a href="#外排序" class="headerlink" title="外排序"></a>外排序</h2><p>一共8G的話，每次load進來2G, 就有四條排好的。可再接用K-sorte merge</p>
<ul>
<li><p>最小可以就維護４個元素</p>
</li>
<li><p>改善heap的效能可用</p>
<ul>
<li><p>敗者樹 可以改善選取最小項的常數項的Ｋ</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l4pvld7j31460ggwgt.jpg" alt="image-20200420202745772" style="zoom:50%;" />
</li>
<li><p>可用選擇-置換的方法　替換掉每組2G的平均分組，就減了少組數，也就是K的大小更小，跑起來更快；每條長度不等就是</p>
<p><a href="https://www.bilibili.com/video/av69423198/" target="_blank" rel="noopener">https://www.bilibili.com/video/av69423198/</a></p>
<ul>
<li>搭配最佳歸併樹，讓IO次數最少的==&gt;帶權路徑最小：Huffman Tree</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>之前write 过程的去重就是在merge过程将重复的key中老的数据去除就好了对吧。<ul>
<li>Yes</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="GFS-amp-BigTable"><a href="#GFS-amp-BigTable" class="headerlink" title="GFS &amp; BigTable"></a>GFS &amp; BigTable</h2><p>Disk vs Excel</p>
<h2 id="B-Tree-–-Balance-Tree"><a href="#B-Tree-–-Balance-Tree" class="headerlink" title="B+ Tree – Balance+ Tree"></a>B+ Tree – Balance+ Tree</h2><ul>
<li>BST 如果有10億數據怎辦？1Gx4Byte == 4GB<ul>
<li>一顆樹就要4GB，多棵樹肯定不行在內存，長期只能在disk</li>
<li>一次尋軌10ms</li>
<li>順序讀取 80MB/S</li>
</ul>
</li>
</ul>
<p>Methods:</p>
<ol>
<li><p>找一次就要4GB/80MB的時間，不可行</p>
</li>
<li><p>只讀取訪問路徑上的數，就是30多個元素 即 30x10ms = 300ms</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0j7utyd2j314q0eogpr.jpg" alt="image-20200420210210409" style="zoom:67%;" />
</li>
<li><p>把二叉變多叉讓樹高降低！</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0j9nfovbj31460ggk0s.jpg" alt="image-20200420210355659" style="zoom:67%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420211217773.png" alt="image-20200420211217773" style="zoom:67%;" />

<p>17是key, data是一堆的其他的feature；B-Tree 為了存硬盤中的數據設計</p>
</li>
<li><p>B+ </p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420211622972.png" alt="image-20200420211622972" style="zoom:67%;" />



</li>
</ol>
<h2 id="Bloom-Filter-vs-Hash-Table"><a href="#Bloom-Filter-vs-Hash-Table" class="headerlink" title="Bloom Filter vs Hash Table"></a>Bloom Filter vs Hash Table</h2><h3 id="Hash"><a href="#Hash" class="headerlink" title="Hash"></a>Hash</h3><img src="/Users/joe/Library/Application Support/typora-user-images/image-20200804213257638.png" alt="image-20200804213257638" style="zoom: 33%;" />

<ul>
<li>衝突時多是用拉鏈法</li>
</ul>
<h3 id="Bloom-Filter-1"><a href="#Bloom-Filter-1" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h3><p>散射到二進制向量來，用一個很長的二進製向量</p>
<ul>
<li><p>eg，Int ==&gt; 32位二進製</p>
</li>
<li><p>空間、時間遠超過一般算法</p>
</li>
<li><p>說沒有時，一定沒有；說有時，不見得真的有</p>
<p>把不存在的先擋掉了，在後面要跟一個真正的db或是file就是文件存儲系統這類的權威機構</p>
</li>
</ul>
<h4 id="Case1"><a href="#Case1" class="headerlink" title="Case1"></a>Case1</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghf42j9a9zj310m0ca44f.jpg" alt="image-20200804214226956" style="zoom:67%;" />

<h4 id="Case2"><a href="#Case2" class="headerlink" title="Case2"></a>Case2</h4><ul>
<li>他說有時，還是要去數據庫查詢一次看是不是真的在</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghf43yx4orj30u00ig44x.jpg" alt="image-20200804214352821" style="zoom:67%;" />

<h4 id="Case3"><a href="#Case3" class="headerlink" title="Case3"></a>Case3</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghf4jy16hwj30vs0himz0.jpg" alt="image-20200804215814464"></p>
<h4 id="Case4"><a href="#Case4" class="headerlink" title="Case4"></a>Case4</h4><p>時間上強過 Trie, Hash, Array, LinkedList</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghf4playf0j31980u0are.jpg" alt="image-20200804220418586"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghf4o2uajsj31cc0oqtkd.jpg" alt="image-20200804220255205"></p>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200804220547109.png" alt="image-20200804220547109" style="zoom:67%;" />

<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><ol>
<li>比特幣<ul>
<li>Redis(內存的緩存，表示把元素暫存在內存，直接返回更快) VS BloomFilter</li>
<li><img src="/Users/joe/Library/Application Support/typora-user-images/image-20200804214916954.png" alt="image-20200804214916954" style="zoom:50%;" />
- 全節點
- SPV節點，sample payment verification，帳戶可以在這個節點很快判斷是否存在就用到BF
- ![image-20200804215035897](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghf4ayb54mj30rw0gcdmb.jpg)
- 存在的話再去相應區塊找</li>
</ul>
</li>
<li>分布式系統</li>
</ol>
<p>一種數據結構，跟hash table類似，檢測一個元素在一個集合有沒有出現。</p>
<ul>
<li>區別在 BF 更省空間。Hash還是比較費內存。BloomFilter沒有鏈表，主要是用0/1數組，叫<strong>Bit Array</strong>，沒有鏈表操作，用到多個hash function。</li>
<li>Hash fn 也不是愈多愈好，這會讓0/1消耗愈快；一般取4~16個，有理論結果。</li>
<li>快速檢測一條紀錄有沒有在某一個大塊之中。</li>
<li>在big table中查大多時候都是沒在某一個小塊，不希望每個小塊都去查二分，小塊其實不小。</li>
<li>hash不是等長結構因為有linked list, 無法直接存出；BF可以存出去，就是個bit array了。</li>
</ul>
<p>當要在兩個超過內存的大file裡找交集的urls時，相比hash一長字串就可以節省很多很多空間；但可能小誤報啦。</p>
<p>正常是不删除的</p>
<ul>
<li><strong>Counting BF vs Standard BF</strong></li>
<li>數組動態膨脹，依據被標為1超過的比例；但BF不存key, 所以複製不過去，膨脹時就是新舊的bit array都要查。效率不是特別好。。短的話誤報率高</li>
</ul>
<blockquote>
<ul>
<li>一个bit 怎么做加法？<ul>
<li><strong>如果是 Counting bloom filter 的话，每一位就用 int 不用 bit 了。</strong></li>
</ul>
</li>
<li>Bloom filter自动膨胀的时候，能不能把原来的bit array丢掉，把所有数据全部拿来重算一遍？我不理解老师说的不能重算一遍这个点。<ul>
<li>不能。比如你把单词 hello world 放到 bloomfilter 里以后，得到的 bit array 是 <code>[1,0,0,1,0,1,0,0,0,0,1,0,0]</code>，难道你可以根据这些0和1反向推导出是 Hello 和 World 这两个单词么？BF 不可逆。</li>
</ul>
</li>
</ul>
</blockquote>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l4frsejj311m0fqgpt.jpg" alt="image-20200420214218446" style="zoom:50%;" />







<h2 id="跳跃表-Skip-List"><a href="#跳跃表-Skip-List" class="headerlink" title="跳跃表 Skip List"></a>跳跃表 Skip List</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420215801422.png" alt="image-20200420215801422" style="zoom:67%;" />

<ul>
<li>為什麼要用跳表？<ul>
<li>是個有序鏈表</li>
<li>可提取一些元素，加一層引索</li>
</ul>
</li>
</ul>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420220446625.png" alt="image-20200420220446625" style="zoom:67%;" />



<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l405klnj311m0fqgpt.jpg" alt="image-20200420220653019"></p>
<ul>
<li>本質是一個分層鏈錶</li>
</ul>
<h1 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h1><img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420123257444.png" alt="image-20200420123257444" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420163656675.png" alt="image-20200420163656675" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420163734278.png" alt="image-20200420163734278" style="zoom:50%;" />



</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/04/11/System-Design/9p/2019-04-11-Distributed%20System%20GFS/">System-Design/9p/2019-04-11-Distributed System GFS</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-11</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/SystemDesign/">SystemDesign</a></span><div class="content"><h1 id="General"><a href="#General" class="headerlink" title="General"></a>General</h1><ul>
<li>GFS Client</li>
<li>Heart Beat</li>
</ul>
<blockquote>
<p>GFS 采用的是 Master-slave 的架构。他的一些基本设计原理，如 chunk 的设计（对应文件系统中的 block）和普通文件系统是相通的。整套系统的设计是基于一些合理假设的（这些假设都是Google从大量工程实践中总结出来的），其中一条假设就是：“We expect a few million files, each typically 100MB or larger in size”。如果实在需要存很多小文件的话， 其实可以将这些小文件打包成一个大文件然后再存储，只要记录下每个小文件在文件内部的起始字节和大小就行了。</p>
</blockquote>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="要解決的問題"><a href="#要解決的問題" class="headerlink" title="要解決的問題"></a>要解決的問題</h2><ul>
<li><p>Storage不够、QPS太大的問題</p>
</li>
<li><p>Google 很多小計算機建DFS</p>
<p>ＶＳ</p>
<p>SUN大型、貴的（大家買不起），09年被Oracle收了</p>
</li>
<li><p>GFS、BigTable、MapReduce</p>
</li>
</ul>
<h1 id="GFS"><a href="#GFS" class="headerlink" title="GFS"></a>GFS</h1><h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><ul>
<li>作為一個文件系統，一定要提供兩種常用的操作：寫跟讀<ul>
<li>寫：要文件名、內容</li>
<li>讀：文件名，返回文件內容</li>
</ul>
</li>
</ul>
<h3 id="需求1-總存储量有多大"><a href="#需求1-總存储量有多大" class="headerlink" title="需求1: 總存储量有多大?"></a>需求1: 總存储量有多大?</h3><ul>
<li>比如 &gt; 1000T，才會上分布式；機器數也是越大越好</li>
</ul>
<h3 id="需求2-多台機器，越多越好"><a href="#需求2-多台機器，越多越好" class="headerlink" title="需求2: 多台機器，越多越好"></a>需求2: 多台機器，越多越好</h3><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><ul>
<li><p>如圖書館要有Server、Client。</p>
<p>GFS提供的一個就是讀取，另一個是寫入的服務</p>
</li>
</ul>
<h3 id="Q-多台Server間怎溝通？"><a href="#Q-多台Server間怎溝通？" class="headerlink" title="Q: 多台Server間怎溝通？"></a>Q: 多台Server間怎溝通？</h3><h4 id="P2P"><a href="#P2P" class="headerlink" title="P2P"></a>P2P</h4><p>平級溝通</p>
<ul>
<li>優：一台掛了還可以工作，沒有單點故障問題</li>
<li>缺：但，大家平級，要常通信保持一致性。</li>
<li>p2p的通信一般是比較難寫的，直覺就是。</li>
</ul>
<h4 id="V-Master-vs-Slaves"><a href="#V-Master-vs-Slaves" class="headerlink" title="(V) Master vs Slaves"></a>(V) Master vs Slaves</h4><p>老大一致對外，對內分配幹活</p>
<ul>
<li><p>優: 數據易<strong>保持一致性</strong>。就老大分配</p>
</li>
<li><p>缺：有單點大哥master掛了，整個系統不work的問題</p>
</li>
<li><p>這是GFS最終的選擇，他是後端的服務，不是前端的，所以掛個一分鐘不會怎樣，就如果master掛了，把master重啟就好了。</p>
</li>
</ul>
<blockquote>
<h5 id="单选题-大家猜猜GFS会用哪种设计模式？"><a href="#单选题-大家猜猜GFS会用哪种设计模式？" class="headerlink" title="单选题]大家猜猜GFS会用哪种设计模式？"></a>单选题]大家猜猜GFS会用哪种设计模式？</h5><p>A.社会主义 P2P23.06% 选择</p>
<p>B.资本主义 Master Slave76.94% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是A</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>Master Slave的优势是：<br>设计简单<br>数据很容易保持一致</p>
</blockquote>
<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><h3 id="Q-大文件存在哪？"><a href="#Q-大文件存在哪？" class="headerlink" title="Q: 大文件存在哪？"></a>Q: 大文件存在哪？</h3><ul>
<li><p>當然是disk，10PB只有disk可以，內存不可能啦</p>
</li>
<li><p>如何存到這個文件系統裡？</p>
<ul>
<li>怎麼設計GFS?</li>
<li>怎麼存？如果總量 &lt; 100G？</li>
</ul>
</li>
</ul>
<blockquote>
<h5 id="单选题-在-GFS-中，大文件存在哪儿？"><a href="#单选题-在-GFS-中，大文件存在哪儿？" class="headerlink" title="[单选题]在 GFS 中，大文件存在哪儿？"></a>[单选题]在 GFS 中，大文件存在哪儿？</h5><p>A.内存中，读写快！5.03% 选择</p>
<p>B.硬盘中，超大空间！94.97% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
</blockquote>
<h3 id="Q-Metadata存哪？"><a href="#Q-Metadata存哪？" class="headerlink" title="Q: Metadata存哪？"></a>Q: Metadata存哪？</h3><ul>
<li>metadata元數據常常被無意識訪問到, 怎麼存好？一般打開文件夾就被動看到了</li>
</ul>
<blockquote>
<h5 id="单选题-Meta-Data应该怎样储存？"><a href="#单选题-Meta-Data应该怎样储存？" class="headerlink" title="[单选题]Meta Data应该怎样储存？"></a>[单选题]Meta Data应该怎样储存？</h5><p><strong>A.所有文件的Meta Data 全放磁盘开头83.51% 选择</strong></p>
<p>B.每个文件的Meta Data都和它的内容放在一起16.49% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
<p><strong>正确答案:</strong>A</p>
<p><strong>解析:</strong></p>
<p>全放磁盘开头可以减少磁盘的寻轨时间</p>
</blockquote>
<p>理由跟硬盤的結構有關，機械硬盤的磁頭到軌道有尋軌時間的，一般磁頭跳到要找的位置要10ms</p>
<p>如果是選B，磁頭就要一直跳，100個文件就要10ms*100就一秒了。</p>
<p>A可以一次全讀出去。</p>
<ul>
<li>文件內容怎麼放呢？在磁盤中也有兩種方式：就是要不要把每個文件拆成各小塊交錯放呢？</li>
</ul>
<blockquote>
<h5 id="单选题-文件内容应怎样储存？"><a href="#单选题-文件内容应怎样储存？" class="headerlink" title="[单选题]文件内容应怎样储存？"></a>[单选题]文件内容应怎样储存？</h5><p>A.文件整体存储，所有文件连在一起21.44% 选择</p>
<p>B.将每个文件等分成很多小块后再存78.56% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>等分成很多小块后存储可以方便文件的修改操作</p>
</blockquote>
<p>如果選A的話，當2號文件是10KB這麼大，寫入後變100KB了，怎麼寫？還得把2號先砍了，再往後找一個夠100KB大小的空間寫進去。</p>
<p>選B的話，文件本來就是一小塊一小塊了，<strong>後面要增加90KB的文件就順勢往後寫就OK了</strong>。</p>
<p>一個硬盤就是一個非常大的array；</p>
<p>一般NTFS(win)、XFS(linux)的文件系統裡的默認Block是　<strong><em>4KB</em></strong></p>
<ul>
<li><p>本來的100G如果變成了100TB會遇到什麼問題？</p>
<ul>
<li><p>Block的數量會超多。要拿來儲存block的空間就已經非常大了！</p>
<ul>
<li>100x1024G = 100x 1024x1024M = 100x1024x1024x1024KB = 25x1024x1024x1024 <strong>blocks</strong></li>
</ul>
</li>
<li><p>怎麼改進？ ==&gt; 增加Block的大小。可能就是變成64MB，然後把這個改名叫<strong>Chunk</strong></p>
<ul>
<li><p>優：Reduce size of Metadata，就是筆數變少了</p>
</li>
<li><p>劣：浪費了些小空間，如果我要存的就是個小文章４MB，那也要拿64MB去存，60MB就變成了磁盤碎片浪費了。但就這樣吧。。</p>
<p> 結論：Z &gt; B . </p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>对于这个trade off，我感觉十分牵强啊。增加chunk size是为了减少meta data 的数量，但是试想，如果存的文件全是非常非常小，都是几百kb或者几mb的，那这样岂不是更浪费空间，造成大量碎片？<ul>
<li>这个在GFS的paper里面有解释，<strong><em>GFS的设计是基于一些假设的（这些假设都是Google从大量工程实践中总结出来的</em></strong>），其中一条假设就是：“We expect a few million files, each typically 100MB or larger in size”。这是其一。另外，如果实在需要存很多小文件的话， 其实可以将这些小文件打包成一个大文件然后再存储，只要记录下每个小文件在文件内部的起始字节和大小就行了。</li>
</ul>
</li>
<li>如果在写新文件时发现之前预先留给metadata的空间被用完了怎么办呢？<ul>
<li>这时候应该就写不进去了 生产环境应该及时关注磁盘容量告警</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Q-100G-on-普通的OS怎存"><a href="#Q-100G-on-普通的OS怎存" class="headerlink" title="Q: 100G on 普通的OS怎存?"></a>Q: 100G on 普通的OS怎存?</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzvx48y5vj30vc0g6dhp.jpg" alt="image-20200822205723260"></p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzvxy7io1j30c607edgd.jpg" alt="image-20200822205814035" style="zoom: 33%;" />



<ol>
<li>因为比较小，預設就是4KB，一般就是照block傳就好了</li>
<li>如果再大一點怎辦？如果再按block算，就會有很多很多個block，去尋執很麻煩，不然我們就by chunk</li>
<li>如果100P就要上多台機子了，P = 1000T</li>
</ol>
<p>當然指的不是一個，而是很多個文件啦</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzvz8ufr8j31160hwtb8.jpg" alt="image-20200822205929382"></p>
<h3 id="Q-100P-真的很大怎存？"><a href="#Q-100P-真的很大怎存？" class="headerlink" title="Q: 100P, 真的很大怎存？"></a>Q: 100P, 真的很大怎存？</h3><p>上分布式</p>
<h2 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzw5mlv1rj317o0l441y.jpg" alt="image-20200822210535805"></p>
<h3 id="Q-10PB存得下嗎？需要多台電腦-Master-Slave工作模式。"><a href="#Q-10PB存得下嗎？需要多台電腦-Master-Slave工作模式。" class="headerlink" title="Q: 10PB存得下嗎？需要多台電腦 Master-Slave工作模式。"></a>Q: 10PB存得下嗎？需要多台電腦 Master-Slave工作模式。</h3><ul>
<li>現在一台服務器現在最多插10個硬盤，一個硬盤撐死就100TB，這樣也就才0.1P，這樣如此的電腦至少要來100台共同工作才能做得了10PB的事</li>
</ul>
<blockquote>
<h5 id="单选题-在-GFS-中-Master-和-Slave-分别存什么数据？"><a href="#单选题-在-GFS-中-Master-和-Slave-分别存什么数据？" class="headerlink" title="[单选题]在 GFS 中 Master 和 Slave 分别存什么数据？"></a>[单选题]在 GFS 中 Master 和 Slave 分别存什么数据？</h5><p>A.Master 存 Metadata，Slave 存实际的文件内容96.81% 选择</p>
<p>B.Master 存实际的文件内容，Slave 存 Metadata3.19% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
</blockquote>
<p>Master存了所有的chunk該在哪台小弟的info</p>
<p><strong><em>所以就是slave就是chunk server</em></strong></p>
<blockquote>
<p>[单选题]GFS Master 是否有必要存储每个 chunk 在 Slave Server 上的 Offset?</p>
<p>你的选择:B</p>
<p>A:有必要</p>
<p>B:没有必要</p>
<p>答对了，您选择的答案是B</p>
</blockquote>
<p>這樣可減輕master壓力；而且slave裡面可以自己調整位置不用通跟master通信。</p>
<p><em>１chunk = <strong>64MB need 64B的metadata(經驗值)</strong>, 10PB needs 10G metadata, 存內存都可以！</em></p>
<blockquote>
<ul>
<li>如果master 挂了 只能恢复master ？ 不能promote slave server 变成master？<ul>
<li>GFS 里的 Master 的概念和 MySQL 里的 MasterSlave 的概念不同。GFS 里的 Master 只负责管理，不负责数据存储。MySQL 里的 Master Slave 都只负责数据存储不负责管理。要注意区分。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Q-每台-chunk-的Offset偏移量可否不在Master上？"><a href="#Q-每台-chunk-的Offset偏移量可否不在Master上？" class="headerlink" title="Q: 每台 chunk 的Offset偏移量可否不在Master上？"></a>Q: 每台 chunk 的Offset偏移量可否不在Master上？</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzw87abomj31660k07ei.jpg" alt="image-20200822210804854"></p>
<h3 id="Q-Master-存10P文件的metadata要多少容量？"><a href="#Q-Master-存10P文件的metadata要多少容量？" class="headerlink" title="Q: Master 存10P文件的metadata要多少容量？"></a>Q: Master 存10P文件的metadata要多少容量？</h3><p>1 chunk = 64MB needs 64B. (经验值) 10P needs 10G</p>
<h1 id="One-Workable-Sol"><a href="#One-Workable-Sol" class="headerlink" title="One Workable Sol!"></a>One Workable Sol!</h1><h2 id="寫"><a href="#寫" class="headerlink" title="寫"></a>寫</h2><h3 id="架構-amp-Q-一個chunk怎麼寫入server的"><a href="#架構-amp-Q-一個chunk怎麼寫入server的" class="headerlink" title="架構 &amp; Q: 一個chunk怎麼寫入server的?"></a>架構 &amp; Q: 一個chunk怎麼寫入server的?</h3><p>這個還沒有做scale</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzwhdnk3dj312y0is11s.jpg" alt="image-20200822211653883"></p>
<h4 id="Q-寫入怎麼寫好？一次vs多次"><a href="#Q-寫入怎麼寫好？一次vs多次" class="headerlink" title="Q: 寫入怎麼寫好？一次vs多次"></a>Q: 寫入怎麼寫好？一次vs多次</h4><blockquote>
<h5 id="单选题-怎么将文件写入GFS"><a href="#单选题-怎么将文件写入GFS" class="headerlink" title="[单选题]怎么将文件写入GFS?"></a>[单选题]怎么将文件写入GFS?</h5><p>A.将文件整体一次性写入3.39% 选择</p>
<p>B.将文件拆分成块多次写入96.61% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>不同的块有可能放到不同的Chunk Server，并且分块后方便重传</p>
</blockquote>
<h4 id="Q-從哪續傳好-當多次寫時每份大小？就Chunk，也是傳輸size"><a href="#Q-從哪續傳好-當多次寫時每份大小？就Chunk，也是傳輸size" class="headerlink" title="Q: 從哪續傳好? 當多次寫時每份大小？就Chunk，也是傳輸size"></a>Q: 從哪續傳好? 當多次寫時每份大小？就Chunk，也是傳輸size</h4><p>如果斷開了，可以從哪斷開的從哪續傳</p>
<blockquote>
<h5 id="单选题-GFS-Client-将文件拆分为多大进行传输比较合适？"><a href="#单选题-GFS-Client-将文件拆分为多大进行传输比较合适？" class="headerlink" title="[单选题]GFS Client 将文件拆分为多大进行传输比较合适？"></a>[单选题]GFS Client 将文件拆分为多大进行传输比较合适？</h5><p>A.64k6.92% 选择</p>
<p>B.1M5.73% 选择</p>
<p><strong>C.64M83.89% 选择</strong></p>
<p>D.1G3.46% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是C</p>
<p><strong>正确答案:</strong>C</p>
<p><strong>解析:</strong></p>
<p>64M 是一个 chunk 的大小。GFS 是按照 chunk 为单位进行存储的，所以 64M 为一组比较合适。</p>
</blockquote>
<p>傳輸單位也就是chunk。</p>
<blockquote>
<h5 id="单选题-GFS中每一个Chunk怎么写入Server？"><a href="#单选题-GFS中每一个Chunk怎么写入Server？" class="headerlink" title="[单选题]GFS中每一个Chunk怎么写入Server？"></a>[单选题]GFS中每一个Chunk怎么写入Server？</h5><p>A.把文件传给master，让Master处理所有的事情6.81% 选择</p>
<p>B.仅让Master分配Chunk Server，然后直接把Chunk传给相应的Chunk Server93.19% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p><strong><em>避免Master成为瓶颈</em></strong></p>
</blockquote>
<p>master的硬盤有限，網路也是有限的，自己會卡死，分下去給小弟們，讓client間也不會排隊</p>
<h4 id="Q-要修改一個-mp4-怎辦？"><a href="#Q-要修改一個-mp4-怎辦？" class="headerlink" title="Q: 要修改一個 .mp4 怎辦？"></a>Q: 要修改一個 .mp4 怎辦？</h4><ol>
<li>先刪掉 /gfs/home/xxx.mp4</li>
<li>重把整個文件寫一份</li>
</ol>
<h5 id="結論就-別update"><a href="#結論就-別update" class="headerlink" title="結論就: 別update!"></a>結論就: 別update!</h5><h3 id="總結"><a href="#總結" class="headerlink" title="總結:"></a>總結:</h3><p><strong><em>Master要存所有的metadata,  chunkserver存真正的大data 讀要找到對應的chunkserver，寫時要找到空閒的chunkserver；</em></strong></p>
<ul>
<li><strong>寫入是每次找老大問，老大分配空間，自己去找小弟寫</strong></li>
<li><strong>讀出是問老大拿到chunklist，然後去問小弟拿到chunk就ok</strong></li>
</ul>
<blockquote>
<ul>
<li>請問gfs讀取file的話, client會從master得到一個chunk list, 那我並行讀取每個chunk,還是一個一個循序讀取chunk? 有什麼比較快的讀取方法嗎?<ul>
<li>GFS中，Chunk分散储存在若干个Chunk Server中，读取的时候Client可以从不同的Chunk Server同时读取Chunk，相当于有一个并行的效果</li>
</ul>
</li>
<li>The client is the end user side, or another proxy? I don’t think the end user client should worry about which chunk server to write in.<ul>
<li>The “client” mentioned here is a library that is linked to the end user’s application, serving as an <strong><em>abstraction layer between the application and the underlying GFS</em></strong>.</li>
</ul>
</li>
<li>client读取档案时，master server是给他一个chunk list，那client写入档案时，master是给他一个chunk list让他依次写入，还是一个写完master再告知下一个chunk该分配到那个chunk server?<ul>
<li>给一个 chunk list 让他依次写入。否则 master 和 client 之间通信太多很负累，也没啥意义。</li>
</ul>
</li>
<li>client 定义是什么 ？是end user吗？ 还是只是gfs 跟外界的一个interface？<ul>
<li>gfs 和外界的一个 interface。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="讀"><a href="#讀" class="headerlink" title="讀"></a>讀</h2><h4 id="Q-一次-or-多次？"><a href="#Q-一次-or-多次？" class="headerlink" title="Q: 一次 or 多次？"></a>Q: 一次 or 多次？</h4><h4 id="Q-Client-怎知-xxx-mp4-被切成了多少塊？"><a href="#Q-Client-怎知-xxx-mp4-被切成了多少塊？" class="headerlink" title="Q: Client 怎知 xxx.mp4 被切成了多少塊？"></a>Q: Client 怎知 xxx.mp4 被切成了多少塊？</h4><p>每個chunk知道在哪個server，所以一個file知道分別位在哪些server</p>
<h3 id="架構"><a href="#架構" class="headerlink" title="架構"></a>架構</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzwmbpejsj314w0k6424.jpg" alt="image-20200822212139023"></p>
<h2 id="SCALE-–-in-One-Workable-Sol"><a href="#SCALE-–-in-One-Workable-Sol" class="headerlink" title="SCALE –  in One Workable Sol"></a>SCALE –  in One Workable Sol</h2><h3 id="Master-做的事"><a href="#Master-做的事" class="headerlink" title="Master 做的事"></a>Master 做的事</h3><h4 id="Q-Master-Task"><a href="#Q-Master-Task" class="headerlink" title="Q: Master Task:"></a>Q: Master Task:</h4><ul>
<li>存儲各個文件數據的 metadata</li>
<li>存儲 Map<ul>
<li>讀取時找到對應的 Chunk Server, </li>
<li>寫入時分配空閒三 Chunk Server</li>
</ul>
</li>
</ul>
<h4 id="Q-單master夠嗎？"><a href="#Q-單master夠嗎？" class="headerlink" title="Q: 單master夠嗎？"></a>Q: 單master夠嗎？</h4><ul>
<li>90%很好了一般都是這樣；大不了就是 Paxos Alg. 的多Master，再多也受不了會有延遲</li>
</ul>
<h5 id="Double-Master"><a href="#Double-Master" class="headerlink" title="Double Master"></a>Double Master</h5><ul>
<li>Paper: Apache Hadoop Goes Realtime at Facebook</li>
</ul>
<h5 id="Multi-Master"><a href="#Multi-Master" class="headerlink" title="Multi Master"></a>Multi Master</h5><ul>
<li>Paper: Paxos Algorithm</li>
</ul>
<h4 id="Q-怎麼看資料有掛了"><a href="#Q-怎麼看資料有掛了" class="headerlink" title="Q: 怎麼看資料有掛了"></a>Q: 怎麼看資料有掛了</h4><ul>
<li><p>CHECKSUM, md5  哈希，原串發生變化，哈希值就巨大變化，一旦不同，就是原數據必毀</p>
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdtqklpedij31260tun45.jpg" alt="image-20200414235740148" style="zoom: 25%;" />



</li>
</ul>
<ul>
<li><p>也可以用 XOR作checksum, </p>
</li>
<li><p>也可以用SHA1, SHA256, SHA512</p>
</li>
<li><p>Read More: <a href="https://en.wikipedia.org/wiki/Checksum" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Checksum</a></p>
</li>
</ul>
<h5 id="Checksum"><a href="#Checksum" class="headerlink" title="Checksum"></a>Checksum</h5><ul>
<li>size: 也就 4Bytes</li>
<li>時機</li>
</ul>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200822213411583.png" alt="image-20200822213411583" style="zoom: 50%;" />

<blockquote>
<h5 id="单选题-什么时候写入-checksum-如上圖"><a href="#单选题-什么时候写入-checksum-如上圖" class="headerlink" title="[单选题]什么时候写入 checksum? 如上圖"></a>[单选题]什么时候写入 checksum? 如上圖</h5><p>A.每个一段时间遍历所有数据计算 checksum 并写入3.32% 选择</p>
<p>B.每个 chunk 在写入的时候计算 checksum 并记录在 chunk 的末尾39.40% 选择</p>
<p>C.每个 chunk 在写入的时候计算 checksum 并集中记录在当前的 Slave 上36.58% 选择</p>
<p>D.每个 chunk 在写入的时候计算 checksum 并集中记录在 Master 上20.70% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
</blockquote>
<blockquote>
<h5 id="单选题-一般来说什么时候检查-checksum？"><a href="#单选题-一般来说什么时候检查-checksum？" class="headerlink" title="[单选题]一般来说什么时候检查 checksum？"></a>[单选题]一般来说什么时候检查 checksum？</h5><p>A.每次读取 chunk 的时候重新计算并对比以前的 checksum90.56% 选择</p>
<p>B.周期性的遍历所有数据检查 checksum9.44% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
<p><strong>正确答案:</strong>A</p>
</blockquote>
<p>而且還可以再週期性地檢查；是根據業務調整。</p>
<h4 id="問題集"><a href="#問題集" class="headerlink" title="問題集"></a>問題集</h4><blockquote>
<ul>
<li>master down了重启， 是通过读log来恢复吗<ul>
<li>是的，但是一般会定期制作一个checkpoint，挂掉之后只需要从上一个checkpoint开始重放log就行了，不需要每次都从头开始。</li>
</ul>
</li>
<li>master down了之后怎么恢复？<ul>
<li>master down了之后重启就好</li>
</ul>
</li>
<li>老师请问还有其他的加密算法么？还是只能用这个MD5<ul>
<li>有啊，网上搜一大堆。比如 sha1 sha2, sha256.</li>
</ul>
</li>
<li>Check Sum 检查一位错误的例子，如果刚好有两个数据发生改变，是有可能导致xor结果不变的吧。这种情况我们就不知道有文件损坏了？<ul>
<li>是的。check sum 本来就是 false positive 的。你说的这种情况出现概率是很低的。系统设计的领域里面有很多允许 false postive 或者 false negative 的情况，如 BloomFilter 就是一个例子。这些场景下，我们都不能保证 100% work，但是高概率是有效的。这个系统设计区别于算法设计的很大的不同，要注意体会这个地方。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="更多-Q-amp-A"><a href="#更多-Q-amp-A" class="headerlink" title="更多 Q&amp;A"></a>更多 Q&amp;A</h2><h4 id="How-to-avoid-data-loss-when-a-Chunk-Server-is-down-fail"><a href="#How-to-avoid-data-loss-when-a-Chunk-Server-is-down-fail" class="headerlink" title="How to avoid data loss when a Chunk Server is down/fail?"></a>How to avoid data loss when a Chunk Server is down/fail?</h4><p>​    Ans: replica 作備份</p>
<blockquote>
<p>​    [单选题]GFS的Replica怎么存放？</p>
<p>1.三个备份都放在一个地方（加州）0.97% 选择</p>
<p>2.三个备份放在三个相隔较远的地方（加州，滨州，纽约州）9.99% 选择</p>
<p>3.两个备份相对比较近，另一个放在较远的地方（2个加州，1个滨州）89.04% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是3</p>
<p><strong>正确答案:</strong>3</p>
<p><strong>解析:</strong></p>
<p>两个备份较近，保证出错时快速恢复，一个较远，保证安全性</p>
</blockquote>
<h4 id="How-to-recover-when-a-chunk-is-broken"><a href="#How-to-recover-when-a-chunk-is-broken" class="headerlink" title="How to recover when a chunk is broken?"></a>How to recover when a chunk is broken?</h4><p>讓Master 幫, Master知道他所有的小弟在哪，</p>
<ol>
<li><h5 id="架構-1"><a href="#架構-1" class="headerlink" title="架構"></a>架構</h5></li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzx20e5tfj31120nwgor.jpg" alt="image-20200822213644383"></p>
<h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4><h4 id="如何知道有個小弟完全掛了？-gt-心跳，小弟自報"><a href="#如何知道有個小弟完全掛了？-gt-心跳，小弟自報" class="headerlink" title="如何知道有個小弟完全掛了？==&gt; 心跳，小弟自報"></a>如何知道有個小弟完全掛了？==&gt; 心跳，小弟自報</h4><blockquote>
<h5 id="单选题-心跳（Heartbeat）机制怎么设计？"><a href="#单选题-心跳（Heartbeat）机制怎么设计？" class="headerlink" title="[单选题]心跳（Heartbeat）机制怎么设计？"></a>[单选题]心跳（Heartbeat）机制怎么设计？</h5><p>A.Master 轮询 Chunk Server13.62% 选择</p>
<p>B.Chunk Server 主动向 Master 汇报86.38% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>主动汇报可以减少通信次数，就一次，不然要兩次。</p>
</blockquote>
<h2 id="Scale-再更多-Q-amp-A"><a href="#Scale-再更多-Q-amp-A" class="headerlink" title="Scale 再更多 Q&amp;A"></a>Scale 再更多 Q&amp;A</h2><h4 id="Q-寫到一台server安全嗎？"><a href="#Q-寫到一台server安全嗎？" class="headerlink" title="Q: 寫到一台server安全嗎？"></a>Q: 寫到一台server安全嗎？</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzx6c0e7bj31180i8n17.jpg" alt="image-20200822214053264"></p>
<h4 id="Q-解決客戶端瓶頸，不然client分別去寫他太累了"><a href="#Q-解決客戶端瓶頸，不然client分別去寫他太累了" class="headerlink" title="Q: 解決客戶端瓶頸，不然client分別去寫他太累了"></a>Q: 解決客戶端瓶頸，不然client分別去寫他太累了</h4><h5 id="讓隊長去-寫怎麼寫？client-把chunk一次傳給三個小弟作replica後，client會變成瓶頸…"><a href="#讓隊長去-寫怎麼寫？client-把chunk一次傳給三個小弟作replica後，client會變成瓶頸…" class="headerlink" title="讓隊長去! 寫怎麼寫？client 把chunk一次傳給三個小弟作replica後，client會變成瓶頸…"></a>讓隊長去! 寫怎麼寫？client 把chunk一次傳給三個小弟作replica後，client會變成瓶頸…</h5><p>隊長再去寫給另兩個人</p>
<blockquote>
<h5 id="单选题-如何解决-client-传输-replica-chunk-的问题？"><a href="#单选题-如何解决-client-传输-replica-chunk-的问题？" class="headerlink" title="[单选题]如何解决 client 传输 replica chunk 的问题？"></a>[单选题]如何解决 client 传输 replica chunk 的问题？</h5><p>A.client 将 chunk 传给 master，由 master 去纷发到 3 台 chunk server6.24% 选择</p>
<p>B.client 将 chunk 传给其中一台 chunk server，然后由这台 chunk server 再传给另外的 2 台63.29% 选择</p>
<p>C.client 将 chunk 传给其中一台 chunk server，另外的两个 chunk server 空了再慢慢传30.47% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
</blockquote>
<p>內網自己傳肯定快得多</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzx9qdmg2j311a0isadu.jpg" alt="image-20200822214408981"></p>
<h4 id="Q-怎麼選隊長？"><a href="#Q-怎麼選隊長？" class="headerlink" title="Q: 怎麼選隊長？"></a>Q: 怎麼選隊長？</h4><ol>
<li>可找最近的 (快)</li>
<li>找現在沒在幹活的 (平衡)</li>
</ol>
<blockquote>
<h5 id="多选题-下列哪些因素是我们挑选-chunk-server-队长时所需要考虑的"><a href="#多选题-下列哪些因素是我们挑选-chunk-server-队长时所需要考虑的" class="headerlink" title="[多选题]下列哪些因素是我们挑选 chunk server 队长时所需要考虑的"></a>[多选题]下列哪些因素是我们挑选 chunk server 队长时所需要考虑的</h5><p>A.机器的繁忙程度38.48% 选择</p>
<p>B.距离的远近36.78% 选择</p>
<p>C.剩余存储空间的大小13.27% 选择</p>
<p>D.CPU 的个数11.47% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是B</p>
<p><strong>正确答案:</strong>AB</p>
</blockquote>
<blockquote>
<h5 id="单选题-每次找的-chunk-server-队长是一样的么？"><a href="#单选题-每次找的-chunk-server-队长是一样的么？" class="headerlink" title="[单选题]每次找的 chunk server 队长是一样的么？"></a>[单选题]每次找的 chunk server 队长是一样的么？</h5><p>A.是1.55% 选择</p>
<p>B.不是20.48% 选择</p>
<p>C.不一定77.97% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是C</p>
<p><strong>正确答案:</strong>C</p>
</blockquote>
<h4 id="Q-解決-Chunk-Server-Failure"><a href="#Q-解決-Chunk-Server-Failure" class="headerlink" title="Q: 解決 Chunk Server Failure"></a>Q: 解決 Chunk Server Failure</h4><p>如果有人掛了，隊長知道了，它會跟master說不要再讓哪台能「被」分配東西了</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzxbqyp6oj313q0kmq6f.jpg" alt="image-20200822214605591"></p>
<ul>
<li>如果沒寫上就一直試，如果太多次，就算了吧大家都完了，都掛了也不大可能</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzxbzzrrhj31320jyac9.jpg" alt="image-20200822214620405"></p>
<h2 id="總結-1"><a href="#總結-1" class="headerlink" title="總結"></a>總結</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ghzxdft2fmj30yy0u079h.jpg" alt="image-20200822214743151"></p>
<h2 id="GFS-Problem"><a href="#GFS-Problem" class="headerlink" title="GFS Problem"></a>GFS Problem</h2><p><a href="https://www.jiuzhang.com/qa/627/" target="_blank" rel="noopener">https://www.jiuzhang.com/qa/627/</a></p>
<blockquote>
<ul>
<li><strong>设计一个只读的lookup service. 后台的数据是10 billion个key-value pair, 服务形式是接受用户输入的key，返回对应的value。</strong></li>
</ul>
<p><strong>已知每个key的size是0.1kB，每个value的size是1kB。要求系统qps &gt;= 5000，latency &lt; 200ms.</strong></p>
<ul>
<li><p>key: 100; value: 1000個ascii字</p>
</li>
<li><p>server性能参数需要自己问，我当时只问了这些，可能有需要的但是没有问到的……<br>commodity server<br>8X CPU cores on each server<br>32G memory<br>6T disk</p>
<p>使用任意数量的server，设计这个service。</p>
<p>就不发我的解法了，真的很渣……=。=|||</p>
</li>
<li><p>我总结了SG_SWE_GM以及其他同学的解答，在此基础上我的想法如下，有问题的地方还请老师同学指正，</p>
<p>=&gt; total key size ~ 10 billion * 0.1kB = 1T        ==&gt; 40台*32G 查起來就是很快<br>=&gt; total value size ~ 10 billion * 1kB = 10T<br>所以每台服务器用两块硬盘，共12T。数据结构用SSTable就好了。</p>
<p>充分利用内存，本来我想用binary search tree做index，但是仔细想想这个服务是只读的，而且硬盘存储键值对用的是SSTable是有序的，key和value长度又是固定的，所以直接把key以有序的方式存在内存就好了，查询的时候对key进行binary search，然后用key在内存中的offset来计算键值对在硬盘中的offset。1T/32G = 31.25. 所以一共需要32台服务器的内存分担key index。前面加一个master负责管理consistent hasing。lg(32G) = 35, 平均查询一个key就算18次内存访问，大约才1800ns，在ms这个量级上可以忽略。</p>
<p>每一次request，在硬盘上读取1kB value的时间：<strong><em>10ms(disk seek)</em></strong> + 4ms(rotation delay for 7200rpm) + 1kB/1MB * <strong><em>30ms(reading 1kB sequentially from disk)</em></strong> = 14ms. 目前一台server能处理的的QPS: 1000ms/14ms = 71, 总的QPS: 71 * 32 = 2272。距离要求还有两倍多的差距。所以我们可以给每台server装上6个6T硬盘，组成3套数据硬盘，3套硬盘可以并行处理3个请求，这样也算是稍微利用了一下8X的多核CPU。这时QPS即为2272 * 3=6816.</p>
<p>延迟：</p>
<ol>
<li>master内存查找consistent hashing map的时间：忽略</li>
<li>master与slave的round trip delay：1 round trip in the same data center is 1ms.</li>
</ol>
</li>
</ul>
<ol start="3">
<li><p>slave内存index查询时间：忽略</p>
<ol start="4">
<li>slave硬盘读取时间：14ms</li>
</ol>
<p>so total latency is 15ms。</p>
</li>
</ol>
</blockquote>
<ul>
<li>企業現在一般怎麼配server?<ul>
<li>16G、32G、64G、256G、1T、3T，一般不會直接用</li>
<li>Disk: </li>
</ul>
</li>
<li>300ms是在尋道</li>
<li>0.5 m 是round trip在同個data center間，比在硬盤上找還是快多了</li>
<li><strong><em>30ms(reading 1kB sequentially from disk)</em></strong><ul>
<li>是因為硬盤估計每秒可讀30MB的數據，所以1MB就是30ms；這個讀了１kB的話就0.03 ms跟disk seek比起來可以不計。</li>
</ul>
</li>
<li>QPS on 1 server : 1s/10ms 次（一秒一台可以100次。） * 2disk = 200次</li>
<li>5000個QPS/200就需要25台服務器</li>
</ul>
<h4 id="Latency"><a href="#Latency" class="headerlink" title="Latency:"></a>Latency:</h4><ol>
<li>找到key : 硬盤中作二分查找，每次的主要的是找的時間。</li>
<li>讀到value很小，每次的可以忽略</li>
</ol>
<ul>
<li>Sol1.</li>
</ul>
<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200412220508731.png" alt="image-20200412220508731" style="zoom:50%;" />

<ul>
<li><input disabled="" type="checkbox"> 分析：log該以2為底，所以就變30 * 10ms(每動一次就要10ms) 就是 300ms, 超過題目規定的200ms，找到key的工作只能在硬盤上做，而且單個硬盤不能並行執行，所以一次query 至少要300ms了，一個硬盤１秒內只能做三次，兩個只能做六次，所以要5000QPS要至少約1000台服務器。這跟他給出的25台差很多！</li>
<li><input disabled="" type="checkbox"> 我們最希望減少的就是300ms的<strong><em>查詢時間</em></strong>；<strong><em>而我們未用上他的內存</em></strong>，一台有32G，<strong><em>40</em></strong>台就有超1TB，就跟所有的數據key量一樣了，所以提示了可以在內存作操作，把所有key都存過去。如果內存中有個內存到硬盤位置的映射的話…！</li>
<li><input disabled="" type="checkbox"> 一個key 0.1kB , 一個position 8Byte，所以一筆仍是0.1kB, 10個billion也是用1TB的內存。所以40台的內存並一起變一個大內存裡直接二分查找就快，每次的時間比硬盤的10ms少到幾乎可不計。</li>
<li><input disabled="" type="checkbox"> 內存去對應硬盤的position。</li>
<li><input disabled="" type="checkbox"> 硬盤二分查找每次要花10ms (disk seek)，但內存不用時間。雖一樣是花30次。所以300ms就省了。所以整體10ms + 0.5ms 就10.5ms。</li>
<li><input disabled="" type="checkbox"> 現在已有40台機器，每台有兩個硬盤，而且每台機器的硬盤可以存下全量的數據 ( 就key啦，就 1T而已)，就是說整體的數據一共有40份拷貝，每個硬盤要花約10ms查找一次，一台可以200次操作(一台有兩顆disk)，40台可以並行作，所以就是<strong>8000QPS &gt; 5000QPS!</strong></li>
<li><input disabled="" type="checkbox"> 總結：一共４０台機器，內存就是４０台合併起來當一大塊用，內存大小是１TB；它存放的是key到硬盤中position的數據。每台兩個硬盤，硬盤中存的是全量的數據，一次查找的過程就是，首先通過整體的內存找到在硬盤上的某個位置，均衡負載到40台的某個機器上，讀它的key, value，因為在內存查的時間可以不計，所以最後的延遲就在disk seek時間10ms還有整體在網路上傳輸的0.5ms。</li>
</ul>
<blockquote>
<ul>
<li>key 1T value 10T 机器6T 为啥一台机器的硬盘可以存下全部的信息？<ul>
<li>key 1T + value 10T = 11T<br>一块硬盘6T，一台机器两块硬盘即可存下</li>
</ul>
</li>
<li>为什么一个硬盘能存全量数据？一个硬盘是6TB，key加value需要11TB。如果是两块硬盘存下所有数据，那应该就是100次，为什么是100*2？<ul>
<li>是2块硬盘存下所有数据，每块存一半。100 * 2 的意思是，你有 2 块一瓶，一块硬盘能够提供 100 IOPS，两块就是 200 IOPS，因为你可以并发嘛。</li>
</ul>
</li>
<li>请问，内存是在不同的40台机器上，如何垮机器二分查找呢？机器间通信延迟怎么可以忽略呢？<ul>
<li>40台机器的内存合并起来看成一个大内存。打个比方，一次查找时可以先去编号20的机器上看，如果大了，再去10号机器上看，如果大了，再去5号机器上看。。。。依次类推。40台机器接在一个交换机（switch）上，通信延迟可以做到很低，如果再配合内核旁路，两台机器内存到内存的延迟最低可以做到接近纳秒级别</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="GFS-QA"><a href="#GFS-QA" class="headerlink" title="GFS QA"></a>GFS QA</h2><ul>
<li>硬盤的一個block默認就4KB, as an array in disk</li>
<li>Checksum ，也紀錄下和, 可用md5算hash<ul>
<li>可能會有False Positive, 比如 1,4 两个数据，check sum = 5，但是 2 + 3 也是 5。所以 check sum 相同不能证明数据一定没有发生变化。但是 check sum 不同就表示原始数据肯定发生了变化。因此 check sum 是存在 False Positive 但不存在 False Negative 的。</li>
</ul>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/04/10/System-Design/9p/2019-04-09-Design%20Chat%20System/">System-Design/9p/2019-04-09-Design Chat System</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/SystemDesign/">SystemDesign</a></span><div class="content"><h1 id="General"><a href="#General" class="headerlink" title="General"></a>General</h1><p>信息</p>
<h1 id="Pre-requisite"><a href="#Pre-requisite" class="headerlink" title="Pre-requisite"></a>Pre-requisite</h1><h2 id="Serialization"><a href="#Serialization" class="headerlink" title="Serialization"></a>Serialization</h2><ul>
<li>把RAM裡的數據存上disk或傳輸，如array ==&gt; “[1, 2, 3]”<ul>
<li>不能讓別人來自己的RAM拿，重開機也都不同了</li>
</ul>
</li>
<li>不同類可以定義不同。</li>
<li>java 的 toString() 只是要打出來看，沒要反序列化，不見得算是</li>
<li><strong>如果序列化沒要給人看，還會考慮盡量小</strong><ul>
<li>FB有 <strong>thrift</strong>，是一個RPC的庫，可做序列化、反序列化，可壓縮數據，如定義一個class結構，只要我的obj是滿足那個類的，就可以丟進去，肉眼不可讀，壓縮率高。當我們加屬性，反序列化還要不出錯！</li>
<li>Google有protobuf。類似的事。</li>
</ul>
</li>
<li>JSON / XML (Web1.0、Android),<ul>
<li>現在Web都是JSON，空間小，可讀</li>
<li>如POST過來有各屬性及其對應的values </li>
</ul>
</li>
</ul>
<h2 id="Cookie-amp-Session"><a href="#Cookie-amp-Session" class="headerlink" title="Cookie &amp; Session"></a>Cookie &amp; Session</h2><ul>
<li>cookie 客戶端<ul>
<li>把 Token放在Cookie裡，長度有限制的，一般跟身份認證有關。</li>
<li>CSRF 偽造用戶的表單；當post來的時候必帶CSRF，當生成網頁時才會生成，有一定時間如５分鐘內。動態碼去避免攻擊</li>
<li>可設有效期</li>
</ul>
</li>
<li>Session 服務器<ul>
<li>Authentication Token </li>
<li>Session Table 存 Authentication Token，看哪個用戶登入了</li>
<li>一般存在數據庫裡較多</li>
</ul>
</li>
</ul>
<h2 id="Message-Queue"><a href="#Message-Queue" class="headerlink" title="Message Queue"></a>Message Queue</h2><ul>
<li>是個Queue, FIFO</li>
<li><ol>
<li>任務慢. 2. 需要重試</li>
</ol>
</li>
<li>消息隊列不放聊天的消息；一般是放任務ID之類的</li>
<li>生產者、消費者模型 – 多線程題<ul>
<li>兩邊速率不一致</li>
<li>倉庫就是緩衝</li>
</ul>
</li>
<li>Web 用 MQ緩衝，worker 去拿 MQ，好了後worker寫到DB，Web自己去刷新或時不時看下DB<ul>
<li>Cache, DB 都是最基本構成</li>
</ul>
</li>
<li>任務例子：<ol>
<li>評測機 - 生產消費者、MQ</li>
<li>發email - 要是發失敗的話，也是MQ可以幫忙的</li>
</ol>
</li>
<li>一般常用的<ul>
<li>RabbitMQ, </li>
<li>Redis 什麼都會一點，可當數據庫(比不過SQL, NoSQL)、緩存(比不過Memcached)、消息隊列(比不過RabbitMQ)；但速度比較快，內存級別速度，存取效率非常快</li>
<li>AWS Simple Queue Service 消息隊列的雲服務；保證兩個worker拿不會有不同人重覆處理。浪費，也可能有問題如重覆扣錢。螞蟻是處理萬億級別的，用的消息隊列機制很牛。</li>
</ul>
</li>
<li>鏡像機制，兩台MQ一直作鏡像，一台專門copy，雙Master, 同時分擔讀和寫</li>
<li>重試機制。</li>
<li>12306一定會有message queue，一堆的請求不會馬上被解決掉，看先來後到。</li>
<li>可設優先級</li>
<li>可有子queue。如登機時不同的排隊。隊列之間是可以併發的，但同一個隊列是FIFO的</li>
</ul>
<blockquote>
<ul>
<li>消息队列 - AWS SQS 和GCP Cloud pub/sub</li>
<li>kafka做message queue的场景也挺多的吧。<ul>
<li>是的</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="Chat-System"><a href="#Chat-System" class="headerlink" title="Chat System"></a>Chat System</h1><blockquote>
<p>[单选投票题]微信是点对点通信么？</p>
<p>你的选择:B</p>
<p>感谢您参与投票！</p>
<p>A:是的(20.62% 选择)</p>
<p>B:不是(79.38% 选择)</p>
</blockquote>
<h2 id="Message-Service-gt-Storage"><a href="#Message-Service-gt-Storage" class="headerlink" title="Message Service == &gt; Storage"></a>Message Service == &gt; Storage</h2><blockquote>
<h5 id="多选投票题-我们需要在聊天软件的Message-Table里存储什么？"><a href="#多选投票题-我们需要在聊天软件的Message-Table里存储什么？" class="headerlink" title="[多选投票题]我们需要在聊天软件的Message Table里存储什么？"></a>[多选投票题]我们需要在聊天软件的Message Table里存储什么？</h5><p>您选择的答案是ABCDEG</p>
<p>感谢您参与投票！</p>
<p>A.消息的主键 id14.74% 选择</p>
<p>B.发送人15.94% 选择</p>
<p>C.收件人15.67% 选择</p>
<p>D.ip 地址2.66% 选择</p>
<p>E.发送时间16.09% 选择</p>
<p>F.信息类型11.38% 选择</p>
<p>G.信息内容14.84% 选择</p>
<p>H.会话 id8.69% 选择</p>
</blockquote>
<p>表單的設計很重要，不然效率太差。</p>
<h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><img src="/Users/joe/Library/Application Support/typora-user-images/image-20200815211310570.png" alt="image-20200815211310570" style="zoom:50%;" />



<h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><h3 id="Q-查起來要查左、查右地遍歷，以及合併跟排序"><a href="#Q-查起來要查左、查右地遍歷，以及合併跟排序" class="headerlink" title="Q: 查起來要查左、查右地遍歷，以及合併跟排序"></a>Q: 查起來要查左、查右地遍歷，以及合併跟排序</h3><h4 id="增加-Thread-Table-–-少了or語句，但還是要遍歷"><a href="#增加-Thread-Table-–-少了or語句，但還是要遍歷" class="headerlink" title="增加 Thread Table – 少了or語句，但還是要遍歷"></a>增加 Thread Table – 少了or語句，但還是要遍歷</h4><p>不用 Session這個字，而用Thread, 就是一個Conversation。</p>
<blockquote>
<h5 id="单选题-这样的Thread-Table有什么问题？"><a href="#单选题-这样的Thread-Table有什么问题？" class="headerlink" title="[单选题]这样的Thread Table有什么问题？"></a>[单选题]这样的Thread Table有什么问题？</h5><p>A.信息会有遗漏12.08% 选择</p>
<p>B.信息查询比较慢50.14% 选择</p>
<p>C.有些信息是私有的37.78% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是B</p>
<p><strong>正确答案:</strong>C　－－　如你讀了但我還沒讀但群裡對我而言就變已讀</p>
</blockquote>
<blockquote>
<ul>
<li>假如需要知道A&amp;B之间的对话历史 怎么知道他们的thread ID呢<ul>
<li>可以根据 participant_hash_code 去反向查询到 thread_id。课里有讲到。</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="方法一：拆，加上User-Thread-Table"><a href="#方法一：拆，加上User-Thread-Table" class="headerlink" title="方法一：拆，加上User Thread Table"></a>方法一：拆，加上User Thread Table</h4><blockquote>
<ul>
<li>存last_message 是为了显示在界面上面吗？<ul>
<li>yes</li>
</ul>
</li>
<li>为什么用id做primary key做sharding会影响效率？<ul>
<li>因为一般我们查询UserThread都是根据user_id去查的，因此要按照user_id来做sharding，这样当我们想获取某一个用户的所有thread_id的时候，就可以直接计算出数据所在的shard。如果用id来做sharding的话，就只能广播到每一台机器查询，然后再汇总结果。</li>
</ul>
</li>
<li>为什么这里直接就用了SQL型数据库？noSQL完全不能用吗？<ul>
<li>用 NoSQL 和 SQL 都可以的。这个课里的观点一直都是，你喜欢用哪个就用哪个。很少一定要用 SQL 或者一定要用 NoSQL。这里会讲解按照 SQL 是如何存储的，后面会讲解按照 NoSQL 是如何存储的，两种方法都可以。</li>
</ul>
</li>
</ul>
</blockquote>
<blockquote>
<h5 id="单选题-这样的存储方式有什么弊端？"><a href="#单选题-这样的存储方式有什么弊端？" class="headerlink" title="[单选题]这样的存储方式有什么弊端？"></a>[单选题]这样的存储方式有什么弊端？</h5><p>A.需要跨表查询比较慢63.87% 选择</p>
<p>B.信息会有遗漏5.72% 选择</p>
<p>C.限制sharding效率30.41% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
<p><strong>正确答案:</strong>A</p>
</blockquote>
<p><strong>跨表用到 JOIN就會慢，要解決掉…</strong></p>
<h4 id="方法二：合，"><a href="#方法二：合，" class="headerlink" title="方法二：合，"></a>方法二：合，</h4><p>把公有復制到每個人裡去</p>
<p>這樣做時，當要拿當前用戶資料時，就可以一張表出來，不用再做JOIN；但也有壞處</p>
<blockquote>
<h5 id="多选题-全部放在一个-UserThread-表单里有什么坏处？"><a href="#多选题-全部放在一个-UserThread-表单里有什么坏处？" class="headerlink" title="[多选题]全部放在一个 UserThread 表单里有什么坏处？"></a>[多选题]全部放在一个 UserThread 表单里有什么坏处？</h5><p>A.需要跨表查询比较慢3.02% 选择</p>
<p>B.冗余存储50.89% 选择</p>
<p>C.更新可能会造成的数据不一致问题41.56% 选择</p>
<p>D.信息会有遗漏4.53% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是BC</p>
<p><strong>正确答案:</strong>BC</p>
</blockquote>
<p>Updated_at 雖是共享，也是分在每個裡，因為要用他做排序</p>
<p>更新操作是避不開的。</p>
<blockquote>
<ul>
<li>这里合成一张表算denormalization吗？<ul>
<li>Yes</li>
</ul>
</li>
<li>能两全吗？<ul>
<li>系统设计很多都是trade off，如果有完美的方法我们肯定会优先用完美的方法的，这里的话只能是选定一个方案然后再用一些手段(比如Cache)去减轻其缺点带来的影响。</li>
</ul>
</li>
<li>这算不算在内存里做JOIN操作？<ul>
<li>可以这么理解。</li>
</ul>
</li>
<li>所以在面试中是不是用SQL数据库比较好分析这个题？面试官如果没有要求用noSQL就不要提？<ul>
<li>看你更熟悉哪个。其实 Chat System 是更适合使用 NoSQL 的。你设计的时候用 SQL 问题也不大。面试官一般会根据需要向你针对性提问的。</li>
</ul>
</li>
</ul>
</blockquote>
<p><strong>所以比較下來，拆還是比較清晰，Thread、用戶分開，比較 DECOUPLE，如果問到 JOIN 再用「合」放在一起，復制幾份，反正字節字不多，全拿出來不用再JOIN。實際工程去 Cache拿，避開 JOIN</strong></p>
<h3 id="Q-THREAD-Table-議題"><a href="#Q-THREAD-Table-議題" class="headerlink" title="Q: THREAD Table 議題"></a>Q: THREAD Table 議題</h3><h4 id="participants-hash-code："><a href="#participants-hash-code：" class="headerlink" title="participants_hash_code："></a>participants_hash_code：</h4><h5 id="怎麼查-Thread-ID-因為發消息時可能沒帶著-–-gt"><a href="#怎麼查-Thread-ID-因為發消息時可能沒帶著-–-gt" class="headerlink" title="怎麼查 Thread ID? 因為發消息時可能沒帶著 –&gt;"></a>怎麼查 Thread ID? 因為發消息時可能沒帶著 –&gt;</h5><p><strong>當用戶發信息時或建群聊時，所以要針對現有看是否他們有現有群</strong></p>
<ul>
<li>用參與者查有沒有他們的Thread<ul>
<li>可在 Tread Table 增加一個 participants_hash_code，這樣有以前的群就可以找到<ul>
<li>user排好序後，大家串一起做個uuid，看有沒有以前的群</li>
<li>可以把這個uuid作index查就很快</li>
<li>為何要hash? 群聊太長啦</li>
<li>uuid就不用去考慮相撞，概率不可能</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><strong>群聊的话可以有新的用户加进来，这时候要更改这个hash code吧？</strong><ul>
<li>Yes</li>
</ul>
</li>
<li>如果有些群聊有人加入有人退出怎么办？participant_hash_code也要改变吗？<ul>
<li>Yes, 也要改變。</li>
</ul>
</li>
<li>如果participant更新是不是要重新generate uuid？<ul>
<li>是的，需要重新生成 participant_hash_code</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Q-MESSAGE-Table-怎存好？怎分片好？–"><a href="#Q-MESSAGE-Table-怎存好？怎分片好？–" class="headerlink" title="Q: MESSAGE Table 怎存好？怎分片好？–"></a>Q: MESSAGE Table 怎存好？怎分片好？–</h3><h4 id="Sharding"><a href="#Sharding" class="headerlink" title="Sharding"></a>Sharding</h4><p>用戶就是一直聊一直聊</p>
<p>要找寫快的，所以NoSQL, 有機會 1.5M QPS，檢索就是 kv。</p>
<blockquote>
<h5 id="单选题-需要查询thread里的message时sharding-key（row-key）是什么？"><a href="#单选题-需要查询thread里的message时sharding-key（row-key）是什么？" class="headerlink" title="[单选题]需要查询thread里的message时sharding key（row key）是什么？"></a>[单选题]需要查询thread里的message时sharding key（row key）是什么？</h5><p>A.user_id11.42% 选择</p>
<p>B.created_at9.40% 选择</p>
<p><strong>C.thread_id56.81% 选择</strong></p>
<p>D.participant_hash_code22.36% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是D</p>
<p><strong>正确答案:</strong>C</p>
</blockquote>
<blockquote>
<ul>
<li>支持消息撤回(recall)的话，一条消息在还允许撤回的这个time window里，也是写在message table里吗？具体撤回是怎么实现的呢？<ul>
<li>增加一个类型的 message，这种 message 就表示要撤回哪条信息。然后客户端拿到这样的信息以后在客户端把消息隐藏掉。这里可能有另外一种容易想到的但是是错的方法是给 message 增加一个是否被撤回了的标记。这种方法有问题的原因是，一般 mobile client 上的信息更新都是增量式的，也就是说，一旦一个消息被下载下来，会在 client 端里也存储一份，并且不会再去检测这条消息是否有被更新过（否则那 client和 server 负担就都大了）因此需要通过“增量式”的方式来更新这条数据的信息，那么自然就是通过一条新的 message 来撤回老的 message。</li>
</ul>
</li>
<li>在cassandra里，可以理解为row key 就是等于 sharding key 吗？有任何反例吗？<ul>
<li>row_key = sharding_key 没错，没有反例，就是一个东西的不同称呼。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Q-THREAD-Table-怎存好？"><a href="#Q-THREAD-Table-怎存好？" class="headerlink" title="Q: THREAD Table 怎存好？"></a>Q: THREAD Table 怎存好？</h3><h4 id="SQL-需-thread-id-amp-participant-hash-code-作-index"><a href="#SQL-需-thread-id-amp-participant-hash-code-作-index" class="headerlink" title="SQL : 需 thread_id &amp; participant_hash_code 作 index"></a>SQL : 需 thread_id &amp; participant_hash_code 作 index</h4><h4 id="NoSQL-同，需-thread-id-amp-participant-hash-code-兩張表都查"><a href="#NoSQL-同，需-thread-id-amp-participant-hash-code-兩張表都查" class="headerlink" title="NoSQL : 同，需 thread_id &amp; participant_hash_code 兩張表都查"></a>NoSQL : 同，需 thread_id &amp; participant_hash_code 兩張表都查</h4><h5 id="NoSQL-with-Thread-Table"><a href="#NoSQL-with-Thread-Table" class="headerlink" title="NoSQL with Thread Table"></a>NoSQL with Thread Table</h5><p>也可以用SQL，兩者都Ok。</p>
<p>index by thread_id 以及 participant_hash_code</p>
<blockquote>
<h5 id="单选题-Thread-Table-如果使用-NoSQL-该如何存储？"><a href="#单选题-Thread-Table-如果使用-NoSQL-该如何存储？" class="headerlink" title="[单选题]Thread Table 如果使用 NoSQL 该如何存储？"></a>[单选题]Thread Table 如果使用 NoSQL 该如何存储？</h5><p>A.一张表单，row_key为 user_id3.44% 选择</p>
<p>B.两张表单，row_key为 participant_hash_code 和 user_id7.07% 选择</p>
<p>C.两张表单，row_key分别为 thread_id 和 user_id21.41% 选择</p>
<p>D.两张表单，row_key分别为 thread_id 和 participant_hash_code68.07% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是D</p>
<p><strong>正确答案:</strong>D</p>
</blockquote>
<p>用雙表去模擬SQL的單表雙index(背後也是幫建表加速)</p>
<p>也不range query就不用 column, 所以可以就 RocksDB</p>
<blockquote>
<ul>
<li>可以根据created_at来排序column_key 吗？这样可以找到最新20条消息？<ul>
<li>这个其实可以在获取完thread列表之后再按照last_message的时间戳在客户端进行排序，这个排序工作量很小，不需要在服务端做。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Q-UserThread-Table-怎存好？怎分片好？"><a href="#Q-UserThread-Table-怎存好？怎分片好？" class="headerlink" title="Q: UserThread Table 怎存好？怎分片好？"></a>Q: UserThread Table 怎存好？怎分片好？</h3><h4 id="NoSQL-存私有-Thread-info，用user-id-分片"><a href="#NoSQL-存私有-Thread-info，用user-id-分片" class="headerlink" title="NoSQL 存私有 Thread info，用user_id 分片"></a>NoSQL 存私有 Thread info，用user_id 分片</h4><blockquote>
<h5 id="单选题-UserThread-Table-用什么做sharding-key（row-key）？"><a href="#单选题-UserThread-Table-用什么做sharding-key（row-key）？" class="headerlink" title="[单选题]UserThread Table 用什么做sharding key（row key）？"></a><strong>[单选题]UserThread Table 用什么做sharding key（row key）？</strong></h5><p>A.user_id68.04% 选择</p>
<p>B.created_at1.37% 选择</p>
<p>C.thread_id24.24% 选择</p>
<p>D.participant_hash_code6.35% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是C</p>
<p><strong>正确答案:</strong>A</p>
</blockquote>
<p>怎麼取就怎麼sharding…</p>
<blockquote>
<ul>
<li>当web server收到一条群聊的消息，他要怎么知道要往哪些用户发送呢？方法1的thread table已经没有participant_user_ids，只能靠user thread table来得知有谁是属于这个thread。但是user thread table的sharding row key是user id，所以如果要查询participant list，只能scan完所有的user 才能知道谁是属于这个thread。<ul>
<li>需要增加一个 ThreadParticipant 的 Table。记录谁参与了哪些 Thread。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="Flow"><a href="#Flow" class="headerlink" title="Flow"></a>Flow</h2><ol>
<li><p>A 發給B</p>
</li>
<li><p>message server 去查是否有A&amp;B混在一起的  participant code, 沒有就建Thread</p>
</li>
<li><p>根據 thread_id 建 message</p>
</li>
<li><p>B 定期每隔如10秒去 Poll</p>
<ul>
<li><p>B POLLing</p>
<p>每隔一段時間的PULL就是POLL</p>
</li>
</ul>
</li>
<li><p>B 拿到 message</p>
</li>
</ol>
<h2 id="Scale-–-GCM-amp-APNS"><a href="#Scale-–-GCM-amp-APNS" class="headerlink" title="Scale – GCM &amp; APNS"></a>Scale – GCM &amp; APNS</h2><ul>
<li>怎麼讓消息更實時？每隔10秒收到消息體驗太差啦!<ul>
<li>PUSH NOTIFICATION</li>
</ul>
</li>
<li>PUSH 通知: 手機上<ul>
<li>Android GCM (Google Cloud Messaging)</li>
<li>Windows 也有它自己的推送系統</li>
<li>iOS APNS (Apple Push Notification Service)<ul>
<li>不同的手機有device token, app註冊完會有, APNS會發給手機, server 拿這個當 push token 去推送給 B，B再過去取 Web Server 取。局限性呢？</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<h5 id="单选题-这样的方法有什么局限性？"><a href="#单选题-这样的方法有什么局限性？" class="headerlink" title="[单选题]这样的方法有什么局限性？"></a>[单选题]这样的方法有什么局限性？</h5><p>A.获取新消息时可能有延迟26.92% 选择</p>
<p>B.数据库压力过大10.22% 选择</p>
<p>C.消息丢失概率大12.08% 选择</p>
<p><strong>D.无法支持web端50.79% 选择</strong></p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是D</p>
<p><strong>正确答案:</strong>D</p>
</blockquote>
<blockquote>
<ul>
<li>这个push notification system在手机系统上是指的它实际运行过程中是在用户的手机上吗？（比如用户手机就是一个server）还是说apple有其他外的服务器来支持apns？<ul>
<li>用户的手机是一个 client 不是一个 server，用户的手机不需要给其他人提供服务，他只问被人要服务。Push Notification System 是 Apple/ Google 单独搭建的一套服务器系统，专门用来提供 push 服务。</li>
</ul>
</li>
<li>是不是现在也有些浏览器提供push notification？比如Chrome？<ul>
<li>浏览器的push notification本质上就是一个弹出的对话框，本质上还是网页自己基于websocket一类的技术实现的，跟GCM/APNS原理不一样。</li>
</ul>
</li>
<li>不太理解为什么server不直接和B通讯呢？通过GCM作为中介的好处是？<ul>
<li>APNS/GCM都是专门用于移动端的消息推送的，为消息推送提供了一个与操作系统深度整合的、高度优化的框架，如果不用这个的话，就得自己重新造一套这样的消息推送框架，费时费力，而且标准不一。而且假设你有50个app, 如果每个app都需要链接各自的push服务器的话，会给手机带来很大的负担。具体的讨论可以看这里：<a href="https://stackoverflow.com/questions/31645010/why-it-is-preferred-to-use-gcm-for-push-notifications" target="_blank" rel="noopener">https://stackoverflow.com/questions/31645010/why-it-is-preferred-to-use-gcm-for-push-notifications</a></li>
</ul>
</li>
</ul>
</blockquote>
<p>沒有提供給所有的browser推送系統的。該怎辦呢？</p>
<h3 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a>Socket</h3><p>解決的辦法就是自己搭push的服務，不要依賴第三方的push 通知服務。實時性會更高。</p>
<p>★ HTTP 只支持客戶端向服務器獲取數據，因為Server不知道客戶在哪； client to server 是有ip、域名的</p>
<p>Socket是讓服務器可以主動向客戶端推數據。</p>
<p>Socket, 跟 http一樣都是tcp/ip，本來就都可以雙向通信。http不能保持連接，socket可以。</p>
<p>要是push server掛的話，就回到PULL，10秒拉一次，反正client是知道連接斷開了的</p>
<blockquote>
<ul>
<li>为什么不用 long polling呢<ul>
<li>当clients很多的时候，long polling会让server返回大量的空结果（没有新信息的时候），这样会给server带来很大的负担，比较浪费资源。</li>
</ul>
</li>
<li>Push server needs to keep too many connections, would not this be a problem?<ul>
<li>yes, so push server always push messsges async</li>
</ul>
</li>
<li>push server是不是和message queue/kafka差不多？<ul>
<li>不是的，push server 和 message queue 是完全不同的东西。push server 里就是通过 socket 连着用户，有消息就推一下而已。而 message queue 是把需要执行的任务信息存储在 queue 里，等着执行任务的进程来逐个领取的一个中间缓存机构。<strong>本质上来说 push server 是 stateless 的，是不存数据的。message queue 是 stateful 的，是存数据的</strong>。<em>就跟 web server 和 database 之间的本质区别一样。</em></li>
</ul>
</li>
<li>如果A或者B离线呢，消息存在哪？<ul>
<li>消息存在 Message Service 里（数据库里），socket 只是“提醒” 的作用，并不是说消息只从 socket 走。你可以认为 socket 里走的数据是“嘿你有消息了，去服务器拿一下”，然后用户去服务器拿到最新消息。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Group-Chat"><a href="#Group-Chat" class="headerlink" title="Group Chat"></a>Group Chat</h3><p>大群裡，可能只有很少人在線，就是只有很少人需要被即時推送。</p>
<p>這樣也不需要優化得那麼實時。</p>
<blockquote>
<ul>
<li>相較於使用channel 篩選在線用戶的方式, 老師後面有說可以建一個 online status 的 table, 那我是不是可以直接從online status裡面檢查群聊還有在線的, 直接send message給在線的就好. 感覺好像比用channel還要subscribe的方式簡單?<ul>
<li>不行。online status table 里在线不代表这个人已经连接上了 push socket（虽然大概率是连着的），但是这种反向调用关系是不好的架构设计，类似写代码的时候底层函数调用了上层函数，会导致系统之间的依赖关系出现循环依赖。</li>
</ul>
</li>
<li>一般来说是多少人以上的群采用channel service呢？还是说哪怕两个人私聊也算一个两个人订阅的channel?<ul>
<li>一般可能10+ 人才会走 public channel。如果是私聊，走的是 private channel，也就是说，比如你的 user_id 是 10，那么所有和你私聊的信息，都走到 “#user_10” 这个channel里去（这个channel名字的格式你自己可以随意定义）。你一上线也会自己订阅这个channel。这个channel 只属于你自己，不和别人共享。</li>
</ul>
</li>
<li>这里的channel service可以换成message queue么？生产者也是只用发一次消息，然后订阅的消费者去取这些消息？这里channel service和massage queue有什么区别？谢谢<ul>
<li>不可以。channel service 是 fanout，是广播。一条消息会被多个 subscriber 拿到。message queue 不是广播，一条任务消息必须保证只被一个 task worker 拿到。</li>
</ul>
</li>
<li>channel service 可以用kafka么？<ul>
<li>这样用不太对。kafka 是消息队列，channel 是一个 key-value 的存储 + 信息 fanout。</li>
</ul>
</li>
<li>所以用户下线以后就自动取消订阅channel service吗？<ul>
<li>用户下线的时候push server会知道用户下线了，然后push server会把这件事通知给channel service。</li>
</ul>
</li>
<li>如果用户不在线的话，历史消息怎么办呢？是储存在channel service里面吗？<ul>
<li>用户不在线的时候channel service就不会给相应用户推送，也不会保存这条消息，channel service只是为了给在线用户实时推送消息，至于历史消息是用户上上线的时候自己去Message Service里面pull的。</li>
</ul>
</li>
<li>以前课提到的明星效应问题可以考虑用pull， 我们的大group 也有点类似明星效应问题，但是不用pull，是不是因为消息系统有实时性的要求？<ul>
<li>大 Group 是 500 个人的 Group。这个和明星的 50m 的粉丝不是一个量级的。这里用 push 的原因最主要还是用户行为，人们阅读聊天信息通常都是被动的。而人们阅读 SNS 的信息通常都是主动的。</li>
</ul>
</li>
<li>那么实际实现的时候，是针对大群才用 channel service？<ul>
<li>不是。都会用到 channel，只是走 Public channel 还是 private channel 的区别。比如私信，或者小群，可以走每个人 private channel，这些 private channel 只有用户自己订阅，其他人不会订阅。而大群就放在 Public channel，里面会有好多人。</li>
</ul>
</li>
<li>所以对于未上线用户，channel service最终还是会把历史消息存到数据库里，等待用户上线pull？<ul>
<li>并不是 channel service 把历史消息存到数据库里。消息最开始就先经过 Message Service，先被存储在了数据库里，然后才通过 channel service 去提醒用户查看。通过 channel 和 push server / socket 走的信息都是提醒信息，你可以认为里面的内容是“嘿，你有新消息了，快去服务器取”，而不要把 channel service 当作了消息本身最开始被传递到的地方。</li>
</ul>
</li>
<li>所以channel service 是fan out的，然后push server是点对点的给client推送的？<ul>
<li>没错， channel server 是负责 fanout 的，push server 是点对点完成具体推送的。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Channel-Service-QA"><a href="#Channel-Service-QA" class="headerlink" title="Channel Service QA"></a>Channel Service QA</h3><blockquote>
<ul>
<li>channel和Thread为什麽不是一一对应的关系？<ul>
<li>channel是架构上的逻辑，thread是具体到代码实现层的，不是一个层面的概念</li>
</ul>
</li>
<li>Channel service要自己写吗？还是说有什么线程的软件？<ul>
<li>自己寫</li>
</ul>
</li>
<li>系统怎么知道哪些消息已经被poll过，或者是从channel发送的？会不会有重复<ul>
<li>client可以通过告诉server自己本地已有的最新一条信息是哪条，或者告诉server自己随后一次同步的时间戳，然后server就直接返回该时间戳之后的消息。另外每个message都是有id的，所以即使client收到了两条一样的消息client也能识别出来是同一条消息。</li>
</ul>
</li>
<li>channel name也应该存在其他table里面吧？要不然web service收到一个讯息时怎么知道该送往哪个channel呢？是该加入在user thread table吗？<ul>
<li>不需要有一个单独的 table。web service 收到 channel 信息的时候直接转给 channel service 去 push 即可。很多时候 web service 不是直接从 client 那里得到 channel name。而是直接构造。比如 thread 的channel 可以就叫 <code>#thread1</code> <code>#thread2</code> 这种。有一个规则可以构造即可。</li>
</ul>
</li>
<li>为什么不对每个thread做channel, personal channel有什么message?对personal channel推送的时候前端怎么知道属于哪个thread?这是一种针对人数少的群的优化么？<ul>
<li>如果对每个 thread 都做一个channel，是可以的。只是不够高效，因为很多用户的 threads 里很多是1对1的聊天，这种做到 channel 里不划算。所以通过一个统一的 personal channel，来推送某个用户1对1或者人数比较少的thread 里的信息提醒。personal channel 推送的时候，可以根据 channel 的名字来解析出属于哪个用户，比如 personal channel 的名字格式可以定义为：”personal#1234” 表示是 1234 这个用户的 channel。</li>
</ul>
</li>
<li>所以channel是messaging service 创建的，对用户透明？<ul>
<li>channel是由单独的channel service创建的，messaging service将推送群聊消息的任务委托给channel service，然后channel service再自己去把消息push给群里的在线用户。这整个过程对用户是透明的，用户眼中只有”群”的概念，没有channel的概念。</li>
</ul>
</li>
<li>不太明白#personal::user_1的用法。即使是1对1私聊user_1也可以跟user_2,user_3同时进行，这时难道#personal::user_1里有user_2和user_3吗？消息串了怎么办？而且user_2,user_3不也有自己的personal channel吗？<ul>
<li>#personal:user1是user1的私人频道，只有user1一个人订阅，所有发给user1的私信都会在这里推送。比如user2给user1发消息的时候，messaging service收到消息之后会让channel service向#personal:user1推送一条消息｛thread: user1#user2, from: user2, content: xxxx}，此时user1收到消息之后，识别出该消息是来自user2的私信，因此显示在跟user2的聊天框里，消息是不会串的。</li>
</ul>
</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>老师可不可以简单解释一下“对方正在打字”这个功能是怎么实现的？<ul>
<li>问得好！这个叫做 TypyIndicator，有的公司专门拿来做面试题。实现方式其实很简单，首先 Storage 这个方面其实这个信息是不需要存储的，即便要存储（比如为了做数据分析）也就是存储最后一次 type 的时间就好了。 client A 检测到用户正在输入以后，发一个轻量级的消息给 server， server 通过 push service 发正在输入的信息给 client B，然后 client B 就显示用户正在输入。约定一个检测时间，比如当用户在输入框里输入的时候，大概每隔1-3s 就发送一次这种信息 push（具体取几秒钟看产品经理怎么拍脑袋了）。</li>
</ul>
</li>
</ul>
</blockquote>
<p>活躍時就poll先休息，所以會用推的比較快</p>
<h2 id="多機登陸"><a href="#多機登陸" class="headerlink" title="多機登陸"></a>多機登陸</h2><p>session中紀錄用戶info</p>
<h2 id="On-line-status"><a href="#On-line-status" class="headerlink" title="On-line status"></a>On-line status</h2><p>可用push的socket去看是否在線嗎？</p>
<p>實時性要求不高，可以用PULL.　</p>
<blockquote>
<ul>
<li>如果server发现用户在一定时间内没有pull请求，是不是可以标记为offline，然后对他的所有好友也告知此人offline？<ul>
<li>可以，不过一般不是server主动去push通知其好友用户A已经offline，而是好友在自己发出heartbeat pull或者主动请求好友在线信息的时候服务器才告诉该好友用户A已经下线。</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h1><img src="/Users/joe/Library/Application Support/typora-user-images/image-20200410203715392.png" alt="image-20200410203715392" style="zoom:50%;" />



<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200410210741607.png" alt="image-20200410210741607" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdoz6w1gbxj30i20ye0we.jpg" alt="image-20200410210804359" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200411222125219.png" alt="image-20200411222125219" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200411222144074.png" alt="image-20200411222144074" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdq6ydxcquj30ii0mk40f.jpg" alt="image-20200411222215512" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200411222247062.png" alt="image-20200411222247062" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200411231451591.png" alt="image-20200411231451591" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdq8ivyb8cj30n20w010o.jpg" alt="image-20200411231633812" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdq8keeocxj30m20oq794.jpg" alt="image-20200411231800981" style="zoom:50%;" /></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/04/10/Web/Django/2019-04-10-Django%20note/">Web/Django/2019-04-10-Django note</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/Web/">Web</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Django/">Django</a></span><div class="content"></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/04/05/System-Design/9p/2019-04-05-Location%20Based%20Service%20Design%20&amp;%20Uber/">System-Design/9p/2019-04-05-Location Based Service Design &amp; Uber</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-05</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/SystemDesign/">SystemDesign</a></span><div class="content"><h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdj89p8x7zj31140fqdpo.jpg" alt="image-20200405214845930"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdj8bi4vkpj31180gw105.jpg" alt="image-20200405215030180"></p>
<p>為了減少磁盤磁頭挪動找的次數，二叉的話樹會太高。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdj8czu4cmj30tu0dyjxn.jpg" alt="image-20200405215156714"></p>
<p>唯一：如user_name, email, phone_number</p>
<p>聯合：關手到兩個col的需求要建聯合索合</p>
<p>條件：去篩掉不付錢的，他們的index太大了，我不想查到他們</p>
<p>★聯合索引不能解決分別查 A 的範圍和 B 的範圍然後再求交集這件事情，它是照第一鍵排序後，當第一鍵有相等值的話再照第二鍵去排。然後將查到的id再去原表裡查，也是座標排序法–x不等的照x排序，x相等的按照y排序。</p>
<h1 id="LBS"><a href="#LBS" class="headerlink" title="LBS"></a>LBS</h1><p>Location Based Service</p>
<h2 id="Uber-技術棧"><a href="#Uber-技術棧" class="headerlink" title="Uber 技術棧"></a>Uber 技術棧</h2><p>派單 vs 搶單</p>
<h3 id="派單模式"><a href="#派單模式" class="headerlink" title="派單模式"></a>派單模式</h3><ul>
<li>ringpop ，一種去中心化的架構</li>
<li>Channel, 高效 RPC</li>
<li>Google S2</li>
<li>Riak</li>
</ul>
<h3 id="T-Channel"><a href="#T-Channel" class="headerlink" title="T Channel"></a>T Channel</h3><p>一種RPC(遠程函數調用)協議。怎麼把不同進程間內存作序列化傳輸及反序列化解析。</p>
<p>RPC 服務器之間的，HTTPS話還要對資訊作加密，防用戶端的hacker之類。</p>
<p>如果流量不大的話用 http 也ok, http 比較現成，比較好寫，如果要寫thrift還要定義結構，不如json方便。http相對rpc要慢一些，其實主要是設計為客戶端跟服務器之間的通信，而客戶端問題多，搗亂的人多，需要驗證，要提供放cookie裡的驗證，一下就好幾k到好幾10k，而服務器之間其實可能就只要幾百個字節。</p>
<p>RPC還多了壓縮，讓效率高、延遲低。server2server就放LAN, 也不用加密。不用擔心不合理的傳來的數據。防驗證該是firewall該做的。</p>
<blockquote>
<h5 id="多选题-现有的比较成熟的、广泛使用的RPC协议有哪些？"><a href="#多选题-现有的比较成熟的、广泛使用的RPC协议有哪些？" class="headerlink" title="[多选题]现有的比较成熟的、广泛使用的RPC协议有哪些？"></a>[多选题]现有的比较成熟的、广泛使用的RPC协议有哪些？</h5><p>A.HTTP27.04% 选择</p>
<p>B.Thrift38.66% 选择</p>
<p>C.ProtoBuf34.30% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是AB</p>
<p><strong>正确答案:</strong>ABC</p>
<p><strong>解析:</strong></p>
<p>Thrift 是 Facebook 的，ProtoBuf 是 Google 的。<br>Http 也可以认为是一个 RPC。</p>
</blockquote>
<blockquote>
<h5 id="单选题-生活中运用地最广泛的RPC协议是？"><a href="#单选题-生活中运用地最广泛的RPC协议是？" class="headerlink" title="[单选题]生活中运用地最广泛的RPC协议是？"></a>[单选题]生活中运用地最广泛的RPC协议是？</h5><p>A.HTTP97.39% 选择</p>
<p>B.Thrift1.21% 选择</p>
<p>C.ProtoBuf1.40% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是A</p>
<p><strong>正确答案:</strong>A</p>
<p><strong>解析:</strong></p>
<p>HTTP 是 client （如 Browser) 与 server 通信的协议。是非常典型的 RPC。</p>
</blockquote>
<blockquote>
<ul>
<li>RPC跟REST API有什么不一样<ul>
<li>rest api一般指的是定义好的http 接口，rpc是一种通信协议</li>
</ul>
</li>
<li>Thrift, ProtoBuf和HTTP协议差别在哪里呢？<ul>
<li>差别可多了，比如最主要的差别是 thrift 和protobuf 的通信量相对 http 都很小，速度也就快很多。其他的差别，建议您可以去 Google 一下 thrift 和 protobuf，或者写一下 thrift 的调用代码和http 的调用代码来体会。</li>
</ul>
</li>
<li>RPC跟RESTful API 有什么不同？<ul>
<li>RPC 是一类协议的统称，Restful API 是一种编码规范。<br>Restful API 也是一种 RPC。我们说过 RPC 的时候通常说的意思是“远程调用”，我们说 Restful API 的时候通常指的是 “GET <a href="https://xxx/api/users/&quot;" target="_blank" rel="noopener">https://xxx/api/users/&quot;</a> 之类的 Web API 设计。Web API 当然也是一种“远程调用”</li>
</ul>
</li>
<li>请问 用Flask写的API和用grpc写的API有什么区别？两者对比有什么好处呢<ul>
<li><strong>Flask 之类的 Web Framework 写的 API就是一个 HTTP API。相比于 GRPC 或者 Thrift 之类写的 API 而言，最主要的区别是 HTTP 的 Request 整个数据传输一般比较大，因为要包含 HTTP Header, Cookie 之类的很多东西，其实很多时候用不上。所以 GRPC 之类的会高效不少</strong>。</li>
</ul>
</li>
<li>client make a request to server 是都要用HTTP协议？<ul>
<li>不一定的，也有用其他协议的。只是大部分都用 http，简单方便成熟。</li>
</ul>
</li>
<li>server to server之间的通信的内容什么？不同伺服器上的数据？<ul>
<li>server to server 当然也是有通信的需求的。<strong>比如在Uber设计中，GeoService 里的 server 就存储了地理位置信息，并提供了访问地理位置信息的接口。这个 GeoService 可以服务于 Uber 打车，还可以服务于 Uber 外卖。不同的其他的 server 都可以调用 GeoService 来存取数据，这就是服务器间通信。</strong></li>
</ul>
</li>
<li>RESTful API 是基于http protocol的？<ul>
<li>是的。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Google-S2-amp-Riak"><a href="#Google-S2-amp-Riak" class="headerlink" title="Google S2 &amp; Riak"></a>Google S2 &amp; Riak</h3><h2 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h2><blockquote>
<h5 id="多选题-对于一个打车软件来说，最核心的两个功能是什么？"><a href="#多选题-对于一个打车软件来说，最核心的两个功能是什么？" class="headerlink" title="[多选题]对于一个打车软件来说，最核心的两个功能是什么？"></a>[多选题]对于一个打车软件来说，最核心的两个功能是什么？</h5><p>A.支付功能10.34% 选择</p>
<p>B.汇报位置34.47% 选择</p>
<p>C.匹配乘客和车辆45.34% 选择</p>
<p>D.评分系统0.31% 选择</p>
<p>E.一键报警0.31% 选择</p>
<p>F.路线导航9.22% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是BC</p>
<p><strong>正确答案:</strong>BC</p>
<p><strong>解析:</strong></p>
<p>支付并不是最重要的，完全可以做一个免费的打车软件，但让司机汇报自己的位置和为乘客匹配一个合适的司机是构成一个打车软件的基本功能。</p>
</blockquote>
<ul>
<li>該可以時不時更新司機位置</li>
<li>為乘客匹配司機</li>
<li>司機同意就繼續完成派單</li>
<li>直接司機告訴服務器自己在哪</li>
</ul>
<p>會發現他一天會有幾單，會橫跨幾個城市，全市</p>
<ul>
<li>有集會時會有大量單，所以透過偷偷蒐集用戶地點，我們可以提前調度司機往那邊跑。</li>
<li>但這理論上是蒐集隱私的行為哈</li>
</ul>
<h2 id="Service-Geo-amp-Dispatch"><a href="#Service-Geo-amp-Dispatch" class="headerlink" title="Service - Geo &amp; Dispatch"></a>Service - Geo &amp; Dispatch</h2><blockquote>
<h5 id="单选题-图中漏了什么？"><a href="#单选题-图中漏了什么？" class="headerlink" title="[单选题]图中漏了什么？"></a>[单选题]图中漏了什么？</h5><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdkglvyl64j30ky0c8ju9.jpg" alt="图片"></p>
<p>A.司机和乘客的双向联系23.16% 选择</p>
<p>B.司机获取到打车订单信息45.00% 选择</p>
<p>C.司机当前接单与否的状态31.84% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是C</p>
<p><strong>正确答案:</strong>B</p>
</blockquote>
<blockquote>
<ul>
<li>Dispatch service 里面包含逻辑处理 + 数据存储。能否解释一下数据储存？这里的数据储存存的是什么数据？是否是司机的地理位置数据？<ul>
<li>地理位置数据可以存在 GeoService 里。Dispatch Service 里存的是派单（Dispatch）信息的数据，比如我把哪个司机分配给了哪个乘客，这个在我们的课程中是存在 Trip Table 里。即用户点击打车就创建了一个 Trip，当分配了一个司机以后，就往里填进去被分配的司机。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="Storage-Trip-amp-Location"><a href="#Storage-Trip-amp-Location" class="headerlink" title="Storage - Trip &amp; Location"></a>Storage - Trip &amp; Location</h2><blockquote>
<h5 id="单选题-Trip-Table-的读写情况如何？"><a href="#单选题-Trip-Table-的读写情况如何？" class="headerlink" title="[单选题]Trip Table 的读写情况如何？"></a>[单选题]Trip Table 的读写情况如何？</h5><p>A.读多写少25.16% 选择</p>
<p>B.读少写多36.25% 选择</p>
<p>C.读写一样多38.58% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTQxNzAwNjYyNTQzIiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjEwODgiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iNjQiIGhlaWdodD0iNjQiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxMiAwYTUxMiA1MTIgMCAxIDAgMCAxMDI0QTUxMiA1MTIgMCAwIDAgNTEyIDB6IG0xNjUuOTUyIDYzNy43NmwxNS4wNCAxNC45NzZhMjguNDE2IDI4LjQxNiAwIDEgMS00MC4yNTYgNDAuMjU2bC0xNC45NzYtMTUuMDRMNTA5LjUwNCA1NDkuNzYgMzcxLjIgNjg4YTI4LjQxNiAyOC40MTYgMCAxIDEtNDAuMjU2LTQwLjI1NmwxMzguMzA0LTEzOC4yNC0xMjMuMi0xMjMuMi0xNS4xMDQtMTUuMTA0YTI4LjU0NCAyOC41NDQgMCAwIDEgMC00MC4yNTYgMjguNTQ0IDI4LjU0NCAwIDAgMSA0MC4yNTYgMGwxNS4wNCAxNS4xMDRMNTA5LjQ0IDQ2OS4yNDhsMTQzLjIzMi0xNDMuMjk2YTI4LjQxNiAyOC40MTYgMCAxIDEgNDAuMjU2IDQwLjI1Nkw1NDkuNzYgNTA5LjUwNGwxMjguMTkyIDEyOC4yNTZ6IiBmaWxsPSIjRjY1RTVFIiBwLWlkPSIxMDg5Ij48L3BhdGg+PC9zdmc+" alt="img">答错了，您选择的答案是C</p>
<p><strong>正确答案:</strong>A</p>
<p><strong>解析:</strong></p>
<p>只有产生用户请求时才会产生写操作，查询操作比较多，因为每隔四秒司机都会需要查询是否有与之匹配的订单</p>
</blockquote>
<blockquote>
<h5 id="单选题-Location-Table-的读写情况如何？"><a href="#单选题-Location-Table-的读写情况如何？" class="headerlink" title="[单选题]Location Table 的读写情况如何？"></a>[单选题]Location Table 的读写情况如何？</h5><p>A.读多写少8.05% 选择</p>
<p>B.读少写多71.88% 选择</p>
<p>C.读写一样多20.08% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是B</p>
<p><strong>正确答案:</strong>B</p>
<p><strong>解析:</strong></p>
<p>每隔四秒司机的位置都需要被更新，属于写操作；读取操作只有在查找乘客附近的司机时才会发生。</p>
</blockquote>
<blockquote>
<ul>
<li>对这个流程有些疑惑，为什么不等到driver同意pick up之后再往trip里写一个新的record，这样不是可以避免driver反复查询吗<ul>
<li>有很多原因。首先，对那么没有成单的打车请求，也有记录数据和分析数据的需求，不仅仅是记录那些匹配上的单子。其次，当一个打车请求产生一个，我们需要提供给打车的人一个查询主体去让他查询有没有匹配上，因此势必是要创建一个 Record 的。至于这个 Record 是一个 Trip 的 Record 还是一个 PendingRequest 的 Record，这个就看你自己愿意怎么设计了。我是觉得统一都是 Trip ，用 status 来标记不同时期的状态是比较好的做法。</li>
</ul>
</li>
<li>如果app上要显示location的话是要读的吧，所以是不是应该是读写一样多呢<ul>
<li>是的。<br>这是属于另外一个功能点的要实现的service-trip routing service。<br>其实这个系统中的Dispatch Service和Geo Web Server还可以分化出来一个专门复杂计算匹配功能的Matching Service 出来。</li>
</ul>
</li>
<li>请问driver要怎么从trip table查询他有没有被assigned到某个trip? 是对driver_id做 index，让我们可以直接search吗？<ul>
<li>是的，对 driver_id 有 index 就行了。</li>
</ul>
</li>
<li>数据库定义 外键会不会影响性能呢？<ul>
<li>定义外键就必须给该列建立索引，这样可以加快查询时的速度，但是相应地在数据更新的时候会花费一些时间来维护索引。</li>
</ul>
</li>
</ul>
</blockquote>
<h1 id="LBS-難點"><a href="#LBS-難點" class="headerlink" title="LBS 難點"></a>LBS 難點</h1><ul>
<li>數據庫不好查圓，改求正方形</li>
</ul>
<blockquote>
<p>课中练习</p>
<p>[单选题]分别对 latitude 和 longitude 建index是否可行？</p>
<p>你的选择:B</p>
<p>A:可行</p>
<p><strong>B:不可行</strong></p>
</blockquote>
<p>那建複合索引ok嗎？就是二元組排序</p>
<p>[单选题]复合索引是否能解决 查询效率低下的问题呢？</p>
<p>A:可以</p>
<p><strong>B:不可以</strong> 仍是不行</p>
<p>要找兩個key都在某個範圍內的range query仍是無法做的，只能找到同lat下的log的範圍。</p>
<p><strong><em>所以，將二維映射到一維做</em></strong></p>
<h2 id="2D-Range-Query"><a href="#2D-Range-Query" class="headerlink" title="2D Range Query"></a>2D Range Query</h2><p>映射到一維的range query，相當於對二維作range query</p>
<ul>
<li>Google S2 –  Hilbert Curve, 一維上近的，二維上的也近；反之否。</li>
<li>Geohash – Peano Curve<ul>
<li>0-9, a-z 去掉 a, i, l, o 為 base32剛好2^5, 因為長相容易看錯，發明算法的人決定去掉這四個char的</li>
<li>find longest-common-prefix</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>所以说Hilbert mapping如果两个点二维空间接近， 一维上不一定接近对吧（比如说最中间的四个点）<ul>
<li>是的。但是反过来是对的，一维越接近，二维就越接近。</li>
</ul>
</li>
<li>Google S2怎么解决障碍物的问题？比如河流，单行线等。<ul>
<li>Google S2不能解决障碍物的问题，障碍物的问题是交给地图解决的。在或取到附近的司机之后，可以过一遍地图，查一下路径，如果不可达或者要绕远路的话可以排除掉。</li>
</ul>
</li>
<li>为什么不直接按地域分，然后在这个地域里直接用for loop算出每个坐标和用户的距离？这个是O(n) 是不是因为 SQL查询是O(logn)所以更快？<ul>
<li>这样就要对每个用户都算一遍距离，n个用户就要对所有坐标算n次，开销很大。而用Geohash就只需要做一个range query就行了，range query一般有索引支持，做起来很快。</li>
</ul>
</li>
<li>对两个column都是range query的情况，如果用复合索引的话，虽然对第二个column是无序的，但是因为第一个的range已经选出来，这时如果delta比较小（range范围小），对于第二个column也只用遍历第一个选出来的部分，相较于加索引的log（n）会差别很大么？<ul>
<li>你的这个假设 delta 比较小是不合理的，我们大部分的需求都是 delta 比较大的。如果你说数据小的话，那什么办法都可以，不加索引也可以。数据小是无敌的。</li>
</ul>
</li>
<li>GoogleS2算法会将那些2D近，1D远的点漏掉对吧？所以它是不是精准度差？<ul>
<li>Google S2会比Geohash精准一点，因为Geohash存在一些很大的突变点，比如两个相隔很近的点刚好分到两个不同的格子的话就会使它们的前缀完全不一样。但是Google S2用的是一种突变没有那么严重的Pieano Curve，因此精准度会高一点。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="GeoHash"><a href="#GeoHash" class="headerlink" title="GeoHash"></a>GeoHash</h2><p>把地球表面分成一個個大格子，並用各個char代表，多次畫分成32份下去，無限逼近所在的位置。</p>
<p>公共長綴愈長，兩點愈來愈接近</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdl1zh1cgbj30sg0f2ta2.jpg" alt="image-20200407114112281" style="zoom: 33%;" />

<blockquote>
<ul>
<li>geohash算法中，如果两个点在分界两边，但其实离得很近，算法会分成两个不同的字母数字么？造成的误差怎么处理呢？<ul>
<li>会很大。一般不处理。因为人会走动，走两步，这个误差就没有了。系统设计不需要保证 100% 正确性。后面的讲解中也会提到。</li>
</ul>
</li>
<li>如何处理处在分界线两边很靠近的两点的情况？比如美国的两点很接近，但分属9和d?<ul>
<li>这个是GeoHash的一个缺陷，即使两个点相隔很近，其字符串的前缀也有可能完全不一样。不过在实际应用中可以有一些方法来弥补，比如在数据库中找距离x点在一定范围内的所有点的时候，除了搜索跟ｘ在一个小格子内的点之外，还可以搜索出于ｘ所在的格子相零的八个格子内的点，然后再汇总结果，这样就不会漏掉答案了。</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>為何是4x8? 而不是5x10、3x6這類的1:2?<ul>
<li>因為GeoHash是一個二分法，實數可以無限往下二分除，得到很長的０１串</li>
<li>base32 去遞歸地劃地圖 <img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdl29586ydj30c808wjt0.jpg" alt="image-20200407115146360" style="zoom: 33%;" /></li>
<li>靠得近有時候geohash還是差得大，但人剛好處於這個大分界上的機率很小，可能下一秒就不在這個分界線上了</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>Geohash为什么不直接比较二进制数，而是比较hash以后的string呢？<ul>
<li>为了最终得出来的表示是整数位，base必须选取2的次方，因此可以将geohash存成2进制、4进制、8进制….等等。这里之所以选32进制是因为这样比较compact，而且最终生成出来的字符串可以被放在url里面，比如<a href="http://api.com/get_nearby_taxi/9q9hu3hhsjxx" target="_blank" rel="noopener">http://api.com/get_nearby_taxi/9q9hu3hhsjxx</a></li>
</ul>
</li>
<li>Geohash为什么不直接比较二进制数，而是比较hash以后的string呢？<ul>
<li>因为 GeoHash 如果存成二进制的话，肉眼不可读，不利于放在 url 里传播，也不利于别人复制粘贴。有方便“人”使用的考虑，如果只是给计算机看，二进制当然是可以的。</li>
</ul>
</li>
<li>geo hash 为什么要比较string？ 不直接比较二进制数？<ul>
<li>GeoHash 用字符串有如下一些好处：<ol>
<li>可读性高，可以直接让人阅读和复制，类似登陆验证码你不能给人一个二进制让人输入吧</li>
<li>可以放在 url 里，方便 url 的传播。</li>
</ol>
</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Range-Query-in-GeoHash-in-DB"><a href="#Range-Query-in-GeoHash-in-DB" class="headerlink" title="Range Query in GeoHash in DB"></a>Range Query in GeoHash in DB</h3><p>視需求要在多少距離內去map找 longest-common-prefix</p>
<p>e.g. 找附近20km的車</p>
<blockquote>
<ul>
<li><p>这里cassandra的row key是什么呢？</p>
<ul>
<li>在 GeoHash 的例子中，row_key 可以是前 4 位的 GeoHash，后面的内容里有提到如何 sharding 的问题。</li>
</ul>
</li>
<li><p>这种找共同prefix的问题是不是通过Trie来实现更高效啊。Redis那种方法其实就是通过hashtable的方式实现trie. 那SQL和Cassandra为什么不通过trie来实现呢，节省空间吗？</p>
<ul>
<li><p>Trie 和 Hash 的效率是一样的，查找一次时间复杂度都是 O(L)，L 是字符串长度（你需要去九章算法强化班补习一下这部分的内容哦）Trie 唯一优势是空间稍微省一些。像 SQL 和 Cassandra 这种数据库的结构是不支持 Trie 的。Trie 是算法和数据结构领域的东西，而不是数据库领域的东西。没有任何数据结构原生的支持 Trie，但是所有 key-value db 都可以认为是一个 hash table on disk。</p>
<p>TODO: <em>多比較此概念, <a href="https://leetcode.com/problems/longest-common-prefix/solution/" target="_blank" rel="noopener">https://leetcode.com/problems/longest-common-prefix/solution/</a></em></p>
</li>
</ul>
</li>
<li><p><strong><em>Redis的情况是要建立32^6+32^5+32^4个key吗？按讲义，9qhhvt,9q9hv,9q9h都是key</em></strong></p>
<ul>
<li>是的，理论上最多会有那么多个key，不过考虑到有很多无人区（海洋，荒山，沙漠啥的），而且公司业务一般不会完全覆盖整个地球，所以实际的数量其实不会达到那么多。如果真的一台机器放不下的话还可以sharding。</li>
</ul>
</li>
<li><p>cassandra的value没有null明白了，那么可不可以有“”(空字符串)呢?另外，cassandra是不是可以不设置columnkey从而只有rowkey和value呢？谢谢。</p>
<ul>
<li>不可以不设置 column key ，否则无法排序<code>&lt;row key, column key&gt;</code> 这个二元组。也无法对 column key 进行 range query。你的需求适合直接放在 RocksDb 这种只有 key-value 的结构里，不适合放在 row key column key value 的这种结构。</li>
</ul>
</li>
<li><p>相当于一个driver的位置要放到三个key的set里面？然后会根据driver位置的变化，会动态地增删这三个set？</p>
<ul>
<li>是的，如果位置分三级的话就是要动态增删三个色它</li>
</ul>
</li>
<li><p>cassandra 的 rowkey 和 column key 的差别是什么啊？ 可以简单说一下吗？</p>
<ul>
<li>Cassandra 中的 row key 就是 hash key 也叫做 partition key，是每次查询必须带上的（第二第三节课里有讲哦~），主要作用是当确定这个数据存在什么地方，你可以认为有一个函数叫做 <code>find_db_instance_by_row_key(row_key)</code> 能够根据 row_key 确定使用哪个 db instance，从而实现分布式存储。<br>很多 NoSQL 数据库没有 column key，比如 Redis。 Cassandra 有 Column key 的作用是，在一台数据库的机器上，所有数据是按照 row_key+column key 排好序的。所以当你确定了 row_key 是啥以后，就可以对 column key 进行范围查询，比如你可以查询 user_id（rowkey） 发的所有帖子里，在 昨天到今天这个范围内发的帖子（帖子的发表时间作为 column key），而 Redis 因为没有 column key 是做不到的。另外在后面的 Big Table 中，会详细介绍这类数据库的实现原理（BigTable 也是一个有 column key 的 db）</li>
</ul>
</li>
<li><p>cassandra把geohash放在column key，那row key该存什么，可以全都存在同一个row key下吗？例如null？</p>
<ul>
<li>Row key 存 geohash 的前 4 位。</li>
</ul>
</li>
<li><p>cassandra的row key是什么</p>
<ul>
<li>前 4 位 GeoHash</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="雙方角度思考業務跟儲存需求"><a href="#雙方角度思考業務跟儲存需求" class="headerlink" title="雙方角度思考業務跟儲存需求"></a>雙方角度思考業務跟儲存需求</h3><blockquote>
<ul>
<li>为什么不需要对driver按离rider距离远近排序？<ul>
<li>要排也行，可以在match的时候将最近的区域内的司机按距离排个序，然后选最近的。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h3><p>用戶無遷移成本，所以用戶黏性太差，分分鐘用戶就換別的用。</p>
<blockquote>
<h5 id="单选题-司机的-Location-信息应该按照什么进行-Sharding？"><a href="#单选题-司机的-Location-信息应该按照什么进行-Sharding？" class="headerlink" title="[单选题]司机的 Location 信息应该按照什么进行 Sharding？"></a>[单选题]司机的 Location 信息应该按照什么进行 Sharding？</h5><p>A.按照 User Id6.23% 选择</p>
<p>B.整个 GeoHash10.18% 选择</p>
<p>C.GeoHash的前4位72.04% 选择</p>
<p>D.GeoHash的前6位11.56% 选择</p>
<p><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNTM0MTgxMjgxODM5IiBjbGFzcz0iaWNvbiIgc3R5bGU9IiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjM3NjIiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMzIiIGhlaWdodD0iMzIiPjxkZWZzPjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+PC9zdHlsZT48L2RlZnM+PHBhdGggZD0iTTUxOC4xMiA1MTYuMTZtLTQ5MCAwYTQ5MCA0OTAgMCAxIDAgOTgwIDAgNDkwIDQ5MCAwIDEgMC05ODAgMFoiIGZpbGw9IiM1NkI0MzIiIHAtaWQ9IjM3NjMiPjwvcGF0aD48cGF0aCBkPSJNMzkzLjIxMzYxOSA2NjQuMzM1NDk1bTI4LjI4NDI3MS0yOC4yODQyNzFsMjk2Ljk4NDg0OS0yOTYuOTg0ODQ4cTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBsMCAwcTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDJsLTI5Ni45ODQ4NDggMjk2Ljk4NDg0OHEtMjguMjg0MjcxIDI4LjI4NDI3MS01Ni41Njg1NDMgMGwwIDBxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDJaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY0Ij48L3BhdGg+PHBhdGggZD0iTTI4OS40Njk4NCA0NTIuODQ3ODgzbTI4LjI4NDI3MSAyOC4yODQyNzFsMTU1LjU2MzQ5MiAxNTUuNTYzNDkycTI4LjI4NDI3MSAyOC4yODQyNzEgMCA1Ni41Njg1NDNsMCAwcS0yOC4yODQyNzEgMjguMjg0MjcxLTU2LjU2ODU0MyAwbC0xNTUuNTYzNDkxLTE1NS41NjM0OTJxLTI4LjI4NDI3MS0yOC4yODQyNzEgMC01Ni41Njg1NDNsMCAwcTI4LjI4NDI3MS0yOC4yODQyNzEgNTYuNTY4NTQyIDBaIiBmaWxsPSIjRkZGRkZGIiBwLWlkPSIzNzY1Ij48L3BhdGg+PC9zdmc+" alt="img">答对了，您选择的答案是C</p>
<p><strong>正确答案:</strong>C</p>
<p><strong>解析:</strong></p>
<p>按照 User Id 是显然不对的，因为乘客的 UserId 和他附近的司机的 UserId 可能会被 sharding 到不同的地方。<br>按照 GeoHash 进行 Sharding 的话，只需要按照前 4 位进行 sharding 即可。因为乘客的查询可能是按照前4，5，6位 GeoHash 进行查询，如果是按照前 6 位进行 sharding 的话，那么 BBBB22 和 BBBB33 就会被拆分开，而我们查询的时候其实希望如果用户在 BBBB22 的话，他能够查到位置在 BBBB33 的司机。</p>
</blockquote>
<p>城市不多，每個城市也是幾10個點圍出來的，可以在code裡直接寫死。</p>
<blockquote>
<ul>
<li>redis内部有自己的sharding机制么？需要自己手动写sharding算法还是只需要给sharding的column就可以？<ul>
<li>早期的版本没有。最近新出的版本里有 auto-sharding 的机制了，但是不是 consistent hashing 的算法，用的是另外一个，有兴趣的话，可以去网上搜一下相关的内容。</li>
</ul>
</li>
<li>但是有的城市uber的用户比较多，比较频发，有的城市用户比较少，怎么样能sharding的均匀呢？<ul>
<li>Uber 的做法是为用户多的城市多配置机器。用户少的城市少配置机器。</li>
</ul>
</li>
<li>按照city sharding，如果这个城市里有很多driver，该如何判断哪一个driver离这个乘客比较近？<ul>
<li>按照 city sharding 以后还是需要根据 geohash 去 filter 的。sharding 只是宏观上数据怎么拆分，geohash 是微观上具体每一条数据存储时的信息。我们根据 <strong>geohash</strong> 可以做范围查询来 filter 离乘客比较近的 driver。</li>
</ul>
</li>
<li>找到乘客周围的2-3 个城市怎么就能避免乘客算a city driver 算b city 但是其实两个人离得很近的问题？<ul>
<li>他俩隔得很近，那么说明 a city 和 b city 隔得很近。因此找到乘客周围 2-3 个城市的意思就是，既让乘客属于 a city ，也让他属于 b city，总共查2-3次。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Riak"><a href="#Riak" class="headerlink" title="Riak"></a>Riak</h3><p>Redis 有 Master slave的機制，M掛了S可以頂上去。</p>
<p>但其實可以用穩定性更強的DB, 更好處理掛掉後恢復的問題。</p>
<p>Uber也是從Redis換成了Riak。</p>
<blockquote>
<ul>
<li>master挂了的话，slave可以进行写操作吗<ul>
<li>不行，必须起一个新的master。</li>
</ul>
</li>
<li>之前课程不是说涉及transaction要用SQL吗？ 为何这边都是用NoSQL？<ul>
<li>这里并不需要用到 transaction，所以用 NoSQL 没有问题。且有一些 NoSQL 也支持 Transaction 了。</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdj8kcgjkzj30kc0m8gn8.jpg" alt="image-20200405215453781" style="zoom: 25%;" />



<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdj8kg0owrj30kc0ws779.jpg" alt="image-20200405215851312" style="zoom:67%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200407165538615.png" alt="image-20200407165538615" style="zoom:67%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdlb1n83q9j30oc0j8tby.jpg" alt="image-20200407165557659" style="zoom:67%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdlb2i72zyj30ow0msn15.jpg" alt="image-20200407165647857" style="zoom:67%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdlb36wqt9j30pa0t00yp.jpg" alt="image-20200407165727152" style="zoom:67%;" />



<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giblli29hij31gm0u07q8.jpg" alt="image-20200902000656510" style="zoom:67%;" />

</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/63/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/63/">63</a><span class="page-number current">64</span><a class="page-number" href="/page/65/">65</a><span class="space">&hellip;</span><a class="page-number" href="/page/77/">77</a><a class="extend next" rel="next" href="/page/65/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2023 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>