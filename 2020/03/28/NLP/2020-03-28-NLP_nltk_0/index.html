<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="NLP/2020-03-28-NLP_nltk_0"><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>NLP/2020-03-28-NLP_nltk_0 | Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.1.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Text-Preprocessing"><span class="toc-number">1.</span> <span class="toc-text">Text Preprocessing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parsing-Text"><span class="toc-number">2.</span> <span class="toc-text">Parsing Text</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Language-Models-Bag-of-Words-Approach"><span class="toc-number">3.</span> <span class="toc-text">Language Models - Bag-of-Words Approach</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Language-Models-N-Grams-and-NLM"><span class="toc-number">4.</span> <span class="toc-text">Language Models - N-Grams and NLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Text-Similarity"><span class="toc-number">5.</span> <span class="toc-text">Text Similarity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Language-Prediction-amp-Text-Generation"><span class="toc-number">6.</span> <span class="toc-text">Language Prediction &amp; Text Generation</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">298</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">25</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">45</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">NLP/2020-03-28-NLP_nltk_0</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-28</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="Text-Preprocessing"><a href="#Text-Preprocessing" class="headerlink" title="Text Preprocessing"></a>Text Preprocessing</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># regex for removing punctuation!</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># nltk preprocessing magic</span></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="comment"># grabbing a part of speech function:</span></span><br><span class="line"><span class="keyword">from</span> part_of_speech <span class="keyword">import</span> get_part_of_speech</span><br><span class="line"></span><br><span class="line">text = <span class="string">"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed."</span></span><br><span class="line"></span><br><span class="line">cleaned = re.sub(<span class="string">'\W+'</span>, <span class="string">' '</span>, text)</span><br><span class="line">tokenized = word_tokenize(cleaned)</span><br><span class="line"></span><br><span class="line">stemmer = PorterStemmer()</span><br><span class="line">stemmed = [stemmer.stem(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokenized]</span><br><span class="line"></span><br><span class="line"><span class="comment">## -- CHANGE these -- ##</span></span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line">lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) <span class="keyword">for</span> token <span class="keyword">in</span> tokenized]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Stemmed text:"</span>)</span><br><span class="line">print(stemmed)</span><br><span class="line">print(<span class="string">"\nLemmatized text:"</span>)</span><br><span class="line">print(lemmatized)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Stemmed text:</span><br><span class="line">[<span class="string">'So'</span>, <span class="string">'mani'</span>, <span class="string">'squid'</span>, <span class="string">'are'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'suitcas'</span>, <span class="string">'these'</span>, <span class="string">'day'</span>, <span class="string">'that'</span>, <span class="string">'you'</span>, <span class="string">'can'</span>, <span class="string">'bare'</span>, <span class="string">'go'</span>, <span class="string">'anywher'</span>, <span class="string">'without'</span>, <span class="string">'see'</span>, <span class="string">'one'</span>, <span class="string">'burst'</span>, <span class="string">'forth'</span>, <span class="string">'from'</span>, <span class="string">'a'</span>, <span class="string">'tightli'</span>, <span class="string">'pack'</span>, <span class="string">'valis'</span>, <span class="string">'I'</span>, <span class="string">'went'</span>, <span class="string">'to'</span>, <span class="string">'the'</span>, <span class="string">'dentist'</span>, <span class="string">'the'</span>, <span class="string">'other'</span>, <span class="string">'day'</span>, <span class="string">'and'</span>, <span class="string">'sure'</span>, <span class="string">'enough'</span>, <span class="string">'I'</span>, <span class="string">'saw'</span>, <span class="string">'an'</span>, <span class="string">'angri'</span>, <span class="string">'one'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'my'</span>, <span class="string">'dentist'</span>, <span class="string">'s'</span>, <span class="string">'bag'</span>, <span class="string">'within'</span>, <span class="string">'minut'</span>, <span class="string">'of'</span>, <span class="string">'arriv'</span>, <span class="string">'she'</span>, <span class="string">'hardli'</span>, <span class="string">'even'</span>, <span class="string">'notic'</span>]</span><br><span class="line"></span><br><span class="line">Lemmatized text:</span><br><span class="line">[<span class="string">'So'</span>, <span class="string">'many'</span>, <span class="string">'squid'</span>, <span class="string">'be'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'suitcase'</span>, <span class="string">'these'</span>, <span class="string">'day'</span>, <span class="string">'that'</span>, <span class="string">'you'</span>, <span class="string">'can'</span>, <span class="string">'barely'</span>, <span class="string">'go'</span>, <span class="string">'anywhere'</span>, <span class="string">'without'</span>, <span class="string">'see'</span>, <span class="string">'one'</span>, <span class="string">'burst'</span>, <span class="string">'forth'</span>, <span class="string">'from'</span>, <span class="string">'a'</span>, <span class="string">'tightly'</span>, <span class="string">'pack'</span>, <span class="string">'valise'</span>, <span class="string">'I'</span>, <span class="string">'go'</span>, <span class="string">'to'</span>, <span class="string">'the'</span>, <span class="string">'dentist'</span>, <span class="string">'the'</span>, <span class="string">'other'</span>, <span class="string">'day'</span>, <span class="string">'and'</span>, <span class="string">'sure'</span>, <span class="string">'enough'</span>, <span class="string">'I'</span>, <span class="string">'saw'</span>, <span class="string">'an'</span>, <span class="string">'angry'</span>, <span class="string">'one'</span>, <span class="string">'jump'</span>, <span class="string">'out'</span>, <span class="string">'of'</span>, <span class="string">'my'</span>, <span class="string">'dentist'</span>, <span class="string">'s'</span>, <span class="string">'bag'</span>, <span class="string">'within'</span>, <span class="string">'minute'</span>, <span class="string">'of'</span>, <span class="string">'arrive'</span>, <span class="string">'She'</span>, <span class="string">'hardly'</span>, <span class="string">'even'</span>, <span class="string">'notice'</span>]</span><br></pre></td></tr></table></figure>



<h3 id="Parsing-Text"><a href="#Parsing-Text" class="headerlink" title="Parsing Text"></a>Parsing Text</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> Tree</span><br><span class="line"><span class="keyword">from</span> squids <span class="keyword">import</span> squids_text</span><br><span class="line"></span><br><span class="line">dependency_parser = spacy.load(<span class="string">'en'</span>)</span><br><span class="line"></span><br><span class="line">parsed_squids = dependency_parser(squids_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assign my_sentence a new value:</span></span><br><span class="line">my_sentence = <span class="string">"Your sentence goes here!"</span></span><br><span class="line">my_parsed_sentence = dependency_parser(my_sentence)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_nltk_tree</span><span class="params">(node)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> node.n_lefts + node.n_rights &gt; <span class="number">0</span>:</span><br><span class="line">    parsed_child_nodes = [to_nltk_tree(child) <span class="keyword">for</span> child <span class="keyword">in</span> node.children]</span><br><span class="line">    <span class="keyword">return</span> Tree(node.orth_, parsed_child_nodes)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">return</span> node.orth_</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> parsed_squids.sents:</span><br><span class="line">  to_nltk_tree(sent.root).pretty_print()</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> my_parsed_sentence.sents:</span><br><span class="line"> to_nltk_tree(sent.root).pretty_print()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">        jumping                       </span><br><span class="line">  _________|_______________________    </span><br><span class="line"> |   |     |      |       out      |  </span><br><span class="line"> |   |     |      |        |       |   </span><br><span class="line"> |   |     |    squids     of     days</span><br><span class="line"> |   |     |      |        |       |   </span><br><span class="line"> So are    .     many  suitcases these</span><br><span class="line"></span><br><span class="line">          go                       </span><br><span class="line">  ________|____________________     </span><br><span class="line"> |   |    |       |      |  without</span><br><span class="line"> |   |    |       |      |     |    </span><br><span class="line"> |   |    |       |      |   seeing</span><br><span class="line"> |   |    |       |      |     |    </span><br><span class="line">You can barely anywhere  .    one  </span><br><span class="line"></span><br><span class="line">          went               </span><br><span class="line">  _________|_________         </span><br><span class="line"> |   |     to        |       </span><br><span class="line"> |   |     |         |        </span><br><span class="line"> |   |  dentist     day      </span><br><span class="line"> |   |     |      ___|____    </span><br><span class="line"> I   .    the   the     other</span><br><span class="line"></span><br><span class="line">             saw                                     </span><br><span class="line">  ____________|___________________                    </span><br><span class="line"> |   |   |    |                  jump                </span><br><span class="line"> |   |   |    |          _________|__________         </span><br><span class="line"> |   |   |    |         |                   out      </span><br><span class="line"> |   |   |    |         |                    |        </span><br><span class="line"> |   |   |    |         |                    of      </span><br><span class="line"> |   |   |    |         |                    |        </span><br><span class="line"> |   |   |    |         |                   bag      </span><br><span class="line"> |   |   |    |         |                    |        </span><br><span class="line"> |   |   |  enough     one                dentist    </span><br><span class="line"> |   |   |    |      ___|____           _____|_____   </span><br><span class="line"> ,   I   .   Sure   an     angry       my          <span class="string">'s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    noticed         </span></span><br><span class="line"><span class="string">  _____|__________   </span></span><br><span class="line"><span class="string">She  hardly even  . </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     goes         </span></span><br><span class="line"><span class="string">  ____|______      </span></span><br><span class="line"><span class="string"> |    |   sentence</span></span><br><span class="line"><span class="string"> |    |      |     </span></span><br><span class="line"><span class="string">here  !     Your</span></span><br></pre></td></tr></table></figure>



<h3 id="Language-Models-Bag-of-Words-Approach"><a href="#Language-Models-Bag-of-Words-Approach" class="headerlink" title="Language Models - Bag-of-Words Approach"></a>Language Models - Bag-of-Words Approach</h3><p>When grammar and word order are irrelevant, this is probably a good model to use.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># importing regex and nltk</span></span><br><span class="line"><span class="keyword">import</span> re, nltk</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="comment"># importing Counter to get word counts for bag of words</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="comment"># importing a passage from Through the Looking Glass</span></span><br><span class="line"><span class="keyword">from</span> looking_glass <span class="keyword">import</span> looking_glass_text</span><br><span class="line"><span class="comment"># importing part-of-speech function for lemmatization</span></span><br><span class="line"><span class="keyword">from</span> part_of_speech <span class="keyword">import</span> get_part_of_speech</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change text to another string:</span></span><br><span class="line"><span class="comment"># text = looking_glass_text</span></span><br><span class="line">text = <span class="string">"hello world i miss you"</span></span><br><span class="line"></span><br><span class="line">cleaned = re.sub(<span class="string">'\W+'</span>, <span class="string">' '</span>, text).lower()</span><br><span class="line">tokenized = word_tokenize(cleaned)</span><br><span class="line"></span><br><span class="line">stop_words = stopwords.words(<span class="string">'english'</span>)</span><br><span class="line">filtered = [word <span class="keyword">for</span> word <span class="keyword">in</span> tokenized <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"></span><br><span class="line">normalizer = WordNetLemmatizer()</span><br><span class="line">normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) <span class="keyword">for</span> token <span class="keyword">in</span> filtered]</span><br><span class="line"><span class="comment"># Comment out the print statement below</span></span><br><span class="line">print(normalized)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define bag_of_looking_glass_words &amp; print:</span></span><br><span class="line">bag_of_looking_glass_words = Counter(normalized)</span><br><span class="line">print(bag_of_looking_glass_words)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'hello'</span>, <span class="string">'world'</span>, <span class="string">'miss'</span>]</span><br><span class="line">Counter(&#123;<span class="string">'hello'</span>: 1, <span class="string">'world'</span>: 1, <span class="string">'miss'</span>: 1&#125;)</span><br></pre></td></tr></table></figure>



<h3 id="Language-Models-N-Grams-and-NLM"><a href="#Language-Models-N-Grams-and-NLM" class="headerlink" title="Language Models - N-Grams and NLM"></a>Language Models - N-Grams and NLM</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk, re</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="comment"># importing ngrams module from nltk</span></span><br><span class="line"><span class="keyword">from</span> nltk.util <span class="keyword">import</span> ngrams</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> looking_glass <span class="keyword">import</span> looking_glass_full_text</span><br><span class="line"></span><br><span class="line">cleaned = re.sub(<span class="string">'\W+'</span>, <span class="string">' '</span>, looking_glass_full_text).lower()</span><br><span class="line">tokenized = word_tokenize(cleaned)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change the n value to 2:</span></span><br><span class="line">looking_glass_bigrams = ngrams(tokenized, <span class="number">2</span>)</span><br><span class="line">looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change the n value to 3:</span></span><br><span class="line">looking_glass_trigrams = ngrams(tokenized, <span class="number">3</span>)</span><br><span class="line">looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change the n value to a number greater than 3:</span></span><br><span class="line">looking_glass_ngrams = ngrams(tokenized, <span class="number">4</span>)</span><br><span class="line">looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Looking Glass Bigrams:"</span>)</span><br><span class="line">print(looking_glass_bigrams_frequency.most_common(<span class="number">10</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\nLooking Glass Trigrams:"</span>)</span><br><span class="line">print(looking_glass_trigrams_frequency.most_common(<span class="number">10</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">n = <span class="number">3</span></span><br><span class="line">print(<span class="string">"\nLooking Glass n-grams:"</span>)</span><br><span class="line">print(looking_glass_ngrams_frequency.most_common(<span class="number">10</span>), <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Looking Glass Bigrams:</span><br><span class="line">[((<span class="string">'of'</span>, <span class="string">'the'</span>), 101), ((<span class="string">'said'</span>, <span class="string">'the'</span>), 98), ((<span class="string">'in'</span>, <span class="string">'a'</span>), 97), ((<span class="string">'in'</span>, <span class="string">'the'</span>), 90), ((<span class="string">'as'</span>, <span class="string">'she'</span>), 82), ((<span class="string">'you'</span>, <span class="string">'know'</span>), 72), ((<span class="string">'a'</span>, <span class="string">'little'</span>), 68), ((<span class="string">'the'</span>, <span class="string">'queen'</span>), 67), ((<span class="string">'said'</span>, <span class="string">'alice'</span>), 67), ((<span class="string">'to'</span>, <span class="string">'the'</span>), 66)] 2</span><br><span class="line"></span><br><span class="line">Looking Glass Trigrams:</span><br><span class="line">[((<span class="string">'the'</span>, <span class="string">'red'</span>, <span class="string">'queen'</span>), 54), ((<span class="string">'the'</span>, <span class="string">'white'</span>, <span class="string">'queen'</span>), 31), ((<span class="string">'said'</span>, <span class="string">'in'</span>, <span class="string">'a'</span>), 21), ((<span class="string">'she'</span>, <span class="string">'went'</span>, <span class="string">'on'</span>), 18), ((<span class="string">'said'</span>, <span class="string">'the'</span>, <span class="string">'red'</span>), 17), ((<span class="string">'thought'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 16), ((<span class="string">'the'</span>, <span class="string">'queen'</span>, <span class="string">'said'</span>), 16), ((<span class="string">'said'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 14), ((<span class="string">'said'</span>, <span class="string">'humpty'</span>, <span class="string">'dumpty'</span>), 14), ((<span class="string">'the'</span>, <span class="string">'knight'</span>, <span class="string">'said'</span>), 14)] 3</span><br><span class="line"></span><br><span class="line">Looking Glass n-grams:</span><br><span class="line">[((<span class="string">'said'</span>, <span class="string">'the'</span>, <span class="string">'red'</span>, <span class="string">'queen'</span>), 15), ((<span class="string">'she'</span>, <span class="string">'said'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 11), ((<span class="string">'alice'</span>, <span class="string">'thought'</span>, <span class="string">'to'</span>, <span class="string">'herself'</span>), 9), ((<span class="string">'to'</span>, <span class="string">'herself'</span>, <span class="string">'as'</span>, <span class="string">'she'</span>), 9), ((<span class="string">'one'</span>, <span class="string">'and'</span>, <span class="string">'one'</span>, <span class="string">'and'</span>), 8), ((<span class="string">'and'</span>, <span class="string">'one'</span>, <span class="string">'and'</span>, <span class="string">'one'</span>), 8), ((<span class="string">'alice'</span>, <span class="string">'said'</span>, <span class="string">'in'</span>, <span class="string">'a'</span>), 6), ((<span class="string">'for'</span>, <span class="string">'a'</span>, <span class="string">'minute'</span>, <span class="string">'or'</span>), 6), ((<span class="string">'a'</span>, <span class="string">'minute'</span>, <span class="string">'or'</span>, <span class="string">'two'</span>), 6), ((<span class="string">'in'</span>, <span class="string">'a'</span>, <span class="string">'tone'</span>, <span class="string">'of'</span>), 6)] 5</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk, re</span><br><span class="line"><span class="keyword">from</span> sherlock_holmes <span class="keyword">import</span> bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3</span><br><span class="line"><span class="keyword">from</span> preprocessing <span class="keyword">import</span> preprocess_text</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> LatentDirichletAllocation</span><br><span class="line"></span><br><span class="line"><span class="comment"># preparing the text</span></span><br><span class="line">corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]</span><br><span class="line">preprocessed_corpus = [preprocess_text(chapter) <span class="keyword">for</span> chapter <span class="keyword">in</span> corpus]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Update stop_list:</span></span><br><span class="line">stop_list = []</span><br><span class="line"><span class="comment"># filtering topics for stop words</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_out_stop_words</span><span class="params">(corpus)</span>:</span></span><br><span class="line">  no_stops_corpus = []</span><br><span class="line">  <span class="keyword">for</span> chapter <span class="keyword">in</span> corpus:</span><br><span class="line">    no_stops_chapter = <span class="string">" "</span>.join([word <span class="keyword">for</span> word <span class="keyword">in</span> chapter.split(<span class="string">" "</span>) <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> stop_list])</span><br><span class="line">    no_stops_corpus.append(no_stops_chapter)</span><br><span class="line">  <span class="keyword">return</span> no_stops_corpus</span><br><span class="line">filtered_for_stops = filter_out_stop_words(preprocessed_corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the bag of words model</span></span><br><span class="line">bag_of_words_creator = CountVectorizer()</span><br><span class="line">bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the tf-idf model</span></span><br><span class="line">tfidf_creator = TfidfVectorizer(min_df = <span class="number">0.2</span>)</span><br><span class="line">tfidf = tfidf_creator.fit_transform(preprocessed_corpus)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the bag of words LDA model</span></span><br><span class="line">lda_bag_of_words_creator = LatentDirichletAllocation(learning_method=<span class="string">'online'</span>, n_components=<span class="number">10</span>)</span><br><span class="line">lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># creating the tf-idf LDA model</span></span><br><span class="line">lda_tfidf_creator = LatentDirichletAllocation(learning_method=<span class="string">'online'</span>, n_components=<span class="number">10</span>)</span><br><span class="line">lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"~~~ Topics found by bag of words LDA ~~~"</span>)</span><br><span class="line"><span class="keyword">for</span> topic_id, topic <span class="keyword">in</span> enumerate(lda_bag_of_words_creator.components_):</span><br><span class="line">  message = <span class="string">"Topic #&#123;&#125;: "</span>.format(topic_id + <span class="number">1</span>)</span><br><span class="line">  message += <span class="string">" "</span>.join([bag_of_words_creator.get_feature_names()[i] <span class="keyword">for</span> i <span class="keyword">in</span> topic.argsort()[:<span class="number">-5</span> :<span class="number">-1</span>]])</span><br><span class="line">  print(message)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\n\n~~~ Topics found by tf-idf LDA ~~~"</span>)</span><br><span class="line"><span class="keyword">for</span> topic_id, topic <span class="keyword">in</span> enumerate(lda_tfidf_creator.components_):</span><br><span class="line">  message = <span class="string">"Topic #&#123;&#125;: "</span>.format(topic_id + <span class="number">1</span>)</span><br><span class="line">  message += <span class="string">" "</span>.join([tfidf_creator.get_feature_names()[i] <span class="keyword">for</span> i <span class="keyword">in</span> topic.argsort()[:<span class="number">-5</span> :<span class="number">-1</span>]])</span><br><span class="line">  print(message)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">~~~ Topics found by bag of words LDA ~~~</span><br><span class="line">Topic <span class="comment">#1: holmes say little upon</span></span><br><span class="line">Topic <span class="comment">#2: house come could man</span></span><br><span class="line">Topic <span class="comment">#3: holmes say know come</span></span><br><span class="line">Topic <span class="comment">#4: holmes would say know</span></span><br><span class="line">Topic <span class="comment">#5: say holmes know see</span></span><br><span class="line">Topic <span class="comment">#6: say holmes man could</span></span><br><span class="line">Topic <span class="comment">#7: say upon mccarthy man</span></span><br><span class="line">Topic <span class="comment">#8: make holmes cry majesty</span></span><br><span class="line">Topic <span class="comment">#9: holmes say man upon</span></span><br><span class="line">Topic <span class="comment">#10: upon holmes see say</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">~~~ Topics found by tf-idf LDA ~~~</span><br><span class="line">Topic <span class="comment">#1: merely upon boot catch</span></span><br><span class="line">Topic <span class="comment">#2: boot save holmes mccarthy</span></span><br><span class="line">Topic <span class="comment">#3: norton resolve help refer</span></span><br><span class="line">Topic <span class="comment">#4: leave remove three lodge</span></span><br><span class="line">Topic <span class="comment">#5: say neck resolute stone</span></span><br><span class="line">Topic <span class="comment">#6: holmes king majesty photograph</span></span><br><span class="line">Topic <span class="comment">#7: fear together heavy upon</span></span><br><span class="line">Topic <span class="comment">#8: holmes say know man</span></span><br><span class="line">Topic <span class="comment">#9: figure surround definite heel</span></span><br><span class="line">Topic <span class="comment">#10: know many swiftly scotland</span></span><br></pre></td></tr></table></figure>



<h3 id="Text-Similarity"><a href="#Text-Similarity" class="headerlink" title="Text Similarity"></a>Text Similarity</h3><p>Most of us have a good autocorrect story. Our phone’s messenger quietly swaps one letter for another as we type and suddenly the meaning of our message has changed (to our horror or pleasure). However, addressing <strong><em>text similarity\</em></strong> — including spelling correction — is a major challenge within natural language processing.</p>
<p>Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering the <strong><em>Levenshtein distance\</em></strong> or minimal edit distance between two words. The distance is calculated through the minimum number of insertions, deletions, and substitutions that would need to occur for one word to become another. For example, turning “bees” into “beans” would require one substitution (“a” for “e”) and one insertion (“n”), so the Levenshtein distance would be two.</p>
<p>Phonetic similarity is also a major challenge within speech recognition. English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia,” but it’s a far more challenging task for a machine! More advanced autocorrect and spelling correction technology additionally considers key distance on a keyboard and <strong><em>phonetic similarity\</em></strong> (how much two words or phrases sound the same).</p>
<p>It’s also helpful to find out if texts are the same to guard against plagiarism, which we can identify through <strong><em>lexical similarity\</em></strong> (the degree to which texts use the same vocabulary and phrases). Meanwhile, <strong><em>semantic similarity\</em></strong> (the degree to which documents contain similar meaning or topics) is useful when you want to find (or recommend) an article or book similar to one you recently finished.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="comment"># NLTK has a built-in function</span></span><br><span class="line"><span class="comment"># to check Levenshtein distance:</span></span><br><span class="line"><span class="keyword">from</span> nltk.metrics <span class="keyword">import</span> edit_distance</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_levenshtein</span><span class="params">(string1, string2)</span>:</span></span><br><span class="line">  print(<span class="string">"The Levenshtein distance from '&#123;0&#125;' to '&#123;1&#125;' is &#123;2&#125;!"</span>.format(string1, string2, edit_distance(string1, string2)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the distance between</span></span><br><span class="line"><span class="comment"># any two words here!</span></span><br><span class="line">print_levenshtein(<span class="string">"fart"</span>, <span class="string">"target"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assign passing strings here:</span></span><br><span class="line">three_away_from_code = <span class="string">"cat"</span></span><br><span class="line"></span><br><span class="line">two_away_from_chunk = <span class="string">"cheek"</span></span><br><span class="line"></span><br><span class="line">print_levenshtein(<span class="string">"code"</span>, three_away_from_code)</span><br><span class="line">print_levenshtein(<span class="string">"chunk"</span>, two_away_from_chunk)</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">The Levenshtein distance from <span class="string">'fart'</span> to <span class="string">'target'</span> is 3!</span><br><span class="line">The Levenshtein distance from <span class="string">'code'</span> to <span class="string">'cat'</span> is 3!</span><br><span class="line">The Levenshtein distance from <span class="string">'chunk'</span> to <span class="string">'cheek'</span> is 2!</span><br></pre></td></tr></table></figure>



<h3 id="Language-Prediction-amp-Text-Generation"><a href="#Language-Prediction-amp-Text-Generation" class="headerlink" title="Language Prediction &amp; Text Generation"></a>Language Prediction &amp; Text Generation</h3><p>How does your favorite search engine complete your search queries? How does your phone’s keyboard know what you want to type next? <strong><em>Language prediction\</em></strong> is an application of NLP concerned with predicting text given preceding text. Autosuggest, autocomplete, and suggested replies are common forms of language prediction.</p>
<p>Your first step to language prediction is picking a language model. Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.</p>
<p>If you go the <em>n</em>-gram route, you will most likely rely on <strong><em>Markov chains\</em></strong> to predict the statistical likelihood of each following word (or character) based on the training corpus. Markov chains are memory-less and make statistical predictions based entirely on the current <em>n</em>-gram on hand.</p>
<p>For example, let’s take a sentence beginning, “I ate so many grilled cheese”. Using a trigram model (where <em>n</em> is 3), a Markov chain would predict the following word as “sandwiches” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data.</p>
<p>A more advanced approach, using a neural language model, is the <strong><em>Long Short Term Memory (LSTM)\</em></strong> model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them better suited for text prediction than traditional neural networks.</p>
<p><strong>1.</strong></p>
<p>Add three short stories by your favorite author or the lyrics to three songs by your favorite artist to <strong>document1.py</strong>, <strong>document2.py</strong>, and <strong>document3.py</strong>. Then run <strong>script.py</strong> to see a short example of text prediction.</p>
<p>Does it look like something by your favorite author or artist?</p>
<p>If you accidentally close one of the files, just click the file folder in the top left corner of the code editor to find the file and re-open it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk, re, random</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, deque</span><br><span class="line"><span class="keyword">from</span> document1 <span class="keyword">import</span> training_doc1</span><br><span class="line"><span class="keyword">from</span> document2 <span class="keyword">import</span> training_doc2</span><br><span class="line"><span class="keyword">from</span> document3 <span class="keyword">import</span> training_doc3</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MarkovChain</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.lookup_dict = defaultdict(list)</span><br><span class="line">    self._seeded = <span class="literal">False</span></span><br><span class="line">    self.__seed_me()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__seed_me</span><span class="params">(self, rand_seed=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self._seeded <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">True</span>:</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> rand_seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">          random.seed(rand_seed)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          random.seed()</span><br><span class="line">        self._seeded = <span class="literal">True</span></span><br><span class="line">      <span class="keyword">except</span> NotImplementedError:</span><br><span class="line">        self._seeded = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add_document</span><span class="params">(self, str)</span>:</span></span><br><span class="line">    preprocessed_list = self._preprocess(str)</span><br><span class="line">    pairs = self.__generate_tuple_keys(preprocessed_list)</span><br><span class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> pairs:</span><br><span class="line">      self.lookup_dict[pair[<span class="number">0</span>]].append(pair[<span class="number">1</span>])</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_preprocess</span><span class="params">(self, str)</span>:</span></span><br><span class="line">    cleaned = re.sub(<span class="string">r'\W+'</span>, <span class="string">' '</span>, str).lower()</span><br><span class="line">    tokenized = word_tokenize(cleaned)</span><br><span class="line">    <span class="keyword">return</span> tokenized</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__generate_tuple_keys</span><span class="params">(self, data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(data) &lt; <span class="number">1</span>:</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data) - <span class="number">1</span>):</span><br><span class="line">      <span class="keyword">yield</span> [ data[i], data[i + <span class="number">1</span>] ]</span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">generate_text</span><span class="params">(self, max_length=<span class="number">50</span>)</span>:</span></span><br><span class="line">    context = deque()</span><br><span class="line">    output = []</span><br><span class="line">    <span class="keyword">if</span> len(self.lookup_dict) &gt; <span class="number">0</span>:</span><br><span class="line">      self.__seed_me(rand_seed=len(self.lookup_dict))</span><br><span class="line">      chain_head = [list(self.lookup_dict)[<span class="number">0</span>]]</span><br><span class="line">      context.extend(chain_head)</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">while</span> len(output) &lt; (max_length - <span class="number">1</span>):</span><br><span class="line">        next_choices = self.lookup_dict[context[<span class="number">-1</span>]]</span><br><span class="line">        <span class="keyword">if</span> len(next_choices) &gt; <span class="number">0</span>:</span><br><span class="line">          next_word = random.choice(next_choices)</span><br><span class="line">          context.append(next_word)</span><br><span class="line">          output.append(context.popleft())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">      output.extend(list(context))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">" "</span>.join(output)</span><br><span class="line"></span><br><span class="line">my_markov = MarkovChain()</span><br><span class="line">my_markov.add_document(training_doc1)</span><br><span class="line">my_markov.add_document(training_doc2)</span><br><span class="line">my_markov.add_document(training_doc3)</span><br><span class="line">generated_text = my_markov.generate_text()</span><br><span class="line">print(generated_text)</span><br></pre></td></tr></table></figure>

</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Joe Huang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/03/28/NLP/2020-03-28-NLP_nltk_0/">http://yoursite.com/2020/03/28/NLP/2020-03-28-NLP_nltk_0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/04/09/jupyter-demo/2020-04-09-SNPS%20stock%20prediction/"><i class="fa fa-chevron-left">  </i><span>jupyter-demo/2020-04-09-SNPS stock prediction</span></a></div><div class="next-post pull-right"><a href="/2020/03/28/Web/2020-03-28-Full%20Stack%20review/"><span>Web/2020-03-28-Full Stack review</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2020 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>