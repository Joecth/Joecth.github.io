<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="BigData_java/2020-12-19-LAB3: Hadoop based DFS &amp; Hive on EMR "><meta name="keywords" content=""><meta name="author" content="Joe Huang"><meta name="copyright" content="Joe Huang"><title>BigData_java/2020-12-19-LAB3: Hadoop based DFS &amp; Hive on EMR  | Awaken Desparado</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-180692466-1', 'auto');
ga('send', 'pageview');</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS-Hadoop-based-DFS-amp-Hive-on-EMR"><span class="toc-number">1.</span> <span class="toc-text">HDFS(Hadoop based DFS) &amp; Hive on EMR</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Access-Remote-EMR-clusters"><span class="toc-number">1.1.</span> <span class="toc-text">Access Remote EMR clusters</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS"><span class="toc-number">1.1.0.1.</span> <span class="toc-text">HDFS</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS-commands"><span class="toc-number">1.2.</span> <span class="toc-text">HDFS commands</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#S3-–-基於AWS的-dfs，HDFS-–-基於Hadoop的-dfs，本質是非常相似的，只不過S3是UI介面"><span class="toc-number">1.2.0.0.1.</span> <span class="toc-text">S3 – 基於AWS的 dfs，HDFS – 基於Hadoop的 dfs，本質是非常相似的，只不過S3是UI介面</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Create-Hive-DB-amp-Table"><span class="toc-number">1.3.</span> <span class="toc-text">Create Hive DB &amp; Table</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive"><span class="toc-number">1.3.0.1.</span> <span class="toc-text">Hive</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Write-data-into-Hive-Table-fr-Spark"><span class="toc-number">1.4.</span> <span class="toc-text">Write data into Hive Table, fr Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#當需要的是「非static-data」，而是「動態被傳入的-data」，Arguments-上就要被進行一個傳入"><span class="toc-number">1.4.0.0.1.</span> <span class="toc-text">當需要的是「非static data」，而是「動態被傳入的 data」，Arguments 上就要被進行一個傳入</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Interact-with-data-in-Hive-Table"><span class="toc-number">1.5.</span> <span class="toc-text">Interact with data in Hive Table</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Case-movie-target-id-1-相關的前-Top3-movie"><span class="toc-number">1.5.0.1.</span> <span class="toc-text">Case: movie target id&#x3D;1 相關的前 Top3 movie</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Joe Huang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">406</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">73</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Awaken Desparado</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">BigData_java/2020-12-19-LAB3: Hadoop based DFS &amp; Hive on EMR </div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-12-19</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/BigData-java/">BigData_java</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="HDFS-Hadoop-based-DFS-amp-Hive-on-EMR"><a href="#HDFS-Hadoop-based-DFS-amp-Hive-on-EMR" class="headerlink" title="HDFS(Hadoop based DFS) &amp; Hive on EMR"></a>HDFS(Hadoop based DFS) &amp; Hive on EMR</h1><h2 id="Access-Remote-EMR-clusters"><a href="#Access-Remote-EMR-clusters" class="headerlink" title="Access Remote EMR clusters"></a>Access Remote EMR clusters</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># joe @ MacBook-Pro-4 in ~/Desktop/mi_BigData/Codes_BD [21:38:47]</span></span><br><span class="line">$ ssh -i onionAWS.pem hadoop@ec2-18-191-169-131.us-east-2.compute.amazonaws.com</span><br><span class="line">The authenticity of host <span class="string">'ec2-18-191-169-131.us-east-2.compute.amazonaws.com (18.191.169.131)'</span> can<span class="string">'t be established.</span></span><br><span class="line"><span class="string">ECDSA key fingerprint is SHA256:UdI9VoiJMwSRoJ/SHjNpgger1vgUIxa4MgWtKU9XQaE.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)? yes</span></span><br><span class="line"><span class="string">Warning: Permanently added '</span>ec2-18-191-169-131.us-east-2.compute.amazonaws.com,18.191.169.131<span class="string">' (ECDSA) to the list of known hosts.</span></span><br><span class="line"><span class="string">Last login: Sat Dec 19 14:59:38 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">       __|  __|_  )</span></span><br><span class="line"><span class="string">       _|  (     /   Amazon Linux 2 AMI</span></span><br><span class="line"><span class="string">      ___|\___|___|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://aws.amazon.com/amazon-linux-2/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR</span></span><br><span class="line"><span class="string">E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R</span></span><br><span class="line"><span class="string">EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R</span></span><br><span class="line"><span class="string">  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R</span></span><br><span class="line"><span class="string">  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R</span></span><br><span class="line"><span class="string">  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR</span></span><br><span class="line"><span class="string">  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R</span></span><br><span class="line"><span class="string">  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R</span></span><br><span class="line"><span class="string">E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R</span></span><br><span class="line"><span class="string">EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ ls</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs --help</span></span><br><span class="line"><span class="string">--help: Unknown command</span></span><br><span class="line"><span class="string">Usage: hadoop fs [generic options]</span></span><br><span class="line"><span class="string">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-cat [-ignoreCrc] &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-checksum &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-chgrp [-R] GROUP PATH...]</span></span><br><span class="line"><span class="string">	[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span></span><br><span class="line"><span class="string">	[-chown [-R] [OWNER][:[GROUP]] PATH...]</span></span><br><span class="line"><span class="string">	[-copyFromLocal [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-count [-q] [-h] [-v] [-t [&lt;storage type&gt;]] [-u] [-x] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-cp [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span></span><br><span class="line"><span class="string">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span></span><br><span class="line"><span class="string">	[-df [-h] [&lt;path&gt; ...]]</span></span><br><span class="line"><span class="string">	[-du [-s] [-h] [-x] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-expunge]</span></span><br><span class="line"><span class="string">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span></span><br><span class="line"><span class="string">	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-getfacl [-R] &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-getmerge [-nl] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-help [cmd ...]]</span></span><br><span class="line"><span class="string">	[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span></span><br><span class="line"><span class="string">	[-mkdir [-p] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span></span><br><span class="line"><span class="string">	[-mv &lt;src&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span></span><br><span class="line"><span class="string">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span></span><br><span class="line"><span class="string">	[-rm [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span></span><br><span class="line"><span class="string">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]]</span></span><br><span class="line"><span class="string">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-stat [format] &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-tail [-f] &lt;file&gt;]</span></span><br><span class="line"><span class="string">	[-test -[defsz] &lt;path&gt;]</span></span><br><span class="line"><span class="string">	[-text [-ignoreCrc] &lt;src&gt; ...]</span></span><br><span class="line"><span class="string">	[-touchz &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-truncate [-w] &lt;length&gt; &lt;path&gt; ...]</span></span><br><span class="line"><span class="string">	[-usage [cmd ...]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Generic options supported are:</span></span><br><span class="line"><span class="string">-conf &lt;configuration file&gt;        specify an application configuration file</span></span><br><span class="line"><span class="string">-D &lt;property=value&gt;               define a value for a given property</span></span><br><span class="line"><span class="string">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides '</span>fs.defaultFS<span class="string">' property from configurations.</span></span><br><span class="line"><span class="string">-jt &lt;local|resourcemanager:port&gt;  specify a ResourceManager</span></span><br><span class="line"><span class="string">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span></span><br><span class="line"><span class="string">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included in the classpath</span></span><br><span class="line"><span class="string">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The general command line syntax is:</span></span><br><span class="line"><span class="string">command [genericOptions] [commandOptions]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: true</span></span><br><span class="line"><span class="string">hive&gt; show databases;</span></span><br><span class="line"><span class="string">OK</span></span><br><span class="line"><span class="string">default</span></span><br><span class="line"><span class="string">Time taken: 0.722 seconds, Fetched: 1 row(s)</span></span><br><span class="line"><span class="string">hive&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><ul>
<li><p>比如5台機器在hadoop 集群裡，hdfs 或我們的分布式文件存儲系統就存在於這5台機器上，不需要去關心裡面，就是黑箱，所有合、拆、刪在裡面；我只要在hdfs平台去交互，我只要給他data，他要怎麼做是他內部自己操作</p>
</li>
<li><p>本質就是文件系統</p>
</li>
<li><p>把data存在N台機器上，但client 處理了內部的所有黑箱，所以我只有一個對手! 就如果我在本地</p>
</li>
</ul>
<h2 id="HDFS-commands"><a href="#HDFS-commands" class="headerlink" title="HDFS commands"></a>HDFS commands</h2><h5 id="S3-–-基於AWS的-dfs，HDFS-–-基於Hadoop的-dfs，本質是非常相似的，只不過S3是UI介面"><a href="#S3-–-基於AWS的-dfs，HDFS-–-基於Hadoop的-dfs，本質是非常相似的，只不過S3是UI介面" class="headerlink" title="S3 – 基於AWS的 dfs，HDFS – 基於Hadoop的 dfs，本質是非常相似的，只不過S3是UI介面"></a>S3 – 基於AWS的 dfs，HDFS – 基於Hadoop的 dfs，本質是非常相似的，只不過S3是UI介面</h5><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:10 .hiveJars</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -mkdir bigdata</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:10 .hiveJars</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:23 bigdata</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ vim file.csv</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ ls</span><br><span class="line">file.csv</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -put file.csv bigdata/</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:10 .hiveJars</span><br><span class="line">drwxr-xr-x   - hadoop hadoop          0 2020-12-19 15:28 bigdata</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls data/</span><br><span class="line">ls: `data/<span class="string">': No such file or directory</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -ls bigdata/</span></span><br><span class="line"><span class="string">Found 1 items</span></span><br><span class="line"><span class="string">-rw-r--r--   1 hadoop hadoop         19 2020-12-19 15:28 bigdata/file.csv</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -cat bigdata/file.csv</span></span><br><span class="line"><span class="string">i. love. big. data</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -cat bigdata/file1.csv</span></span><br><span class="line"><span class="string">cat: `bigdata/file1.csv'</span>: No such file or directory</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -rm bigdata/file.csv</span><br><span class="line">Deleted bigdata/file.csv</span><br><span class="line">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -rm bigdata</span><br><span class="line">rm: `bigdata<span class="string">': Is a directory</span></span><br><span class="line"><span class="string">[hadoop@ip-172-31-47-22 ~]$ hdfs dfs -rmr bigdata</span></span><br><span class="line"><span class="string">rmr: DEPRECATED: Please use '</span>-rm -r<span class="string">' instead.</span></span><br><span class="line"><span class="string">Deleted bigdata</span></span><br></pre></td></tr></table></figure>



<h2 id="Create-Hive-DB-amp-Table"><a href="#Create-Hive-DB-amp-Table" class="headerlink" title="Create Hive DB &amp; Table"></a>Create Hive DB &amp; Table</h2><h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h4><ul>
<li>只是把data映射出來讓大家很好地作query</li>
<li>仍是存儲在HDFS上，以文件的型式存儲的</li>
<li>所以該指定的除了 Table Name、Schema, 還該指定最底層的data source的format, 可以是 ORC, Parquet, Avro，由我指定，然後data會被自動存成這個型式，hive會幫我們做這些操作! 不需要操心<ul>
<li>我需要操心的是寫進去hdfs時，想要以怎樣的一個文件型式寫入!</li>
</ul>
</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8q6u4gj20o607omxk.jpg" alt="image-20201219234856829" style="zoom: 33%;" />

<ul>
<li>所有的指令都和SQL一樣，底層就是antler的解析器</li>
<li>hive還是要存去底層給hdfs上去的，然後要指定format，這邊指定成了 ORC</li>
</ul>
<figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@ip-172-31-47-22 ~]$ hive</span><br><span class="line"></span><br><span class="line">Logging initialized using configuration <span class="keyword">in</span> file:/etc/hive/conf.dist/hive-log4j2.properties Async: <span class="literal">true</span></span><br><span class="line">hive&gt; show database;</span><br><span class="line">NoViableAltException(78@[846:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | createMaterializedViewStatement | dropViewStatement | dropMaterializedViewStatement | createFunctionStatement | createMacroStatement | createIndexStatement | dropIndexStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=&gt; grantPrivileges | ( revokePrivileges )=&gt; revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole | abortTransactionStatement );])</span><br><span class="line">	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)</span><br><span class="line">	at org.antlr.runtime.DFA.predict(DFA.java:116)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:3757)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2382)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:244)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:158)</span><br><span class="line">FAILED: ParseException line 1:5 cannot recognize input near <span class="string">'show'</span> <span class="string">'database'</span> <span class="string">'&lt;EOF&gt;'</span> <span class="keyword">in</span> ddl statement</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">Time taken: 0.263 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; create bigdata;</span><br><span class="line">NoViableAltException(24@[846:1: ddlStatement : ( createDatabaseStatement | switchDatabaseStatement | dropDatabaseStatement | createTableStatement | dropTableStatement | truncateTableStatement | alterStatement | descStatement | showStatement | metastoreCheck | createViewStatement | createMaterializedViewStatement | dropViewStatement | dropMaterializedViewStatement | createFunctionStatement | createMacroStatement | createIndexStatement | dropIndexStatement | dropFunctionStatement | reloadFunctionStatement | dropMacroStatement | analyzeStatement | lockStatement | unlockStatement | lockDatabase | unlockDatabase | createRoleStatement | dropRoleStatement | ( grantPrivileges )=&gt; grantPrivileges | ( revokePrivileges )=&gt; revokePrivileges | showGrants | showRoleGrants | showRolePrincipals | showRoles | grantRole | revokeRole | setRole | showCurrentRole | abortTransactionStatement );])</span><br><span class="line">	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)</span><br><span class="line">	at org.antlr.runtime.DFA.predict(DFA.java:144)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:3757)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2382)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)</span><br><span class="line">	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)</span><br><span class="line">	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)</span><br><span class="line">	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.run(RunJar.java:244)</span><br><span class="line">	at org.apache.hadoop.util.RunJar.main(RunJar.java:158)</span><br><span class="line">FAILED: ParseException line 1:7 cannot recognize input near <span class="string">'create'</span> <span class="string">'bigdata'</span> <span class="string">'&lt;EOF&gt;'</span> <span class="keyword">in</span> ddl statement</span><br><span class="line">hive&gt; create database bigdata</span><br><span class="line">    &gt; ;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.301 seconds</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">bigdata</span><br><span class="line">default</span><br><span class="line">Time taken: 0.028 seconds, Fetched: 2 row(s)</span><br><span class="line">hive&gt; use bigdata;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.105 seconds</span><br><span class="line">hive&gt; CREATE TABLE IF NOT EXISTS bigdata.movie_similarity</span><br><span class="line">    &gt; (</span><br><span class="line">    &gt; movie1          INT,</span><br><span class="line">    &gt; movie2 INT,</span><br><span class="line">    &gt; num_pairs INT,</span><br><span class="line">    &gt; similarity DOUBLE</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; STORED AS ORC;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.56 seconds</span><br><span class="line">hive&gt; describe movie_similarity;</span><br><span class="line">OK</span><br><span class="line">movie1              	int</span><br><span class="line">movie2              	int</span><br><span class="line">num_pairs           	int</span><br><span class="line">similarity          	double</span><br><span class="line">Time taken: 0.093 seconds, Fetched: 4 row(s)</span><br><span class="line">hive&gt; describe formatted movie_similarity;</span><br><span class="line">OK</span><br><span class="line"><span class="comment"># col_name            	data_type           	comment</span></span><br><span class="line"></span><br><span class="line">movie1              	int</span><br><span class="line">movie2              	int</span><br><span class="line">num_pairs           	int</span><br><span class="line">similarity          	double</span><br><span class="line"></span><br><span class="line"><span class="comment"># Detailed Table Information</span></span><br><span class="line">Database:           	bigdata</span><br><span class="line">Owner:              	hadoop</span><br><span class="line">CreateTime:         	Sat Dec 19 15:53:27 UTC 2020</span><br><span class="line">LastAccessTime:     	UNKNOWN</span><br><span class="line">Retention:          	0</span><br><span class="line">Location:           	hdfs://ip-172-31-47-22.us-east-2.compute.internal:8020/user/hive/warehouse/bigdata.db/movie_similarity</span><br><span class="line">Table Type:         	MANAGED_TABLE</span><br><span class="line">Table Parameters:</span><br><span class="line">	COLUMN_STATS_ACCURATE	&#123;\"BASIC_STATS\":\"<span class="literal">true</span>\"&#125;</span><br><span class="line">	numFiles            	0</span><br><span class="line">	numRows             	0</span><br><span class="line">	rawDataSize         	0</span><br><span class="line">	totalSize           	0</span><br><span class="line">	transient_lastDdlTime	1608393207</span><br><span class="line"></span><br><span class="line"><span class="comment"># Storage Information</span></span><br><span class="line">SerDe Library:      	org.apache.hadoop.hive.ql.io.orc.OrcSerde</span><br><span class="line">InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat</span><br><span class="line">OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat</span><br><span class="line">Compressed:         	No</span><br><span class="line">Num Buckets:        	-1</span><br><span class="line">Bucket Columns:     	[]</span><br><span class="line">Sort Columns:       	[]</span><br><span class="line">Storage Desc Params:</span><br><span class="line">	serialization.format	1</span><br><span class="line">Time taken: 0.125 seconds, Fetched: 33 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure>



<h2 id="Write-data-into-Hive-Table-fr-Spark"><a href="#Write-data-into-Hive-Table-fr-Spark" class="headerlink" title="Write data into Hive Table, fr Spark"></a>Write data into Hive Table, fr Spark</h2><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8slwhcj21e406mq3w.jpg" alt="image-20201220001941568" style="zoom:50%;" />

<ul>
<li><p>得確認我所generate 出來的DataFrame的schema跟我所需要寫進去的Table裡的schema是完全一致的，不然就會報錯</p>
</li>
<li><p>run Step 的差别在：現在在EMR上run它的JAR的話，data不是存到S3，而是會存到hive連下去的DFS</p>
</li>
</ul>
<h5 id="當需要的是「非static-data」，而是「動態被傳入的-data」，Arguments-上就要被進行一個傳入"><a href="#當需要的是「非static-data」，而是「動態被傳入的-data」，Arguments-上就要被進行一個傳入" class="headerlink" title="當需要的是「非static data」，而是「動態被傳入的 data」，Arguments 上就要被進行一個傳入"></a>當需要的是「非static data」，而是「動態被傳入的 data」，Arguments 上就要被進行一個傳入</h5><ul>
<li><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltmtsovq8j317i0owq9d.jpg" alt="image-20201220003046498" style="zoom:50%;" />



</li>
</ul>
<ul>
<li><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a87yanmj20sm09u3zs.jpg" alt="image-20201220003235826" style="zoom:50%;" />





</li>
</ul>
<h2 id="Interact-with-data-in-Hive-Table"><a href="#Interact-with-data-in-Hive-Table" class="headerlink" title="Interact with data in Hive Table"></a>Interact with data in Hive Table</h2><p>Complete後<br><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltmxlpvqtj319o0l87ok.jpg" alt="image-20201220003427565" style="zoom:50%;" /></p>
<ul>
<li>Limit 1 後才有Trigger，就是aws上的hive還是怪怪的</li>
</ul>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8abfduj21620nen25.jpg" alt="image-20201220003534838" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltmz8c6y9j30n00fstnt.jpg" alt="image-20201220003601410" style="zoom:50%;" />



<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8giz5qj216a0fcwka.jpg" alt="image-20201220003929425" style="zoom:50%;" />



<ul>
<li>snappy就是個常見的壓縮方式</li>
</ul>
<p>想要找到某一個target 的movie所對應的相關聯的電影中的top 3；我們的方法呢？</p>
<p>那時還沒有hive，也沒有query engine，就只能建新的java的file叫 recommend_movie.java 去query output 的 ORC的data，然後把這些data load到DataFrame去，然後DataFrame再進行一系列的 JOIN的操作，然後再把結果給print出來</p>
<p>現在有了Hive，也許就不需要再Spark這種heavy的操作</p>
<ul>
<li><p>Spark – 需要 </p>
<ul>
<li>寫一個spark application</li>
<li>要把spark applicateion 上傳去AWS</li>
<li>要去add step、去執行它<ul>
<li>再等AWS裡面的Spark job還要去 schedule, 之後還要run，run後生成的結果才能去std.out查到!</li>
</ul>
</li>
</ul>
</li>
<li><p>現在有hive，又可以view了，不如就是用hive直接交互數據</p>
</li>
</ul>
<h4 id="Case-movie-target-id-1-相關的前-Top3-movie"><a href="#Case-movie-target-id-1-相關的前-Top3-movie" class="headerlink" title="Case: movie target id=1 相關的前 Top3 movie"></a>Case: movie target id=1 相關的前 Top3 movie</h4><ul>
<li>在 Hive裡 SQL 語句 – query的界面</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltnmjustxj315s0gyasu.jpg" alt="image-20201220005825999"></p>
<ul>
<li>這個時候未啟動 reducer</li>
</ul>
<ul>
<li>↓這時候在reducer裡進行了一系列的排序(?!)，是因為有了排序，所以比較慢了點</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gltnn83zk0j316e0h44h3.jpg" alt="image-20201220005905548"></p>
<img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h38a8kox1ij20om0lctcp.jpg" alt="image-20201220005953451" style="zoom:50%;" />

<ul>
<li>偷看一眼用</li>
<li>這樣工作效率就高很多，就是因為有 Hive! 可以取代了 Spark code in java</li>
<li>可以簡化了基礎的流程</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Joe Huang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/12/19/BigData_java/2020-12-19-LAB3:%20Hadoop%20based%20DFS%20&amp;%20Hive%20on%20EMR%20/">http://yoursite.com/2020/12/19/BigData_java/2020-12-19-LAB3:%20Hadoop%20based%20DFS%20&amp;%20Hive%20on%20EMR%20/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/12/30/BigData_java/Spark/Spark%20APIs%20Note/"><i class="fa fa-chevron-left">  </i><span>BigData_java/Spark/Spark APIs Note</span></a></div><div class="next-post pull-right"><a href="/2020/12/19/BigData_java/2020-12-20-Structued%20Streaminng/"><span>BigData_java/2020-12-20-Structued Streaminng</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2024 By Joe Huang</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>