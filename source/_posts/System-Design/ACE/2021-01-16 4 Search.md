---
layout: post
categories: SystemDesign
tag: []
date: 2020-01-15
---



# Crawl

## 



## 1.2 Req

|      | Fn Req: crawl all web (for google search)    |
|      | Non Fn Req: high extension -- support 全網爬 |
|      |                                              |



| Fn Req             |                                                              |
| ------------------ | ------------------------------------------------------------ |
| Non-Fn Requirement | high extension -- support 全網爬<br />核心網站要專高效爬取，避免反覆爬相同內容，如動態生出來時<br />高可用，避免死循環或crawl trap<br /> |
| Non Requirement    |                                                              |

Non-Fn Req多了politeness



#### Q: 瓶頸? --> QPS



#### Q3: 如何實現優先級?

A: Heap, PQueue



#### Q4: 怎控制爬取同一個網站的freq?

總不狂轟爛炸爆它吧…

A: Single Host Queue, 同一個domain，和別的網站的是分開的, 



### URL Frontier Design

URL可是從parser來，裡面找到很多的網址，發到了prioritizer，給後端的fetcher跟render

Prioritizer會給每個url打個分，基本先取加上ε隨機性 -- 在front queue selector。

back queue是同個domain的，如wikipedia，根據politeness



#### Q5: 有數萬個Single Host Queue，怎麼在低於O(N)下找到符合頻率要求的網址?

A: Heap (current_ts + time_gap, queue_id)

爬完後，他知道下次是x秒後；用個 min-heap



