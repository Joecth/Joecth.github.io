---
layout: post
categories: SystemDesign
tag: []
date: 2019-04-15

---



# General

- NoSQL DB
- SS table 的讀跟寫
- Bloom Filter
- Look up service
- Merge K Sorted Arrays

```python
# 486. Merge K Sorted Arrays
# 中文English
# Given k sorted integer arrays, merge them into one sorted array.

# Example
# Example 1:

# Input: 
#   [
#     [1, 3, 5, 7],
#     [2, 4, 6],
#     [0, 8, 9, 10, 11]
#   ]
# Output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
# Example 2:

# Input:
#   [
#     [1,2,3],
#     [1,2]
#   ]
# Output: [1,1,2,2,3]
# Challenge
# Do it in O(N log k).

# N is the total number of integers.
# k is the number of arrays.


import heapq
class Solution:
    """
    @param arrays: k sorted integer arrays
    @return: a sorted array
    """
    def mergekSortedArrays(self, arrays):
        # write your code here
        return self.helper_heapq(arrays)
        
    def helper_heapq(self, arrays):
        hp = []
        for k in range(len(arrays)):
            if not arrays[k]: continue
            heapq.heappush(hp, (arrays[k][0], k, 0))
            
        # print(hp)
        res = []
        while hp: 
            _, kth, idx = heapq.heappop(hp)
            # print(val, arrays[kth][idx], kth, idx)
            res.append(arrays[kth][idx])
            if idx+1 <= len(arrays[kth])-1:
                heapq.heappush(hp, (arrays[kth][idx+1], kth, idx+1))
            # else:
            # 大可以等下一輪拉出來再push那條的下一個元素，不急
            #     # while idx+1 > len(array[kth])-1:
            #     val, kth, idx = heapq.heappop(hp)
            #     res.append(val)
        return res
```

- Binary Tree Serialization



# Big Table 

Big Table vs GFS

|              | GFS                | BiGTable        |
| ------------ | ------------------ | --------------- |
| 本質         | 分布式File System  | 分布式 NoSQL DB |
| 開發公司     | Google             | Google          |
| 是否開源     | No                 | No              |
| 類似開源產品 | HDFS               | HBase           |
| 操作         | Read & Write       | CRUD            |
| 類似什麼     | 家用電腦的文件系統 | 大數據版的Excel |



結構化文件可上BigTable終究是存上GFS。

> - BigTable 和 Dremel 主要有什么区别？
>   - bigtable是nosql storage system，dremel是query system。
> - 



## Scenario

- 修改 CUD
- 查找 R

後端通常給web server 使用，Scenario比較單一



## Storage

數據庫在邏輯上就是kv型式。硬盤不能存表結構，最終都轉成string去存在disk上。

就是要從文件系統的基礎上去思考搭數據庫系統。

Q: 在文件裡怎麼更好支持查詢操作？

1. 把文件硬盤全讀到內存，然後在內存作流程管理
2. Better, 讀文件進內存後，排序+二分。有什麼問題？==> 
   - 直接在硬盤中對數據作排序 + 硬盤中二分。
     - How? 外排序

Q: 如果有相應的feature有修改操作了，怎辦？

1. 直接在文件裡修改？==> 大家都要移動
2. 讀整個文件，修改好後，把原文件刪了，重新寫入新文件。==> 耗時，每次要讀出寫入其他多餘不變的內容
3. 不修改，直接append操作追加一條紀錄。V! ==> 特快！
   - 怎麼識別最新的紀錄？
     - 在所有數據後面加個時間戳，時間戳最大的就是真正的我們要找的紀錄
   - 沒順序怎麼二分？
     - 分塊有序
       - 每個塊內部有序
       - 寫時候只有最後一塊無序，並且隔一段時間整理成有序。
       - 例：每次寫時就是在最後塊append一條紀錄，for看最後塊有沒有那個值，沒的話就往前一塊作二分查找；每隔時間到就把最後塊給整理成有序下，然後啟新的塊。一個塊是256MB，滿了後就不再寫了。
     - 分塊太多時會有很多重複紀錄啊？如某個feature一直被改改改
       - Solution: 把有序塊中作去重，作k路去重merge，如果是根據時間去重，當時間一樣時，

Big Table 為了寫優化，選了３作。

> - 老师，请问，数据库系统不存储数据，所有数据最终都存储在文件系统当中， 数据库只是负责查询和组织诗句是吗
>   - 是的，大多DB都使用现有的文件系统作为底层存储机制
> - 数据库是不是可以建立在任何的文件系统， 还是要为数据库写一个特殊的文件系统以提高效率？
>   - 一般没有必要，使用普通的文件系统在性能上已经足够了。
> - 利用数据库系统查询文件系统的时候 是在写入文件时就将文件里的所有表内容 按key-value的形式 再写入数据库么？
>   - 按特定的数据结构存储，正如你说的key-value的形式，然后写入数据库中
> - 这里除去duplicate的k路归并也是全部读到内存中进行么？每一块中可以有duplicate存在么？
>   - K路归并的方法在后面的拓展视频中有讲到，每一个块中可以有duplicate
> - 我如果想查找k的值，我怎么知道k在哪个文件里？
>   - 每个文件存储的k都是一个范围内的k值，直接二分搜索找到k在哪个范围内就找到了那个文件
> - append到文件末尾也会增加文件大小啊，怎么避免和其他文件储存的位置冲突？
>   - 如果是基于文件系统上搭建数据库，不必考虑此问题，操作系统会帮你完成；如果是自己直接基于disk搭建数据库，每个数据块初始化的时候就预先开辟适量的disk空间，当写入增加的数据量超过原有分配的空间的时候就需要申请一块更大的disk空间，将它拷贝过去，然后将原有的空间释放即可。（可以看下操作系统书中有关机械磁盘基本原理的介绍）
> - 直接把修改内容append到最后，这个方法，如果修改得太多了，会不会感觉很乱，每次读的时候cost增加，因为一个file可能有三分之一的size是修修改改？需不需要在修改的量超过一个比例的时候处理一下？
>   - 首先要明确一点的是，没有任何一个数据库是生来都适合所有操作和场景的。也就是说，BigTable 本来就不适合修改特别多的场景。另外 BigTable 这类数据库会定期 Merge sstable，从而消除那些中间态的修改记录。
> - 分块是什么意思？
>   - 是指将指定的“filename” 这个文件分成很多块，然后每次存储的时候找该文件编号的最后一块进行写入。如果写满了 是需要在文件系统里追加写入一个新的对应该文件名的块
> - 在讲到“分块有序”时，老师说到每个块之间都是有序的。但是块与块之间是不是有序的呢？也就是说是不是第1个块的 key 值都小于第2块的 key 值，第2块的 key 值都小于第3块的key值？（最后一块除外）
>   - 块与块之间是有序的。这里可以是key-value存储记录块的索引地址；也可以是B+树索引，方便查找修改。
> - 那每次查询都要查询每一个块吗？不能直接锁定就在哪一个块中（例如索引之类的）？
>   - ***好问题，后面的视频中讲到了索引，带着问题继续学习，棒！***



# One Work Solution

### 寫入過程

​	一個無序快快寫滿時，就排序然後寫入到FS。

1. 讀入到內存作快排 X
2. 外排序 X;  
   - 每個文件最多就也256M，一個內存完全可放下，完全不需要外排
3. 可不可一開始就放在內存？V!

比方，內存會用跳表這個結構，

### 機器掛了怎辦？

Write Ahead Log (WAL)，就只是append而已

append寫disk非常快和一般寫不同，打開模式為寫

如果要把最後一個塊放在硬盤的話，它不只要可以append還要可以read的操作；

append和寫不同，打開如果是append模式時，ptr是在尾巴；如果是寫模式，文件原內容會被清楚，ptr會指在文件頭。

所以：

内存排序+1次硬盘统一写入+1次硬盘写Log

Link: http://www.larsgeorge.com/2010/01/hbase-architecture-101-write- ahead-log.html

> - 每个块最大256M，是Google的Big Table自己定义的吗？还是一种通用的标准呢？而且这个“块”的学名是什么呀？谢谢！
>   - 是的，是官方定义。
>     256M 块的大小是google工程师长期实践的一个经验值，统计过这个值比较合理。
>     最后，块的专业名词为：chunk
> - 什么是硬盘外部排序？
>   - 当参加排序的数的量太大，或内存不足以存放时，需要使用外排序。外部排序指的是大文件的排序，即待排序的记录存储在外存储器上，待排序的文件无法一次装入内存，需要在内存和外部存储器之间进行多次数据交换，以达到排序整个文件的目的。
> - 老师，最后一个块是存在内存里的，存满256MB之后再写入硬盘。请问每次append一条数据到块里，是每次利用二分查找来保证这条数据插入到正确位置，还是每次就直接在块的尾部直接添加数据，直到块满了256MB，再排序变为有序啊。
>   - 直接在尾部添加，直到块满就进行排序。
> - 这里的WAL 不是应该 每次有新的数据到内存都要写一次WAL吗？ 为什么说是1次的硬盘写LOG ？
>   - 是的，Write Ahead Log，就是对内存中Sorted Table里，new append data在HDD做一次append的操作。请再认真听讲，要确认每一个细节，其实再认真听讲是可以找到答案的，老师有讲到此细节。确实有问题了，仔细思考后再提出问题，这样学习深刻。
> - Write ahead log 为什么是一次， 不应该每次有update 都要写一次吗？
>   - 这块的WAL是多次的数据操作才会触发一次log写操作。这是为了性能的考量，HDD写的速度相比磁头寻道速度还是很快的，不想把太多时间浪费在寻道上，也就是每次写操作就需要寻道一次。



# 讀取優化

本來是只能在FS上作二分，更好的是建index

- 一個簡單的建index 的方法：
  - Key
    - 就不需要整個文件作二分
    - 把一些Key放入內存作為index
    - Index有效減少磁盤讀寫次數
- B-tree Index



> - 所以内存中有无序的存储块，和sorted存储块（sorted list），先用for查找无序没有，再去有内存中sorted list上跳跃查找？
>   - 大体思路是对的，详细流程稍微更正如下：
>     (1)query Master server by the key;
>     (2) tablet server（即SSTable slave server）;
>     (3) check skip list(即跳表，在memory中) in the tablet server（对象没有排序的Sorted List）;
>     (4) check bloom filter(memory) for a SSTable(Sorted String Table)（最好一块有序块）;
>     (5) check index(memory) for a SSTable(查找其key的大致范围);
>     (6) find the value in the SSTable(根据在内存index定位到的小范围，在磁盘中通过二分查找这个key的文件内容).
> - "看到这里我对于建立index又有些迷茫了。本来一个块中就是有序的，直接二分不就行了。就算是先看index，也是要先对index进行二分，再拿着找到的位置去块本身里找东西。那么index的好处是不是因为index比数据本身的size小，所以在index里二分比在数据里二分块？ "
>   - index 是在内存里的，你在内存里二分 index 比直接去 disk 上二分要快好几十倍。***文件操作远远慢于内存操作***。

> 这里是课上提到的 B-Tree / B+ Tree 的一些资料：
>
> Wiki:
> https://en.wikipedia.org/wiki/B-tree
>
> 以B tree和B+ tree的区别来分析mysql索引实现:
> https://www.jianshu.com/p/0371c9569736
>
> 这部分的内容只需要了解原理，不需要代码实现。



## Bloom Filter

### 繼續讀優化

因為寫的時候我們用的是append, 但就是讀的時候被搞亂了，所以讀就要一直優化。

數組長度自己定。

- 多個 hash fn時，還是一樣同個數組在作mapping作查詢。
- 是可能會有False Positive的；但就是起了filter的作用
- 精度跟
  1. 加入的字符串數目
  2. hash個字
  3. array長度。
- 有誤判率計算公式



***先用bloom filter看這個塊有沒有、有的話再用index找key可能在塊中的範圍，再用二分找這個塊裡的值***　當然，在這之前是要先做完對最後那個在內存的塊的線性查找。



## SS Table & Skip List

- String is Store in the File. 
- SSTable = Sorted String Table
- Sorted List 用 Skip List 实现

1. Skip List
    Code: https://github.com/petegoodliffe/skip_list Wiki: http://bit.ly/2g0C29a
2. SSTable
    Google SSTable Page: http://bit.ly/1kqwrFe



> - "比如如果我用64个hash函数，每个文件按256M算(按每行数据256bytes算就是一百万行)。你这里难道把一百万个key都放在bloom filter里么 "
>   - 是的，bloom filter的基本单位是bit，这里记录的是hash后的数字，不是文件，***一个字节8bit，大约10w 字节，就是100kb，也就是0.1mb***。（为了精度可能需要大概几mb的内存就够了）
> - 之前一小节讲的查一个key在不在这个块里需要先把这个块的index load到内存中再二分。你load index文件的时候不也是O(n)么？不在硬盘上二分是因为磁头移动的时间？所以宁愿先load一整个index file到内存中
>   - 是的，因为磁头寻道时间开销相对内存来说很大。（可以读下《现代操作系统》 5.4 磁盘臂调度算法）为了减少磁头寻道的开销衍生出的一些算法
> - "算两次hash值是为了防止collision么？可以扩大bloom filter的array大小，这样hash也不容易碰撞，这样可以么？ "
>   - 多个hash function可以减少误判率，扩大array的大小也可以减少误判率
> - index是B+ tree对吧，那为什么只能找到范围？最后一步，二分搜索在磁盘上进行，不是会非常慢么？
>   - 对的，因为具体的内容要到块当中去找，块里的内容key是用字典序排列的，因此存在相同字母开头的key，所以只能锁定一个范围。锁定这个范围之后就可以二分了，logn的速度可以接受
> - 之前提到BigTable会定期执行Disk Merge Sort，这样不就可以直接对磁盘Blocks二分了吗? 为什么还要设计Bloom Filter？
>   - 有很多请求是需要判断key是否在文件中，HDD二分查找也是log级别的操作，Bloom Filter来过滤掉很多不必要的HDD操作
> - 对于SSTable来说好像Index和Bloomfilter都是想要做一件事。视频的图中为什么两个都需要建立呢？
>   - Boom Filter是起到快速筛选的，如果Bloom Filter中key存在，那么就可能在此SSTable中存在，如果不存在那么肯定就不存在此有序列块中。当key经过Bloom Filter确定可能存在后，再通过index确定其所限的查找范围，确定后，再去磁盘中进行二分查找。



# Scale - Sharding

- 垂直沒那麼好，因為可能也想關心這筆data的其他元素。會增加讀取其他feature的難度。

- ### 所以水平 consistent hashing

  - 大表被拆成了很多個小表們



# BigTable的分布式鎖

- 讀寫鎖

  - 一把鎖，兩種屬性：讀跟寫

    - 讀鎖：讀的時候上的

    - 寫鎖：寫的時候上的

    - 特性

      1. 線程一加讀鎖成功了，又來了兩個讀線程，可以加鎖成功，可進行讀共享

      2. 線程一加寫鎖成功了，又來了兩個讀線程，不能加鎖成功，兩個讀的要block；

         寫的時候只允許一個線程對內存作寫操作

      3. 線程一加讀鎖成功了，來了寫被block，又來了讀就要被block了；寫的優先級是比較高的，並且線程二來得早線程三就得要等了。

         1. 讀是共享
         2. 寫是獨占
         3. 寫的優先級更高

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l4p7wsnj30nw0nwte3.jpg" alt="image-20200420195125198" style="zoom:50%;" />

![image-20200420195309119](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0h80fthij30s00j0n0m.jpg)

- 鎖服務器：

  - Master的一致性哈希分配小弟也就這邊執行了

  

> - 如果是多台锁服务器，怎么处理并发呢，或者说这么保证对同一个key的上锁操作在同一台锁服务器上进行呢？也是hash吗？map存在web server 上？
>   - 如果锁是一个cluster，***多个节点之间会通过一致性协议来保持同步，对外表现得就像是只有一个节点一样，所以不用担心不一致的问题***。
> - 这里的图显示 读 和写的都要LOCK 这个KEY，那不是会神慢吗？ 如果KEY LOCKED了后续怎么样呢？ 写一个RETRY吗？ 如果 很多程序同时读这个 KEY，那都得等待了？
>   - 从安全和数据一致性来说，会相对重要些。lock肯定会慢些，但是这不影响总体的运行效率和用户体验。lock本身就是一个信号量互斥的实现机制，也线程间通信的基础， 详阅https://en.wikipedia.org/wiki/Lock_(computer_science)
> - consistent hash map存在lock server上，那master还有什么作用？
>   - 比如 heatbeat 。还有好多事儿要做呢，除了存储还有计算的工作呀。包括管理很多机器的启动，备份恢复等等。
> - 感觉这个lock server不是distributed lock, 实际上是一个centralized locking service?
>   - lock service内部是distributed实现的，但对外接口是中心化锁服务。具体使用的是chubby。
> - 如果写入的是一个新key，metadata里面还没有这个key对应的consistent hashing value，这时候的写入过程是什么呢？
>   - lock server查找有这个key的话锁上-》lock server返回server id-》去tablet server对应id写入-》写入GFS-》成功写入 返回True-》lock server释放锁
> - 请问 summary of write 这张图里的 master， 所以其实没有起到任何作用， master 的功能全部被 lock server代替了么？
>   - master还是提供分发控制的作用；lock server就是保证数据一致性（同一时间只有一个线程操作）
> - 另外你这个读请求的锁，应该只是block其他的对于这个key的write request吧。如果有很多对这个key的read request怎么办？这种情况下难道不能单机多线程读这个key吗。 虽然这种hot partition的case很多情况下是用户没设计好partition key..
>   - 如果有很多对这个key的read request，系统只能等待，性能确实就会下降很多。如果想提这块的性能，可以使用双指针的方法，数据多份备份，这样这块的性能就会提升，但是数据脏读概率增大了。
> - 读取data这种操作 为何也需要锁 没有锁也没有任何问题吧。
>   - 因为在read的同时可能还有针对同一个对象的updata/delete正在进行，为了防止读数据读到一半数据被删了这种不愉快的事情发生，加锁是必要的。
> - 所以timestamp这个信息是存在lock server上的内存中么？一旦完成了上一个，lock server会向下一个最新的request的server发送消息告诉它，现在unlock了，可以开始处理它的请求了？还是以别的方式做信息交互？
>   - 你说的是对的，不过需要知道的是timestamp在sstable上也会存，便于查找的时候知道哪一个是现在最新的值
> - 如果client 拿到锁之后 还没来得及写就意外挂了， 锁怎么释放？有时间限制吗？
>   - "A client’s session
>     expires if it is unable to renew its session lease within the
>     lease expiration time. When a client’s session expires, it
>     loses any locks and open handles." client超时后，会释放锁资源。
> - Disk 中的 每个SSTable 都有自己的bloomfilter 和 index 吗？ 不同SSTable 的可以不一样吗
>   - 对，这样理解是对的。当TableServer启动后开始服务初始化期间，会读取所有的有序块（SSTable）的Bloom Filer和Index到某个内存区域，以便服务查找。
> - Lock Server存的metadata的数据来源是哪里呢？从BigTable Master里来的？他们之间怎么交流的呢？
>   - BigTable Master的key
>     这个系统对外会提供一个接口，接口不可直接操作bigtable master、lock server等，这些操作都是内部同步实现的逻辑
>     通信都是socket，可以考虑之间加上消息队列，直接append提交任务给目标server
> - the entire process did not involve the master server... are we saying that if we have lock/zookeeper, we dont have to access master when reading and writing?
>   - only the first time from the a client for request, the master server should handle it, as master server need all the tablet server status for the first time of health-check, later then master server will synchronize/delegate all the tablet server status to locking server for the task hand-over. in that case, master server will move on to focus the health-check only.
> - 分布式锁这里面的“分布式”是什么意思？就是说这个锁适用于处理distributed system里面多台服务器需要对同一个资源进行读写的情形？那么不分布式的锁是怎么回事呢？
>   - 这里的分布式指的是适用于多台服务器试图访问同一资源，相当于多台服务器上的多个进程共享资源。
>     非分布式的锁，就是单机上对资源的访问锁。通常就是线程间的资源共享或者单机上多个进程资源共享，通常使用操作系统提供的基础锁设施就可以完成。
> - 整个write 过程， 那一点是系统最慢的过程？ 同样，整个read 过程哪一点是系统最慢的过程？ 这个分布式锁应该会让系统变慢不少把？
>   - 写的过程中，写满一个SSTable序列化到硬盘的时候相对比较慢，读的时候查引索时相对慢一点。实际在使用时，bigtable会使用一种叫做copy on write(cow)的机制，来保证读写操作的原子性，降低分布式锁对速度的影响
> - 感觉bigtable 这种read 的时候需要一个个SSTable 分别查找的方式不如Cassandra 这种直接一个 consistent hashing ring，一步就能知道data 在哪个node上面快啊？
>   - bigtable首先会在锁服务器上查一致性哈希表，得到在哪个server上。得知之后，会去那个server上查memtable、bloomfilter、sstable
> - 为什么一开始shard还是要通过master 来进行呢，为什么不能通过Locker 来进行呢
>   - 通过最开始的文件sharding，取得与其负责管辖的tablet servers的建立联系，并且记录其各自的运行情况（alive/not alive）和健康信息。
> - 这里讲的master会shard file是什么意思？以及consistent hashing的map如果是存在 master/lock server的内存里，master/lock挂了怎么办
>   - shard file是共享文件的意思，就是master所管理的各个node server都可以访问和操作这个文件。
>     以前确出现在master挂掉导致整个集群挂掉的情况，现在master都会加有数据持久化的功能、master备份（主master挂掉，备份master就会启动，基本不影响集群运行）
> - 把metadata放在lock里跟放在master里比有什么优势吗？如果这样做会使系统性能更好，那是为什么呢？
>   - 这里相当于将lock这个服务从master里面拆分了出来，这样肯定好的，master一般就是系统的性能瓶颈，减少master的负载，提高系统性能。



# Appendixes

## K sorted merge

每次找最小的都要O(K), N個下來就要 O(NxK), 用個heap吧 可O(Nxlg(K))



## 外排序

一共8G的話，每次load進來2G, 就有四條排好的。可再接用K-sorte merge

- 最小可以就維護４個元素

- 改善heap的效能可用

  - 敗者樹 可以改善選取最小項的常數項的Ｋ

    <img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l4pvld7j31460ggwgt.jpg" alt="image-20200420202745772" style="zoom:50%;" />

  - 可用選擇-置換的方法　替換掉每組2G的平均分組，就減了少組數，也就是K的大小更小，跑起來更快；每條長度不等就是

    https://www.bilibili.com/video/av69423198/

    - 搭配最佳歸併樹，讓IO次數最少的==>帶權路徑最小：Huffman Tree

> - 之前write 过程的去重就是在merge过程将重复的key中老的数据去除就好了对吧。
>   - Yes



## GFS & BigTable

Disk vs Excel



## B+ Tree

- BST 如果有10億數據怎辦？1Gx4Byte == 4GB
  - 一顆樹就要4GB，多棵樹肯定不行在內存，長期只能在disk
  - 一次尋軌10ms
  - 順序讀取 80MB/S

Methods:

1. 找一次就要4GB/80MB的時間，不可行

2. 只讀取訪問路徑上的數，就是30多個元素 即 30x10ms = 300ms

   <img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0j7utyd2j314q0eogpr.jpg" alt="image-20200420210210409" style="zoom:67%;" />

3. 把二叉變多叉讓樹高降低！

   <img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0j9nfovbj31460ggk0s.jpg" alt="image-20200420210355659" style="zoom:67%;" />

   <img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420211217773.png" alt="image-20200420211217773" style="zoom:67%;" />

   17是key, data是一堆的其他的feature；B-Tree 為了存硬盤中的數據設計

4. B+ 

   <img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420211622972.png" alt="image-20200420211622972" style="zoom:67%;" />



## Bloom Filter vs Hash Table

一種數據結構，跟hash table類似，檢測一個元素在一個集合有沒有出現。

- 區別在 BF 更省空間。Hash還是比較費內存。BloomFilter沒有鏈表，主要是用0/1數組，叫Bit Array，沒有鏈表操作，用到多個hash function。
- Hash fn 也不是愈多愈好，這會讓0/1消耗愈快；一般取4~16個，有理論結果。
- 快速檢測一條紀錄有沒有在某一個大塊之中。
- 在big table中查大多時候都是沒在某一個小塊，不希望每個小塊都去查二分，小塊其實不小。
- hash不是等長結構因為有linked list, 無法直接存出；BF可以存出去，就是個bit array了。

當要在兩個超過內存的大file裡找交集的urls時，相比hash一長字串就可以節省很多很多空間；但可能小誤報啦。

- Counting BF vs Standard BF
- 數組動態膨脹，依據被標為1超過的比例；但BF不存key, 所以複製不過去，膨脹時就是新舊的bit array都要查。效率不是特別好。。短的話誤報率高

> - 一个bit 怎么做加法？
>   - 如果是 Counting bloom filter 的话，每一位就用 int 不用 bit 了。
> - Bloom filter自动膨胀的时候，能不能把原来的bit array丢掉，把所有数据全部拿来重算一遍？我不理解老师说的不能重算一遍这个点。
>   - 不能。比如你把单词 hello world 放到 bloomfilter 里以后，得到的 bit array 是 `[1,0,0,1,0,1,0,0,0,0,1,0,0]`，难道你可以根据这些0和1反向推导出是 Hello 和 World 这两个单词么？BF 不可逆。

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l4frsejj311m0fqgpt.jpg" alt="image-20200420214218446" style="zoom:50%;" />



## 跳跃表 Skip List

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420215801422.png" alt="image-20200420215801422" style="zoom:67%;" />

- 為什麼要用跳表？
  - 是個有序鏈表
  - 可提取一些元素，加一層引索

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420220446625.png" alt="image-20200420220446625" style="zoom:67%;" />



![image-20200420220653019](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge0l405klnj311m0fqgpt.jpg)



- 本質是一個分層鏈錶

  

# QA

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420123257444.png" alt="image-20200420123257444" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420163656675.png" alt="image-20200420163656675" style="zoom:50%;" />

<img src="/Users/joe/Library/Application Support/typora-user-images/image-20200420163734278.png" alt="image-20200420163734278" style="zoom:50%;" />