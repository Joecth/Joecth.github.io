---
layout: post
categories: Machine-Learning
date: 2018-11-20
tag: [] 




---

### 1. SVM

Ref: https://blog.csdn.net/sinat_35512245/article/details/54984251
$$
d=|w^Tx+b|||w||　　　　　　　　　　　（１）
$$
其中，||w||为w的范数。

当点A表示某一实例$x_i$，其类标记为yi=+1yi=+1。点A与超平面的距离记作didi，那么
$$
di=wTxi+b||w||　　　　　　　　　　（２）
$$
当点A表示某一实例xixi，其类标记为yi=−1yi=−1。点A与超平面的距离记作didi，那么
$$
di=−wTxi+b||w||　　　　　　　　　（３）
$$
一般地，点xixi与超平面的距离是
$$
di=yiwTxi+b||w||　　　　　　　　　（４）
$$
公式（４）也被称为超平面关于样本点xixi的几何间隔。

#### 最大间隔分离超平面

![这里写图片描述](https://tva1.sinaimg.cn/large/006y8mN6ly1g9cwe8dihaj30c007cdgl.jpg)

如上图所示，距离超平面最近的这几个训练样本点被称为**支持向量**，两个异类支持向量（即分别位于超平面两侧的点）到超平面的距离之和为
$$
d=2/||w||　　　　　　　　　　　　　　　（５）
$$
上面（5）的d称为间隔（margin）。
要求得最大间隔（即最大化 $2/w$），就是要满足：

![这里写图片描述](https://tva1.sinaimg.cn/large/006y8mN6ly1g9cwamnyyij309h02sjrb.jpg)

显然，为了最大化间隔，仅需最大化$||w||^{−1}$，这等价于最小化$||w||^2$，于是上式可以重写为：

![这里写图片描述](https://tva1.sinaimg.cn/large/006y8mN6ly1g9cwaz5iy0j309u02mdfs.jpg)

这就是支持向量机的基本模型。

**到目前为止，我们的 SVM 还比较弱，只能处理线性的情况，下面我们将引入核函数，进而推广到非线性分类问题。**

**非线性支持向量机和核函数**

非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题。先看一个例子：

------



### 2. DeepLearning Frameworks: Dynamic vs Static Graphs

# 动态图 vs. 静态图

ref: https://www.cnblogs.com/yifdu25/p/8763717.html

在 fast.ai，我们在选择框架时优先考虑程序员编程的便捷性（能更方便地进行调试和更直观地设计），而不是框架所能带来的模型加速能力。这也正是我们选择 PyTorch 的理由，因为它是一个**具有动态图机制的灵活框架**。

依据采用动态计算或是静态计算的不同，可以将这些众多的深度学习框架划分成两大阵营，当然也有些框架同时具有动态计算和静态计算两种机制（比如 MxNet 和最新的 TensorFlow）。动态计算意味着程序将按照我们编写命令的顺序进行执行。这种机制将使得调试更加容易，并且也使得我们将大脑中的想法转化为实际代码变得更加容易。而静态计算则意味着程序在编译执行时将先生成神经网络的结构，然后再执行相应操作。从理论上讲，静态计算这样的机制允许编译器进行更大程度的优化，但是这也意味着你所期望的程序与编译器实际执行之间存在着更多的代沟。这也意味着，代码中的错误将更加难以发现（比如，如果计算图的结构出现问题，你可能只有在代码执行到相应操作的时候才能发现它）。**尽管理论上而言，静态计算图比动态计算图具有更好的性能，但是在实践中我们经常发现并不是这样的。**

谷歌的 TensorFlow 主要使用了静态计算图，而 Facebook 的 PyTorch 则使用了动态计算图机制。（注：TensorFlow 在两周前宣布了一个动态计算选项 Eager Execution（http://t.cn/RlZizQ2），不过该特性还比较新颖并且 TensorFlow 的文档和项目依然以静态计算为主）。在九月份，fast.ai 宣布将在今年的课程中采用 PyTorch 框架进行教学以及开发 fast.ai 自己的框架（实际上就是采用了更好的编码方式对 PyTorch 进行高级封装）。简而言之，以下是我们选择 PyTorch 的几个原因（更详细的原因请参见这里http://t.cn/Rpqj6pu）：

- 更加容易调试
- 动态计算更适用于自然语言处理
- 传统的面向对象编程风格（这对我们来说更加自然）
- TensorFlow 中采用的诸如 scope 和 sessions 等不寻常的机制容易使人感到疑惑不解，而且需要花费更多时间学习

谷歌在推广 TensorFlow 上已经花费了大量的资源，其投入要远远大于任何其它公司或者团队，并且我想这也是为什么 TensorFlow 会如此出名的原因之一（对于很多深度学习的门外汉，TensorFlow 是他们唯一听说过的框架）。正如之前所述，TensorFlow 在几周前发布了动态计算选项，这将解决了一些上述提到的问题。然后许多人就向 fast.ai 提问说我们是否考虑迁移回 TensorFlow 框架。**但是目前 TensorFlow 提供的动态选项还比较新颖而且开发也不够完善，所以我们依然选择继续愉快地使用 PyTorch**。但是 TensorFlow 团队非常乐意于接受我们的想法，我们也很高兴看到我们的 fastai 库（http://t.cn/RYyK6jC）被移植到 TensorFlow 中。

------

